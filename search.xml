<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[HelloBlog]]></title>
    <url>%2F2019%2F04%2F02%2FHelloBlog%2F</url>
    <content type="text"><![CDATA[怕什么真理无穷,进一寸有一寸的欢喜 import torch print(torch.__version__) next主题NexT 主题优化个性化配置教程]]></content>
      <categories>
        <category>SpareTime</category>
      </categories>
      <tags>
        <tag>SpareTime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention机制的理解]]></title>
    <url>%2F2018%2F06%2F12%2FAttention%E6%9C%BA%E5%88%B6%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Attention机制的理解Attention Model类似于人脑的注意力模型，说到底是一种资源分配模型，在某个特定时刻，你的注意力总是集中在画面中的某个焦点部分，而对其它部分视而不见。 |Encoder-Decoder框架文本处理领域的AM模型，因为目前绝大多数文献中出现的AM模型是附着在Encoder-Decoder框架下的，当然，其实AM模型可以看作一种通用的思想，本身并不依赖于Encoder-Decoder模型，这点需要注意。 Encoder-Decoder框架可以看作是一种文本处理领域的研究模式，应用场景异常广泛，下图是文本处理领域里常用的Encoder-Decoder框架最抽象的一种表示： Encoder-Decoder框架可以这么直观地去理解：可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对&lt;X,Y&gt;，我们的目标是给定输入句子X，期待通过Encoder-Decoder框架来生成目标句子Y。X和Y可以是同一种语言，也可以是两种不同的语言。而X和Y分别由各自的单词序列构成： Encoder顾名思义就是对输入句子X进行编码，将输入句子通过非线性变换转化为中间语义表示C：对于解码器Decoder来说，其任务是根据句子X的中间语义表示C和之前已经生成的历史信息y1,y2….yi-1来生成i时刻要生成的单词yi ： Encoder-Decoder框架是个通用的计算机框架，可以有各种不同的模型结合，具体用什么模型由研究者自己决定，常见的比如CNN/RNN/BiRNN/GRU/LSTM/Deep LSTM 。 Attention Model图1中展示的Encoder-Decoder模型是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Y中每个单词的生成过程如下： 其中： f是decoder的非线性变换函数。由此可看出，在生成目标句子的单词时，不论生成哪个单词（y1,y2还是y3），他们使用的句子X的语义编码C都是一样的，没有任何区别。而语义编码C是由句子X的每个单词经过Encoder 编码产生的，这意味着不论是生成哪个单词，y1,y2还是y3，其实句子X中任意单词对生成某个目标单词yi来说影响力都是相同的，没有任何区别（其实如果Encoder是RNN的话，理论上越是后输入的单词影响越大，并非等权的，估计这也是为何Google提出Sequence to Sequence模型时发现把输入句子逆序输入做翻译效果会更好的小Trick的原因）。这就是为何说这个模型没有体现出注意力的缘由。这类似于你看到眼前的画面，但是没有注意焦点一样。如果拿机器翻译来解释这个分心模型的Encoder-Decoder框架更好理解，比如输入的是英文句子：Tom chase Jerry，Encoder-Decoder框架逐步生成中文单词：“汤姆”，“追逐”，“杰瑞”。在翻译“杰瑞”这个中文单词的时候，分心模型里面的每个英文单词对于翻译目标单词“杰瑞”贡献是相同的，很明显这里不太合理，显然“Jerry”对于翻译成“杰瑞”更重要，但是分心模型是无法体现这一点的，这就是为何说它没有引入注意力的原因。没有引入注意力的模型在输入句子比较短的时候估计问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。 上面的例子中，如果引入AM模型的话，应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值： （Tom,0.3）(Chase,0.2)(Jerry,0.5) 每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词Yi的时候，原先都是相同的中间语义表示C会替换成根据当前生成单词而不断变化的Ci。理解AM模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。增加了AM模型的Encoder-Decoder框架理解起来如图2所示。即生成目标句子单词的过程成了下面的形式：]]></content>
      <categories>
        <category>DeepLearning</category>
        <category>Attention</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
        <tag>Attention</tag>
        <tag>Encoder-Decoder</tag>
        <tag>Neural Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fast and Robust Neural Network Joint Models for Statistical Machine.md]]></title>
    <url>%2F2018%2F06%2F09%2FFast-and-Robust-Neural-Network-Joint-Models-for-Statistical-Machine-md%2F</url>
    <content type="text"><![CDATA[Fast and Robust Neural Network Joint Models for Statistical Machine TranslationNNJM:通过一个源上下文窗口扩展NNLM（which augments the NNJM with a source context window）。该模型是纯词汇化(purely lexicalized)的，可以集成到任何MT的Decoder中。具体来说，该模型利用m-word源窗口扩展一个n-gram目标语言模型。和以往的联合模型不同，该模型能够很容易作为一个feature被整合到任何SMT解码器中。 NNJM近似地估计了以源句子S为条件的目标假设T的概率。遵循目标的标准n-gram LM分解，其中每个目标字ti都受前面的n- 1个目标字的制约。为了使这个模型成为一个联合模型，对源上下文向量 进行了条件分析： 每一个目标词ti都直接对应着一个在位置ai的源词，是在以ai为中心的m-word的源窗口。 这种从属（affiliation）概念源自单词对齐，但与单词对齐不同，每个目标单词必须与一个非空(non-NULL)源单词相关联。 中文-英语平行句子的NNJM上下文模型例子如下图： 论文中采用的是n=4、m=11的15-gram LM 模型（神经网络语言模型能够优雅地扩展并利用任意大的上下文大小）。 论文中的神经网络结构与Bengio et.al等人的前溃神经网络语言模型结构基本相似，如下图。 1. NNJM中的神经网络结构NNJM中的神经网络架构与Bengio et al.(2003)所描述的原始前馈NNLM体系结构（feed-forward NNJM architecture）几乎相同。隐藏层大小、词汇表大小和源窗口大小选择了这些值: 2. 神经网络训练除了使用平行语料库代替单语语料库外，训练过程与NNLM相同。在形式上，我们寻求使训练数据的逻辑可能性最大化： * 优化（Optimization）: 带SGD的标准后向传播。 * 权重（Weights）：[-0.05,0.05]之间进行随机初始化 * 学习率： 10^-3 * minibatch size: 128 * 20,000 minibatches/each epoch, 计算验证集的可能性。 * 40 epochs * 我们在没有L2正则化或动量的情况下执行基本的权值更新。 * Training is performed on a single Tesla K10 GPU, with each epoch (128*20k = 2.6M samples) 3. Self-Normalized Neural NetworkNNLM的计算成本在解码中是一个重要的问题，并且这个成本由整个目标词汇表上的输出softmax所支配。我们的目标是能够使用相当大的且没有词类（word-classes）的词汇表，并且简单地避免在解码时计算整个输出层。为此，我们提出了自规一化（self-normalization）的新技术，其中输出层分数是接近于没有显示执行softmax的概率。我们所定义的标准softmax对数似然函数如下： 由上看出，在解码阶段当log(Z(x))等于0时（Z(x)=1）我们就只需要计算输出层的r行而不是计算整个矩阵，但是很难保证用这个来训练神经网络，所以可以通过增加训练目标函数来明确鼓励log-softmax正态化器（explicitly encourage the log-softmax normalizer）尽可能接近0： 在这种情况下，输出层的偏置权值初始化为log(1/|V|)，因此初始网络是自归一化的。在解码时，使用作为特征得分，而不是选用log(P(x))。在本篇论文中NNNJM结构中，在解码过程中，self-normalization将查找速度提高了15倍。 在用噪声对比估计（ Noise Contrastive Estimation ，NCE）训练自归一化的NNLMs时虽然加速了训练时间，但是没有机制能控制自归一化程度。 4. Pre-Computing the Hidden Layer自归一化显著提高了NNJM查找的速度，该模型仍比 back-off LM慢几个数量级。在这里，我们展示了预计算(Pre-Computing)第一个隐藏层的技巧，它进一步将NNJM查找速度提高了1000倍。请注意，这种技术只会导致自归一化，前向反馈，有一个隐藏层的NNLM-style网络的显著加速。 5. Decoding with the NNJM论文所提出的NNJM本质上是一个带有附加源上下文的n-gram NNLM，所以可以很容易地集成到任何SMT解码器中。 NNJM is fundamentally an n-gram NNLM with additional source context, it can easily be integrated into any SMT decoder。 主要介绍将NNJM集成到分层解码器时必须考虑的事项。 Hierarchical Parsing（分层句法分析） Affiliation Heuristic（加入启发式）]]></content>
      <categories>
        <category>Papers</category>
        <category>SMT</category>
      </categories>
      <tags>
        <tag>论文阅读笔记</tag>
        <tag>GEC</tag>
        <tag>SMT</tag>
        <tag>NNJM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Learning with PyTorch(60 Minute)]]></title>
    <url>%2F2018%2F06%2F09%2FPytorch-Deep-Learning-with-PyTorch-A-60-Minute-Blitz%2F</url>
    <content type="text"><![CDATA[Deep Learning with PyTorch:A 60 Minute Blitz 一、PyTorch 是什么它是一个基于Python的科学计算包，目标用户有两类： 为了使用GPU来替代numpy。 一个深度学习援救平台：提供最大的灵活性和速度。 开始张量（Tensors)张量类似于Numpy的ndarrays，不同之处在于张量可以使用GPU来加快计算。 from __future__ import print_function import torch 构建一个未初始化的5*3的矩阵： x = torch.empty(5, 3) print(x) 构建一个随机初始化的矩阵： x = torch.rand(5, 3) print(x) 构建一个以零填充且数据类型为long的矩阵： x = torch.zeros(5, 3, dtype=torch.long) print(x) 直接从数据构造张量： x = torch.tensor([5.5, 3])print(x) 也可以根据现有张量创建张量。这些方法将重用输入张量的属性，例如dtype，除非用户提供了新的值： x = x.new_ones(5, 3, dtype=torch.double) #new_* methods 获取大小 print(x) x = torch.randn_like(x, dtype=torch.float) # 重写dtype print(x) # 结果具有相同的大小 获取张量大小： print(x.size()) 注意：torch.Size实际上是一个元组，所以它支持所有的元组操作。 操作（Operations）张量上的操作有多重语法形式，下面我们一加法为例进行讲解。 加法：语法1 print(&quot;x: &quot;, x) y = torch.rand(5, 3) print(x + y) 加法：语法2 print(&quot;x: &quot;, x) print(torch.add(x, y)) 加法：提供输出张量作为参数 result = torch.empty(5, 3) torch.add(x, y, out=result) print(result) 加法: in-place #adds x to y y.add_(x) print(y) 注意：任何使张量在原位发生变异的操作都是用， 例如: x.copy(y), x.t_(),都将会改变x。 可以任意使用标准Numpy-like索引： print(x) print(x[:, 1]) print(x[1, :]) print(x[2, 4]) 调整大小(Resizing)：如果您想调整大小/重塑张量，可以使用torch.view x = torch.randn(4, 4) y = x.view(16) z = x.view(-1, 8) print(x.size(), y.size(), z.size()) 如果有一个单元张量(a one element tensor)，请使用.item（）将该值作为Python数字来获取 x = torch.rand(1) print(x) print(x.item()) 这里描述了100+张量操作，包括转置，索引，切片，数学运算，线性代数，随机数等。 NumPy Bridge把一个torch张量转换为numpy数组或者反过来都是很简单的。 Torch张量和numpy数组将共享潜在的内存，改变其中一个也将改变另一个。 将Torch张量转换成一个NumPy数组 ： &gt;&gt;&gt; a = torch.ones(5) &gt;&gt;&gt; print(a) Out: tensor([ 1., 1., 1., 1., 1.]) &gt;&gt;&gt; b = a.numpy() &gt;&gt;&gt; print(b) Out: [1. 1. 1. 1. 1.] numpy数组的值如何在改变？ a.add_(1) print(a) print(b) 把NumPy数组转换成Torch张量：看看改变numpy数组如何自动改变torch张量。 import numpy as np a = np.ones(5) b = torch.from_numpy(a) np.add(a, 1, out=a) print(a) print(b) 所有在CPU上的张量，除了字符张量，都支持在numpy之间转换。 CUDA 张量使用 .to 函数可以将张量移动到GPU上。 # let us run this cell only if CUDA is available # We will use ``torch.device`` objects to move tensors in and out of GPU if torch.cuda.is_available(): device = torch.device(&quot;cuda&quot;) # a CUDA device object y = torch.ones_like(x, device=device) x = x.to(device) z = x + y print(z) print(z.to(&quot;cpu&quot;, torch.double)) Out: tensor([ 0.5921], device=’cuda:0’)tensor([ 0.5921], dtype=torch.float64) Autograd: 自动求导（automatic differentiation）PyTorch中所有神经网络的核心是autograd包。我们首先简单介绍一下这个包,然后训练我们的第一个神经网络。 autograd包为张量上的所有操作提供了自动求导.它是一个运行时定义的框架,这意味着反向传播是根据你的代码如何运行来定义,并且每次迭代可以不同. 接下来我们用一些简单的示例来看这个包。 Tensortorch.Tensor是包的核心类，如果将其属性requires_grad设置为true,它开始跟踪它上面的所有操作。 当完成计算时可以调用.backward()并自动计算所有的梯度。该张量的梯度被计算放入搭配到.grad属性中。阻止跟踪历史的张量，可以通过调用.detch()将其从计算历史记录中分离出来，并防止跟踪将来的计算。为了防止跟踪历史记录（和使用内存），您还可以在 with torch.no_grad（）包装代码块。这在评估模型时特别有用，因为该模型可能具有requires_grad = True的可训练参数，但我们不需要梯度。 对自动求导的实现还有一个非常重要的类,即函数(Function) 张量（Tensor）和函数(Function)是相互联系的,并形成一个非循环图来构建一个完整的计算过程.每个变量有一个.grad_fn属性,它指向创建该变量的一个Function(用户自己创建的变量除外,它的grad_fn属性为None)。 如果你想计算导数,可以在一个张量上调用.backward().如果一个Tensor是一个标量(它只有一个元素值),你不必给backward()指定任何的参数,但是该Variable有多个值,你需要指定一个和该张量相同形状的的gradient参数(查看API发现实际为gradients参数)。 import torch # 创建一个张量，饼设置reuqires_grad=True来跟踪计算过程 x = torch.ones(2, 2, reuqires_grad=True) print(x) Out: tensor([[ 1., 1.], [ 1., 1.]]) 在张量上执行操作: y = x + 2 print(y) Out: tensor([[ 3., 3.], [ 3., 3.]]) 因为y是通过一个操作创建的,所以它有grad_fn,而x是由用户创建,所以它的grad_fn为None. print(y.grad_fn) Out: &lt;AddBackward0 object at 0x0000020D2A5CC048&gt; 在张量y上执行更多操作： z = y * y * 3 out = z.mean() print(&quot;z : &quot;, z, &quot;, out: &quot;, out) Out: z : tensor([[ 27., 27.], [ 27., 27.]]) , out: tensor(27.) .requires_grad_（…）按位（in-place）更改现有张量的requires_grad标志。如果没有给出，输入标志默认为True。 a = torch.randn(2, 2) a = ((a * 3) / (a - 0)) print(a.requires_grad) a.requires_grad_(True) print(a.requires_grad) b = (a * a).sum() print(b.grad_fn) 梯度现在我们来执行反向传播,因为out包含一个标量,out.backward()相当于执行out.backward(torch.tensor(1)): out.backward() # 输出out对x的梯度d(out)/d(x): print(&quot;x.grad: &quot;, x.grad) 输出： 你应该得到一个值全为4.5的矩阵，我们把out称为张量O。则 我们还可以用自动求导做更多有趣的事： x = torch.randn(3, requires_grad=True) y = x * 2 while y.data.norm() &lt; 1000: y = y * 2 print(y) 输出： gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)y.backward(gradients) print(x.grad) 还可以通过使用torch.no_grad（）包装代码块来停止autograd跟踪在张量上的历史记录，其中require_grad = True： print(x.requires_grad) print((x ** 2).requires_grad) with torch.no_grad(): print((x ** 2 ).requires_grad) 输出： Documentation of autograd and Function is at http://pytorch.org/docs/autograd 神经网络（Neural Networks）可以使用torch.nn包来构建神经网络。我们已知道autograd包,nn包依赖autograd包来定义模型并求导.一个nn.Module包含各个层和一个faward(input)方法,该方法返回output。 例如,我们来看一下下面这个分类数字图像的网络。convnet 这是一个简单的前溃神经网络，它接受一个输入，然后一层接着一层的输入，直到最后得到结果。 神经网络的典型训练过程如下： 定义神经网络模型。它有一些可学习的参数（或者权重）； 在输入数据集上迭代 通过神经网络处理输入，主要体现在网络的前向传播; 计算损失（输出结果和正确值的差距大小） 将梯度反向传播回网络的参数，反向传播求梯度。 根据梯度值更新网络的参数，主要使用如下简单的更新原则： weight = weight - learning_rate * gradient 定义网络Let’s define this network: import torch import torch.nn as nn import torch.nn.functional as F Class Net(nn.Module): def __init__(self): # super就是在子类中调用父类方法时用的。 super(Net, self).__init__() # 对继承自父类的属性进行初始化。而且是用父类的初始化方法来初始化继承的属性。也就是说，子类继承了父类的所有属性和方法，父类属性自然会用父类方法来进行初始化。当然，如果初始化的逻辑与父类的不同，不使用父类的方法，自己重新初始化也是可以的。 # 1 input image channel, 6 output channels, 5*5 square convution # kernel self.comv1 = nn.Conv2d(1, 6, 5) self.comv2 = nn.Conv2d(6, 16, 5) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv1(x),(2, 2)) # if the size is a square you can only specify a single number x = F.max_pool2d(F.relu(self.conv2(x), 2) x = view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] num_features = 1 for s in size: num_features *= s return num_features net = Net() print(net) 输出： learn more about the network：在pytorch中只需要定义forward函数即可，backward函数会在使用autograd时自动创建（其中梯度是计算过的）。可以在forward函数中使用Tensor的任何操作。 net.aprameters()会返回模型中可学习的参数。 params = list(net.parameters()) print(&apos;可学习参数的个数：&apos;, len(params)) #print(&quot;可学习的参数：&quot;, params) print(params[0].size()) # conv1&apos;s的权值 for param in params: print(param.size()) 以上代码段实现将该神经网络的可学习参数都放到params中,并且输出了第一层conv的参数大小.输出： 注意：我们来尝试一个3232的随机输入，这个网络（LeNet）期望的输入大小是3232。如果使用的是MINIST数据集来训练这个网络，请把数据集中图片大小调整到32*32。 input = torch.randn（1，1，32，32） print(&quot;input: &quot;, input) out = net(input) print(&quot;out: &quot;, out) 输出： 把所有参数的梯度缓存区清零，然后进行随机梯度的的反向传播。 net.zero_grad() out.backward(torch.randn(1, 10)) Note: torch.nn只支持小批量输入（mini-batches）。整个torch.nn包仅支持作为最小样本量的输入，而不支持单个样本。 例如，nn.Conv2d只接受一个四维张量（nSamples nChannels Height Width），即样本数通道数高度宽度。 如果你有单个样本,只需使用input.unsqueeze(0)来添加一个虚假的批量维度. 在继续之前,我们回顾一下到目前为止见过的所有类。 回顾: torch.Tensor - 一个支持autograd等操作（比如banckward()）的多维数组，也支持梯度w、r、t等张量。 nn.Module - 神经网络模块。封装参数的便捷方式，移动到GPU运行，导出，加载等。 nn.Parameters - 一种张量，当作为属性赋值给一个模块（Module）时,能被自动注册为一个参数。 autograd.Function - 实现一个自动求导操作的前向和反向定义,每个张量操作至少创建一个函数节点，该节点连接到创建Tensor并对其历史进行编码的函数。 以上内容： 定义一个神经网络 处理输入和调用backward 剩下的内容: 计算损失值 更新神经网络的权值 损失函数（Loss Function）一个损失函数接受一对（output， target）作为输入(output为网络的输出,target为实际值),，计算一个值来评估网络的输出和目标值（实际值）相差多少。 在nn包中有几种不同的损失函数。一个简单的损失函数是:nn.MSELoss,它计算的是网络的输出和目标值之间的均方误差。 例如： output = net(input) target = torch.arrange(1, 11) # 例如，虚拟目标 target = target.view(1, -1)# 使其与输出(output)形状相同 criterion = nn.MSELoss() loss = criterion(output, target) print(loss) 输出： 现在，如果反向跟踪loss,用它的属性.grad_fn，你会的到下面这样的一个计算图： 所以, 当调用loss.backward(),整个图与w、r、t的损失不同,图中所有变量（其requres_grad=True）将拥有.grad变量来累计他们的梯度. 为了说明,我们反向跟踪几步: print(loss.grad_fn) # MSELoss print(loss.grad_fn.next_functions[0][0]) # Linear print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLU 输出： 反向传播（Backprop）为了反向传播误差,我们所需做的是调用loss.backward().你需要清除已存在的梯度,否则梯度将被累加到已存在的梯度。 现在，我们将调用loss.backward(),并查看conv1层的偏置项在反向传播前后的梯度。 net.zero_grad()# zeroes the gradient buffers of all parameters print(&apos;conv1.bias.grad before backward&apos;) print(net.conv1.bias.grad) loss.backward() print(&apos;conv1.bias.grad after backward&apos;) print(net.conv1.bias.grad) 输出： 现在我们也知道了如何使用损失函数。 神经网络包包含各种深度神经网络的构建模块和损失函数。完整的文档列表在这里。 接下来是更新权重。 更新权重最简单的更新权重的方法是：随机梯度下降（SGD）。 weight = weight - learning_rate * gradient 可以通过以下代码实现梯度更新： learning_rate = 0.01 for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) 当使用神经网络时，有几种不同的权重更新方法，例如有SGD,Nesterov-SGD,Adam,RMSProp等。为了能够更好地使用这些方法，Pytorch提供了一个小工具包：torch.optim来实现上述所说的更新方法。使用起来也很简单，代码如下： import torch.optim as optim # create your optimizer optimizer = optim.SGD(net.parameters(), lr=0.01) # in your training loop: optimizer.zero_grad() # zero the gradient buffers output = net(input) loss = criterion(output, target) loss.backward() optimizer.step() # Does the update 训练分类器（Training a classifier）What about Data?一般来说，在处理图像、文本、音频或者视频数据时可以使用标准的python包把数据加载成一个numpy数组， 然后再把这个数组转换成一个 torch.*Tensor。 对于图像来说，可以使用Pillow、OpenCV等包。 对于音频来说，可以使用scipy和librosa包。 对于文本来说，无论是原始的Python还是基于Cython的加载，或者NLTK和SpaCy都是有用的 特别是对于视觉，我们已经创建了一个名为torchvision的软件包，它具有常用数据集的数据加载器，如Imagenet，CIFAR10，MNIST等，以及图像数据转换器，即torchvision.datasets 和 torch.utils.data.DataLoader。 这提供了巨大的便利并避免了编写样板代码。 本例中我们将使用CIFAR-10数据集。它有以下几类： ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’。在CIFAR-10数据集中的数据大小是33232，例如尺寸为32x32像素的3通道彩色图像。 训练一个图像分类器 使用torchvision加载并归一化（normalizing）CIFAR10训练和测试数据集。 定义一个卷积神经网络 定义一个损失函数 在训练集上训练网络 在测试集上测试数据 加载和归一化CIFAR10 torchvison能够很简单的加载CIFAR10。 import torch import torchvision import torchvision.transforms as transforms torchvision数集的输出是范围为[0, 1]的PILImage图像。我们将其转换为归一化范围[-1, 1]的张量。 transform = transform.Compose([transform])]]></content>
      <categories>
        <category>Pytorch，Tutorials</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Adapting Grammatical Error Correction Based on the Native Language of Writers with Neural Network Joint Models]]></title>
    <url>%2F2018%2F06%2F04%2FAdapting-Grammatical-Error-Correction-Based-on-the-Native-Language-of-Writers-with-Neural-Network-Joint-Models%2F</url>
    <content type="text"><![CDATA[Adapting Grammatical Error Correction Based on the Native Language of Writers with Neural Network Joint Models本篇论文： 采用一个使用L1-specific学习者文本的NNJM（神经网络联合模型），并把它作为一个feature整合到一个基于GEC的统计机器翻译系统（解码器）中。本文的两点贡献： 这是第一个使用SMT方法并覆盖所有错误类型的工作来对GEC执行基于L1的自适应 我们引入了一种新的NNJM适应方法，并证明该方法可以处理比一般域数据小得多的域内数据。 适应（Adaptation）是通过使用训练在一般域数据上的未适应的NNJM来完成的。使用自归一化的对数似然目标函数作为起点，使用较小的L1-specific的域内数据进行后续迭代训练，并使用包含Kullback-Leibler (KL)离散正则项的修正目标函数。 摘要尽管第一语言(The Native Language,L1)对第二语言(The Second Language,L2)的写作有显著的影响，但是基于作者母语（L1)的适应（Adaptation）是 语法纠错(GEC)任务仍未充分探索的一个重要方面。本文采用神经网络联合模型(神经网络联合模型,NNJM)，使用L1-specific的学习者文本，将其集成到基于统计机器翻译(SMT)的GEC系统中。具体地说，我们针对一般学习者文本(general learner text，不是L1-specific的)训练NNJM，然后再使用Kullback-Leibler divergence正则化目标函数训练L1-specific的数据，以保持模型的泛化。我们将这个调整后的NNJM作为一个基于SMT的英语GEC系统的功能，并表明该系统在L1中文、俄语和西班牙语作者的英语文本上获得了显著的F0.5。 为什么考虑 L1-specific 学习者文本？主要是L1背景不同，学习第二语言时有不同的影响，也就是L1和L2之间的跨语言影响。 芬兰的英语学习者：过度概括了介词‘in’的使用。 例如：“When they had escaped in the police car they sat under the tree.”这个句子中的介词&quot;in&quot; 应该为 “from” 。 中国的英语学习者：由于汉语中没有动词形态变化，所以在书写英语时经常会出现动词时态和动词形式错误。 第一语言对第二语言写作的跨语言影响是一个非常复杂的现象，学习者所犯的错误不能直接归因于两种语言的相似或不同。 学习者似乎遵循着两个互补的原则（Ortega 2009）:第一语言中起作用的可能在第二语言中起作用，因为人类语言基本上是相似的;但如果听起来太像L1，那么在L2中可能就行不通了。 因此本文采用数据驱动（data-driven）的方法对这些影响因素进行建模,并使用具有相同L1背景的作者撰写的L2文本对GEC系统进行调整。 Why SMT ?GEC中两个最常用的方法是：分类方法（the classification approach）和 统计机器翻译方法（the statistical machine translation approach）。 SMT的优势： SMT方法把不合语法的文本转换成格式良好的文本的学习文本转换的能力，使得它能够纠正各种各样的错误，包括复杂的错误，而这些错误是很难用分类方法（the classification approach）处理的，这也使得SMT成为GEC的流行范例。 SMT方法并不用于专门的错误类型建模，也不需要像解析（parsing）和词性标注(POS tagging)这样的语言分析。 NNJM –&gt; Neural Network Joint Model关于NNJM在论文Fast and Robust Neural Network Joint Models for Statistical MachineTranslation（Devlin et al.,2014） NNJM:通过一个源上下文窗口扩展NNLM（which augments the NNJM with a source context window）。该模型是纯词汇化(purely lexicalized)的，可以集成到任何MT的Decoder中。具体来说，该模型利用m-word源窗口扩展一个n-gram目标语言模型。和以往的联合模型不同，该模型能够很容易作为一个feature被整合到任何SMT解码器中。 NNJM近似地估计了以源句子S为条件的目标假设T的概率。遵循目标的标准n-gram LM分解，其中每个目标字ti都受前面的n- 1个目标字的制约。为了使这个模型成为一个联合模型，对源上下文向量 进行了条件分析： 每一个目标词ti都直接对应着一个在位置ai的源词，是在以ai为中心的m-word的源窗口。 这种从属（affiliation）概念源自单词对齐，但与单词对齐不同，每个目标单词必须与一个非空(non-NULL)源单词相关联。 中文-英语平行句子的NNJM上下文模型例子如下图： 论文中采用的是n=4、m=11的15-gram LM 模型（神经网络语言模型能够优雅地扩展并利用任意大的上下文大小）。论文中的神经网络结构与Bengio et.al等人的前溃神经网络语言模型结构基本相似，如下图。 NNJM中的神经网络结构NNJM中的神经网络架构与Bengio et al.(2003)所描述的原始前馈NNLM体系结构（feed-forward NNJM architecture）几乎相同。隐藏层大小、词汇表大小和源窗口大小选择了这些值: 由于NNJM使用的是一个固定窗口的上下文，所以很容易将其整合到SMT解码器框架中，实验结果也证明了这样提升了SMT-based GEC的性能。 A MT Framework For GEC本文中将GEC视为从一个可能错误的输入句子到一个纠正句子的翻译过程。框架设计细节： 采用一个基于短语的SMT系统–Moses框架,它主要是通过一个对数线性模型来找到最佳假设 T*： SMT中两个主要部分：翻译模型(TM)和语言模型（LM）。 TM: 主要负责生成假设T（通常是短语表），使用并行数据（即，学习者写入的句子（源数据）及其相应的校正句子（目标数据））进行训练。还使用正向和反向短语翻译概率和词汇权重等特征对假设进行评分，从而选出最佳假设T*。 LM: 在格式良好的文本上进行驯良从而保证输出的流畅性。用MERT计算特征权重，用开发集优化度量。 由于NNJM有依赖于固定上下文的前馈结构，所以很容易将其作为一个feature整合到SMT解码器框架中。特征值由logP(T|S)给出，这个logP(T|S)是给出上下文的假设T中每个单词的对数概率总和。 上下文hi由n-1个之前的目标词和围绕与目标词ti对齐的源词的m个源词组成。 神经网络输出层的每个维度(Chollampatt et al.， 2016)给出了给定上下文h的输出词汇表中单词t出现的概率。 神经网络中的参数包括权值、偏差和嵌入矩阵都是用带随机梯度下降反向传播进行训练，损失函数使用的是与Devlin等所用(2014)相似的自归一项的对数似然目标函数。 KL Divergence Regularized Adaptation in-domain data： 由L1-specific的作者所写的错误文本及其相应的修正文本组成。 这种自适应训练是使用具有正则化项K的修正目标函数来完成的，该函数用于最小化pGD(y|h)与网络输出概率分布p(y|h)之间的KL离散度。 K将防止训练期间估计的概率分布偏离通用域NNJM的分布。 最终的自适应步骤的目标函数是L和K中的项的线性组合。 数据和评价训练数据处理和来源：来源： 新加坡国立大学学生英语语料库（the NUSCorpus of Learner English (NUCLE) (Dahlmeieret al., 2013)） Lang-8学习者语料库（the Lang-8 Learner Corpora v2(Mizumoto et al., 2011)），Lang-8提取的是只学习英语的学习者的文本。 处理： 用语言识别工具langid.py（https://github.com/saffsd/langid.py）来获取纯净的英语句子 删除Lang-8中的噪声源-目标句子对（ noisy sourcetarget sentence pairs），即其中源句子和目标句子长度的比率在[0.5,2.0]之外的句子对，或者它们的单词重叠比率小于0.2的句子对。 删除NUCLE和Lang-8中源句子或目标句子超过80个单词的句子对。 预处理后训练数据的统计见Table1： 基于Lang-8中提供的L1信息获取的自适应L1-specific 域内信息。 每一个L1，它的域外数据是由除L1-specific域内数据在外的联合训练数据（CONCAT）中获取的。 开发测试集从公共可用的 CLC-FCE 语料库中获取。FCE语料库包含由1,244位不同候选人在1,2000年和2001年参加剑桥ESOL英语第一证书（FCE）考试所写的1,244个脚本。根据脚本数量分成数量大致相等的两部分作为开发集和测试集。 Evaluation 实验结果Baseline SMT-based GEC sysytem用Moses(Version 3)构建所有基于SMT的GEC系统。 NNJM Adaptation :用全部的训练数据训练10个epoch。源上下文窗口大小设置为5，目标上下文窗口大小设置为4，形成一个(5+5)-gram的联合模型。使用一个mini-batch大小为128、学习率为0.1的随机梯度下降（SGD）进行训练。 Comparison to Other Adaptation TechniquesEffect of Adaptation Data关于正则化的影响 Evaluation on Benchmark Dataset 讨论和错误分析 相关工作 HOO(Helping Our Own)和 CoNll共享任务 使得GEC变得普及流行。 GEC已发表的相关工作旨在构建针对具体错误类型分类器和基于规则的系统，并将其结合构建成混合系统（hybrid systems）。 L1和L2间的跨语言影响主要用于母语识别任务，还用于类型学预测和ESL数据的预测误差分布。 最近，针对GEC提出了端对端（end-to-end）神经机器翻译框架，显示出了具有竞争力的结果。 本文中利用SMT方法和神经网络联合模型的优点，将基于L1背景的作者的NNJM模型整合到SMT框架中。通过KL离散正则化自适应来避免在较小的域内数据中的过拟合。 SMT中其它调节技术包括混合建模（mixture modeling）和可选的解码路径（alternative decoding paths）。]]></content>
      <categories>
        <category>Papers</category>
        <category>GEC</category>
      </categories>
      <tags>
        <tag>论文阅读笔记</tag>
        <tag>GEC</tag>
        <tag>NNJM</tag>
        <tag>Papers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keras实现CRF层遇到的问题：3D张量的报错]]></title>
    <url>%2F2018%2F04%2F25%2Fkeras%E5%AE%9E%E7%8E%B0CRF%E5%B1%82%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%9A3D%E5%BC%A0%E9%87%8F%E7%9A%84%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[keras实现CRF层遇到的问题：3D张量的报错问题：在使用CRF层构建模型进行训练时，总是报错： 下面是ValueError的完整信息： ValueError: Index out of range using input dim 2; input has only 2 dims for &apos;loss/crf_1_loss/strided_slice&apos; (op: &apos;StridedSlice&apos;) with input shapes: [?,?], [3], [3], [3] and with computed input tensors: input[3] = &lt;1 1 1&gt;. 是张量的shape问题。具体的原因还需要进一步查找，先标记下，五一假后解决。]]></content>
      <categories>
        <category>Keras</category>
      </categories>
      <tags>
        <tag>Keras</tag>
        <tag>CodingErrors</tag>
        <tag>CRF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4月17-4月23日-学习计划]]></title>
    <url>%2F2018%2F04%2F18%2F4%E6%9C%8817-4%E6%9C%8823%E6%97%A5-%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[4月18-4月23日 学习计划 18日 - 19日：调代码，CGED的position阶段，读论文 20日 - 21日：调代码，复现《A Nested Attention Neural Hybrid Model for Grammatical Error》实验。Correction 22日：参加中文信息学会沙龙-CGED，下午整理笔记，调试代码。整理一篇关于多分类多标签任务的博客(尽量)。 23日：调代码，看论文。今天公布CGED测试数据………………………….. 下周见]]></content>
      <categories>
        <category>学习计划</category>
      </categories>
      <tags>
        <tag>学习计划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3-25-3-31学习计划]]></title>
    <url>%2F2018%2F03%2F25%2F3-25-3-31%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[3.25-3.31 学习计划 学习tensorflow, 实现bilstm+crf模型 seq2seq+Attention 机制模型详解]]></content>
      <categories>
        <category>学习计划</category>
      </categories>
      <tags>
        <tag>学习计划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras之BiLSTM+CRF实现语法错误判断出现shape不符合问题]]></title>
    <url>%2F2018%2F03%2F23%2Fkeras%E4%B9%8BBiLSTM-CRF%E5%AE%9E%E7%8E%B0%E8%AF%AD%E6%B3%95%E9%94%99%E8%AF%AF%E5%88%A4%E6%96%AD%E5%87%BA%E7%8E%B0shape%E4%B8%8D%E7%AC%A6%E5%90%88%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[keras之BiLSTM+CRF实现语法错误判断出现shape不符合问题在用keras构建BiLSTM+CRF模型实现汉语语法错误诊断过程中，进行训练的时候总是报错： ValueError: Cannot feed value of shape (128, 1) for Tensor u&apos;crf_1_target:0&apos;, which has shape &apos;(?,?,?) 出现错误位置在： #bilstm layer bilstm_layer = Bidirectional(LSTM(hidden_dim, return_sequences=False)) model.add(bilstm_layer) print(&apos;bilstm_layer.input_shape:&apos;, bilstm_layer.input_shape) print(&apos;bilstm_layer.output_shape:&apos;, bilstm_layer.output_shape) drop_layer = Dropout(dropout_rate) model.add(drop_layer) dense = Dense(num_class) model.add(dense) print(&apos;drop_layer.input_shape:&apos;, drop_layer.input_shape) print(&apos;drop_layer.output_shape:&apos;, drop_layer.output_shape) time_layer = TimeDistributed(Dense(num_class)) model.add(time_layer) print(&apos;time_layer.input_shape:&apos;, time_layer.input_shape) print(&apos;time_layer.output_shape:&apos;, time_layer.output_shape) #加载CRF层 crf_layer = CRF(num_class, sparse_target=True) model.add(crf_layer) print(&apos;crf_layer.input_shape:&apos;, crf_layer.input_shape) print(&apos;crf_layer.output_shape:&apos;, crf_layer.output_shape) #pdb.set_trace() model.compile(loss=&apos;sparse_categorical_crossentropy&apos;, optimizer=&apos;sgd&apos;) BiLSTM是一个时序序列，添加层的参数设置中return_sequence值应为Fasle，若设置为True则每个时间步都会有一个输出值返回，设为False只会返回序列中最后一个时间步的输出值。其次在BiLSTM后接的CRF层也是时许序列，对应着的输出也是每个时间有输出，因此CRF的最终输出为（batch_size,timesteps,num_class）。而在本次任务中，只要求判断输入句子是否有语法错误，真值是一个shape为（batch_size,num_class）d的输出。还有time_layer = TimeDistributed(Dense(num_class))，其中TimeDistributed这个包装器将一个层应用于输入的每个时间切片，所以都是导致最后shape不符的原因，因此会出现上述错误。 解决方法：弃用CRF层和TimeDistributed层。发现不报错了……但目前还不知道怎么能让CRF和TimeDistributed层存在的情况下不报错，后续有了解决方法会有更新。 代码参考：用keras搭建bilstm crf]]></content>
      <categories>
        <category>Keras</category>
      </categories>
      <tags>
        <tag>Keras</tag>
        <tag>CodingErrors</tag>
        <tag>CRF</tag>
        <tag>BiLSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras序贯模型快速开始]]></title>
    <url>%2F2018%2F03%2F20%2FKeras%E5%BA%8F%E8%B4%AF%E6%A8%A1%E5%9E%8B%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B%2F</url>
    <content type="text"><![CDATA[Keras序贯模型快速开始re: 官方-中文文档序贯模型是多个网络层的线性堆叠。可以通过向Sequential模型传递一个layer的list来构造该模型： “&apos;Python from keras.models import Sequential from keras.layers import Dense,Activation model = Sequential([ Dense(32,units=784),Activation(&apos;relu&apos;),Dense(10),Activation(&apos;softmax&apos;), ]) 也可以通过.add()方法一个个的添加layer到模型中 “&apos;Python model = Sequential() model.add(Dense(32,input_shape=(784,)) model.add(Actiovation(&apos;relu&apos;)) 指定输入数据的shape模型需要知道输入数据的shape，因此Sequential的第一层需要接受一个关于输入数据shape的参数，后面各个层则可以自动的推导出中间数据的shape，因此不需要为每个层都指定这个参数。有几种方法来为第一层指定输入数据的shape： 传递一个input_shape的关键字参数给第一层，input_shape是一个tuple类型的数据，其中也可以填入None，如果填入None则表示此位置可能是任何正整数。数据的batch大小不应包含在其中。 有些2D层 ，如Dense,支持通过指定其输入维度input_dim来隐含的指定输入数据shape,是一个Int类型的数据。一些3D的时域层支持通过参数input_dim和input_length来指定输入shape。 “&apos;Python model = Sequential() model.add(Dense(32, input_dim=784)) 如果你需要为输入指定一个固定大小的batch_size（常用于stateful RNN网络），可以传递batch_size参数到一个层中，例如你想指定输入张量的batch大小是32，数据shape是（6，8），则你需要传递batch_size=32和input_shape=(6,8)。 “&apos;Python model = Sequential() model.add(Dense(32, input_shape=(784,))) 编译在训练模型之前，我们需要通过compile来对学习过程进行配置。compile接收三个参数： “&apos;Python # For a multi-class classification problem model.compile(optimizer=&apos;rmsprop&apos;, loss=&apos;categorical_crossentropy&apos;, metrics=[&apos;accuracy&apos;]) # For a binary classification problem model.compile(optimizer=&apos;rmsprop&apos;, loss=&apos;binary_crossentropy&apos;, metrics=[&apos;accuracy&apos;]) # For a mean squared error regression problem model.compile(optimizer=&apos;rmsprop&apos;, loss=&apos;mse&apos;) # For custom metrics import keras.backend as K def mean_pred(y_true, y_pred): return K.mean(y_pred) model.compile(optimizer=&apos;rmsprop&apos;, loss=&apos;binary_crossentropy&apos;, metrics=[&apos;accuracy&apos;, mean_pred]) 训练Keras以Numpy数组作为输入数据和标签的数据类型。训练模型一般使用fit函数，该函数的详情见这里。下面是一些例子。 “&apos;Python # For a single-input model with 2 classes (binary classification): model = Sequential() model.add(Dense(32, activation=&apos;relu&apos;, input_dim=100)) model.add(Dense(1, activation=&apos;sigmoid&apos;)) model.compile(optimizer=&apos;rmsprop&apos;, loss=&apos;binary_crossentropy&apos;, metrics=[&apos;accuracy&apos;]) # Generate dummy data import numpy as np data = np.random.random((1000, 100)) labels = np.random.randint(2, size=(1000, 1)) # Train the model, iterating on the data in batches of 32 samples model.fit(data, labels, epochs=10, batch_size=32) 第二个例子 : “` # For a single-input model with 10 classes (categorical classification): model = Sequential() model.add(Dense(32, activation=&apos;relu&apos;, input_dim=100)) model.add(Dense(10, activation=&apos;softmax&apos;)) model.compile(optimizer=&apos;rmsprop&apos;, loss=&apos;categorical_crossentropy&apos;, metrics=[&apos;accuracy&apos;]) # Generate dummy data import numpy as np data = np.random.random((1000, 100)) labels = np.random.randint(10, size=(1000, 1)) # Convert labels to categorical one-hot encoding one_hot_labels = keras.utils.to_categorical(labels, num_classes=10) # Train the model, iterating on the data in batches of 32 samples model.fit(data, one_hot_labels, epochs=10, batch_size=32)]]></content>
      <categories>
        <category>Keras</category>
      </categories>
      <tags>
        <tag>Keras</tag>
        <tag>序贯模型</tag>
      </tags>
  </entry>
</search>
