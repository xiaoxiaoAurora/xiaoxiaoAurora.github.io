<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>XiaoXiao</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xiaoxiaoaurora.github.io/"/>
  <updated>2019-05-14T06:38:30.469Z</updated>
  <id>https://xiaoxiaoaurora.github.io/</id>
  
  <author>
    <name>Xiao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Semantic Role Labeling(语义角色标注)</title>
    <link href="https://xiaoxiaoaurora.github.io/2019/05/14/Semantic-Role-Labeling-%E8%AF%AD%E4%B9%89%E8%A7%92%E8%89%B2%E6%A0%87%E6%B3%A8/"/>
    <id>https://xiaoxiaoaurora.github.io/2019/05/14/Semantic-Role-Labeling-语义角色标注/</id>
    <published>2019-05-14T06:23:08.000Z</published>
    <updated>2019-05-14T06:38:30.469Z</updated>
    
    <content type="html"><![CDATA[<h1 id="What-is-Semantic-Role-Labeling"><a href="#What-is-Semantic-Role-Labeling" class="headerlink" title="What is Semantic Role Labeling ?"></a>What is Semantic Role Labeling ?</h1><p>（<a href="https://blog.csdn.net/huhehaotechangsha/article/details/80463118" target="_blank" rel="noopener">from reference</a>）</p><h2 id="概念（from-reference）"><a href="#概念（from-reference）" class="headerlink" title="概念（from reference）"></a>概念（<a href="https://blog.csdn.net/huhehaotechangsha/article/details/80463118" target="_blank" rel="noopener">from reference</a>）</h2><ul><li>自然语言分析技术大致分为三个层面：词法分析、句法分析和语义分析。语义角色标注是实现浅层语义分析的一种方式。<strong>在一个句子中，谓词是对主语的陈述或说明，指出“做什么”、“是什么”或“怎么样，代表了一个事件的核心，跟谓词搭配的名词称为谓元（论元）</strong>。<strong>语义角色是指谓元在动词所指事件中担任的角色</strong>。主要有：施事者（Agent）、受事者（Patient）、客体（Theme）、经验者（Experiencer）、受益者（Beneficiary）、工具（Instrument）、处所（Location）、目标（Goal）和来源（Source）等</li></ul><h2 id="任务介绍"><a href="#任务介绍" class="headerlink" title="任务介绍"></a>任务介绍</h2><p>语义角色标注(SRL)<strong>任务是指以句子的谓词为中心，不对句子所包含的语义信息进行深入分析，</strong>只分析句子中各成分与谓词之间的关系，即句子的谓词（Predicate）- 论元（Argument）结构**，并用语义角色来描述这些结构关系，是许多自然语言理解任务（如信息抽取，篇章分析，深度问答等）的一个重要中间步骤。在研究中一般都假定谓词是给定的，所要做的就是找出给定谓词的各个论元和它们的语义角色。</p><ul><li><p>Motivation:  From Sentences to Propositions(抽取句子的主干意义)</p></li><li><p>将语言<strong>信息结构化</strong>，方便计算机理解句子中蕴含的语义信息。</p><blockquote><p>语义角色标注 (Semantic Role Labeling, SRL) 是一种浅层的语义分析技术，标注句子中某些短语为<strong>给定谓词</strong>的论元 (语义角色) ，如施事、受事、时间和地点等。其能够对问答系统、信息抽取和机器翻译等应用产生推动作用。</p></blockquote></li><li><p>语义标注的不足之处</p><ul><li>仅仅对于特定谓词进行论元标注，那多谓词呢？没有涉及到。</li><li>不会补出句子所省略的部分语义。信息有所缺失。</li></ul></li><li><p>核心的语义角色: A0-5 六种，A0 通常表示动作的施事，A1通常表示动作的影响等，A2-5 根据谓语动词不同会有不同的语义含义。</p></li><li><p>附加的15种语义角色</p><p><img src="https://i.loli.net/2019/05/14/5cda20e8d1bf745091.jpg" alt></p></li></ul><h2 id="标注"><a href="#标注" class="headerlink" title="标注"></a>标注</h2><ul><li>传统方法<ul><li>依赖句法分析的结果进行。因为句法分析包括短语结构分析、浅层句法分析、<strong>依存关系分析</strong>，所以语义角色标注也可以按照此思路分类。</li><li>基于短语结构树的语义角色标注方法</li><li>基于浅层句法分析结果的语义角色标注方法</li><li><strong>基于依存句法分析结果的语义角色标注方法</strong></li><li>基于特征向量的 SRL</li><li>基于最大熵分类器的 SRL</li><li>基于核函数的 SRL</li><li>基于条件随机场的 SRL</li></ul></li><li><strong>统一标注的过程：句法分析-&gt;候选论元剪除-&gt;论元识别-&gt;论元标注-&gt;语义角色标注结果</strong><ul><li>论元剪除：在较多候选项中去掉肯定不是论元的部分（span）</li><li>论元识别：一个二值分类问题，即：是论元和不是论元</li><li>论元标注：一个多值分类问题</li></ul></li></ul><p><img src="https://i.loli.net/2019/05/14/5cda234db961121245.jpg" alt="短语结构分析"></p><blockquote><p>传统的SRL系统大多建立在句法分析基础之上，通常包括5个流程：<img src="https://img-blog.csdn.net/20170922120117082?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbWluZ3phaTYyNA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><ol><li>构建一棵句法分析树，例如，上图是对上面例子进行依存句法分析得到的一棵句法树。 </li><li>从句法树上识别出给定谓词的候选论元。 </li><li>候选论元剪除；一个句子中的候选论元可能很多，候选论元剪除就是从大量的候选项中剪除那些最不可能成为论元的候选项。 </li><li>论元识别：这个过程是从上一步剪除之后的候选中判断哪些是真正的论元，通常当做一个二分类问题来解决。</li><li>对第4步的结果，通过多分类得到论元的语义角色标签。可以看到，句法分析是基础，并且后续步骤常常会构造的一些人工特征，这些特征往往也来自句法分析。</li></ol></blockquote><h2 id="如何设计分类问题的特征？"><a href="#如何设计分类问题的特征？" class="headerlink" title="如何设计分类问题的特征？"></a>如何设计分类问题的特征？</h2><ul><li>谓词本身、</li><li>短语结构树路径、</li><li>短语类型、</li><li>论元在谓词的位置、</li><li>谓词语态、</li><li>论元中心词、</li><li>从属类别、</li><li>论元第一个词和最后一个词、</li><li>组合特征</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;What-is-Semantic-Role-Labeling&quot;&gt;&lt;a href=&quot;#What-is-Semantic-Role-Labeling&quot; class=&quot;headerlink&quot; title=&quot;What is Semantic Role Labeling ?
      
    
    </summary>
    
      <category term="NLP" scheme="https://xiaoxiaoaurora.github.io/categories/NLP/"/>
    
      <category term="Semantic Role Labeling" scheme="https://xiaoxiaoaurora.github.io/categories/NLP/Semantic-Role-Labeling/"/>
    
    
      <category term="NLP" scheme="https://xiaoxiaoaurora.github.io/tags/NLP/"/>
    
      <category term="Semantic Role Labeling" scheme="https://xiaoxiaoaurora.github.io/tags/Semantic-Role-Labeling/"/>
    
      <category term="SRL" scheme="https://xiaoxiaoaurora.github.io/tags/SRL/"/>
    
      <category term="语义角色标注" scheme="https://xiaoxiaoaurora.github.io/tags/%E8%AF%AD%E4%B9%89%E8%A7%92%E8%89%B2%E6%A0%87%E6%B3%A8/"/>
    
      <category term="知识点" scheme="https://xiaoxiaoaurora.github.io/tags/%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    
  </entry>
  
  <entry>
    <title>Semantic Role Labeling for Learner Chinese--the Importance of Syntactic Parsing and L2-L1 Parallel Data</title>
    <link href="https://xiaoxiaoaurora.github.io/2019/05/14/Semantic%20Role%20Labeling%20for%20Learner%20Chinese/"/>
    <id>https://xiaoxiaoaurora.github.io/2019/05/14/Semantic Role Labeling for Learner Chinese/</id>
    <published>2019-05-13T16:00:00.000Z</published>
    <updated>2019-05-14T06:38:10.248Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Semantic-Role-Labeling-for-Learner-Chinese–the-Importance-of-Syntactic-Parsing-and-L2-L1-Parallel-Data"><a href="#Semantic-Role-Labeling-for-Learner-Chinese–the-Importance-of-Syntactic-Parsing-and-L2-L1-Parallel-Data" class="headerlink" title="Semantic Role Labeling for Learner Chinese–the Importance of Syntactic Parsing and L2-L1 Parallel Data"></a>Semantic Role Labeling for Learner Chinese–the Importance of Syntactic Parsing and L2-L1 Parallel Data</h1><p><img src="https://i.loli.net/2019/05/14/5cda1b19d65b334883.jpg" alt>  </p><p><a href="https://arxiv.org/pdf/1808.09409.pdf" target="_blank" rel="noopener">【论文地址】</a></p><p>本文阅读目的：了解SRL标注工作及在现有parsing model上的评估与分析，作为Dependency SRL研究工作的基础知识学习。</p><h2 id="题目与摘要"><a href="#题目与摘要" class="headerlink" title="题目与摘要"></a>题目与摘要</h2><h3 id="本文针对什么任务？任务简要介绍下。"><a href="#本文针对什么任务？任务简要介绍下。" class="headerlink" title="本文针对什么任务？任务简要介绍下。"></a>本文针对什么任务？任务简要介绍下。</h3><ul><li><p><strong>本文主要以面向汉语学习者的中介语(interlanguage， L2)的语义分析为研究对象，进行语义角色标注(Semantic Role Labeling, SRL)。</strong></p></li><li><p><strong>语义角色标注</strong>（<a href="https://blog.csdn.net/huhehaotechangsha/article/details/80463118" target="_blank" rel="noopener">from reference</a>）</p><ul><li>自然语言分析技术大致分为三个层面：词法分析、句法分析和语义分析。语义角色标注是实现浅层语义分析的一种方式。<strong>在一个句子中，谓词是对主语的陈述或说明，指出“做什么”、“是什么”或“怎么样，代表了一个事件的核心，跟谓词搭配的名词称为谓元（论元）</strong>。<strong>语义角色是指谓元在动词所指事件中担任的角色</strong>。主要有：施事者（Agent）、受事者（Patient）、客体（Theme）、经验者（Experiencer）、受益者（Beneficiary）、工具（Instrument）、处所（Location）、目标（Goal）和来源（Source）等</li><li><strong>语义角色标注(SRL)</strong>任务是指以句子的谓词为中心，不对句子所包含的语义信息进行深入分析，<strong>只分析句子中各成分与谓词之间的关系，即句子的谓词（Predicate）- 论元（Argument）结构</strong>，并用语义角色来描述这些结构关系，是许多自然语言理解任务（如信息抽取，篇章分析，深度问答等）的一个重要中间步骤。在研究中一般都假定谓词是给定的，所要做的就是找出给定谓词的各个论元和它们的语义角色。</li><li>Motivation:  From Sentences to Propositions(抽取句子的主干意义)</li><li>详细介绍</li></ul></li></ul><h3 id="本文发现了什么问题？该文大体是怎么解决的？解决得如何？"><a href="#本文发现了什么问题？该文大体是怎么解决的？解决得如何？" class="headerlink" title="本文发现了什么问题？该文大体是怎么解决的？解决得如何？"></a>本文发现了什么问题？该文大体是怎么解决的？解决得如何？</h3><p><strong>本文工作首先为一组学习者文本手工标注语义角色，以获得进行自动SRL的黄金标准。并基于这些新标注的数据，在三个现有的SRL系统（the PCFGLA-parser-based, neural-parserbased, neural-syntax-agnostic systems ）上进行评估来衡量SRL在学习者汉语文本上的解析性能。</strong>  针对评估结果作者提出了两点不太明显的结论：</p><p>（1）L1句子训练的系统对L2数据的处理能力较差；</p><p>（2）<strong>两种parser-based系统在解析L1数据到L2数据的时候性能下降要小得多，这说明了SRL中句法分析对于跨语言的重要性。</strong></p><p>针对以上结论，<strong>作者提出了一种面向大规模L2-L1平行数据的agreement-based model来探索语义一致性 (coherency ) 信息</strong>。同时也证明了这些信息对于提高L2的语篇学习水平是非常有效的。<strong>最终结果获得了72.06分的F-score，比最佳基线提高了2.02分。</strong></p><h3 id="解释下题目。题目起得如何？能概括内容并吸引人吗？"><a href="#解释下题目。题目起得如何？能概括内容并吸引人吗？" class="headerlink" title="解释下题目。题目起得如何？能概括内容并吸引人吗？"></a>解释下题目。题目起得如何？能概括内容并吸引人吗？</h3><p>简要明了：</p><ul><li>Semantic Role Labeling Learner Chinese 指明是面向汉语中介语的语义角色标注工作</li><li>the Importance of Syntactic Parsing and L2-L1 Parallel Data 指明本文中心立意</li></ul><hr><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>学习者语言(中介语)是由第二语言学习者或外语学习者发展起来的一种个人习语，它可能保留了一些学习者母语的某些特征。</p><h3 id="这个任务以往是如何解决的？作者沿着哪条路径继续研究的？为什么？"><a href="#这个任务以往是如何解决的？作者沿着哪条路径继续研究的？为什么？" class="headerlink" title="这个任务以往是如何解决的？作者沿着哪条路径继续研究的？为什么？"></a>这个任务以往是如何解决的？作者沿着哪条路径继续研究的？为什么？</h3><ul><li><p>之前关于构建学习者语言的自动句法分析工作相当令人振奋(Nagata<br>and Sakaguchi, 2016) ，<strong>但是对于语义处理是如何执行的仍然是未知的，而将学习者语言(L2)解析为语义表示是对学习者语言进行各种深入分析的基础</strong>，例如作文自动评分。</p></li><li><p>因此，在本文中作者以语义角色标注(SRL)为案例任务，以汉语学习者为案例语言，研究中介语的语义分析。</p></li></ul><h3 id="目前存在什么问题？为什么？你觉得可能还存在什么其他问题？为什么？"><a href="#目前存在什么问题？为什么？你觉得可能还存在什么其他问题？为什么？" class="headerlink" title="目前存在什么问题？为什么？你觉得可能还存在什么其他问题？为什么？"></a>目前存在什么问题？为什么？你觉得可能还存在什么其他问题？为什么？</h3><p>本文从三个concern展开介绍自己的研究工作：</p><ul><li>（1）本文作者提出：在讨论计算系统之前，首先考虑语言能力和语言运用。并提出了以下疑问：</li></ul><p>​        <strong>人类能够准确地理解学习者的文本吗?或者更准确地说，以英语为母语的人能   在多大程度上理解语言学习者写的句子的意思?</strong> </p><blockquote><p>Can human robustly understand learner texts? Or to be more precise, to what extent, a native speaker can understand the meaning of a sentence written by a language learner? </p></blockquote><ul><li><strong>（2）作者提出的第二个关注点是如何通过计算机程序模拟人类的语义处理能力</strong>。</li><li><strong>（3）探索大规模的L2-L1并行语料对于增强SRL系统的潜力。</strong>提出利用L2-L1并行语料来增强学习者文本的NLP系统这一观点也是第一次提出。</li></ul><h3 id="该文准备如何解决这个问题？为什么可以这样解决？你觉得该文解决这个问题的方法如何？为什么？你觉得可以如何-或更好的解决这个问题？为什么？"><a href="#该文准备如何解决这个问题？为什么可以这样解决？你觉得该文解决这个问题的方法如何？为什么？你觉得可以如何-或更好的解决这个问题？为什么？" class="headerlink" title="该文准备如何解决这个问题？为什么可以这样解决？你觉得该文解决这个问题的方法如何？为什么？你觉得可以如何/或更好的解决这个问题？为什么？"></a>该文准备如何解决这个问题？为什么可以这样解决？你觉得该文解决这个问题的方法如何？为什么？你觉得可以如何/或更好的解决这个问题？为什么？</h3><ul><li>直觉上，以上问题（1）答案是积极的一面。为了验证这一点，作者<strong>根据 Chinese PropBank (CPB)的规范（针对L1的），标注了一些L2-L1平行句子的 predicate–argument (谓语-论元)结构</strong>。最终的标注一致度表明，这种规范对于L2的语言理解具有很高的鲁棒性。【此工作也是在进行Dependency parsing的标注工作模式】。<strong>在语义标注过程中，作者提出：可以重用为L1开发的语义标注规范Chinese PropBank来标注L2句子中的语义角色。</strong>    </li></ul><blockquote><p>During the course of semantic annotation, we find a non-obvious fact that we can re-use the semantic annotation specification, Chinese PropBank in our case, which is developed for L1. Only modest rules are needed to handle some tricky phenomena. This is quite different from syntactic treebanking for learner sentences, where defining a rich set of new annotation heuristics seems necessary</p></blockquote><ul><li>问题（1）中关于重用L1标注规范的可行性意味着可以用标准的CPB数据来训练一个SRL系统来处理学习者的文本。为了测试最优SRL算法的鲁棒性，论文在两种SRL框架进行了评估。<ul><li>传统的SRL系统：利用句法分析器和繁重的特性工程来获取语义角色的显式信息(explicit information), 使用了两个不同的解析器进行比较:<ul><li>1) the PCFGLA-based parser, viz. Berkeley parser (<a href="https://www.aclweb.org/anthology/P06-1055" target="_blank" rel="noopener">Petrov et al., 2006</a>)</li><li>2) a minimal span-based neural parser (<a href="https://arxiv.org/pdf/1705.03919.pdf" target="_blank" rel="noopener">Stern et al., 2017</a>) </li></ul></li><li>另一个SRL系统使用堆叠的BiLSTM隐式地捕获局部和非局部信息。</li></ul></li></ul><h3 id="列出该文贡献（该文自己觉得的）"><a href="#列出该文贡献（该文自己觉得的）" class="headerlink" title="列出该文贡献（该文自己觉得的）"></a>列出该文贡献（该文自己觉得的）</h3><p>1、基于CPB规范，构建了L2-L1平行语料的SRL语料库，经验证可以重用为L1开发的语义标注规范CPB来标注L2句子中的语义角色。(<a href="https://www.aclweb.org/anthology/C12-1053" target="_blank" rel="noopener">Feng et al., 2012</a>)</p><p>2、</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="整体介绍（主要是图）"><a href="#整体介绍（主要是图）" class="headerlink" title="整体介绍（主要是图）"></a>整体介绍（主要是图）</h3><h3 id="模型创新点"><a href="#模型创新点" class="headerlink" title="模型创新点"></a>模型创新点</h3><h3 id="（仅对要进一步跟进的paper）详细介绍模型，从输入到输出，输入矩阵维度，公式等"><a href="#（仅对要进一步跟进的paper）详细介绍模型，从输入到输出，输入矩阵维度，公式等" class="headerlink" title="（仅对要进一步跟进的paper）详细介绍模型，从输入到输出，输入矩阵维度，公式等"></a>（仅对要进一步跟进的paper）详细介绍模型，从输入到输出，输入矩阵维度，公式等</h3><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集及评价标准介绍"><a href="#数据集及评价标准介绍" class="headerlink" title="数据集及评价标准介绍"></a>数据集及评价标准介绍</h3><h3 id="Baseline介绍"><a href="#Baseline介绍" class="headerlink" title="Baseline介绍"></a>Baseline介绍</h3><h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><h3 id="你觉得这篇paper创新与贡献是（不一定如作者所说）？为什么？"><a href="#你觉得这篇paper创新与贡献是（不一定如作者所说）？为什么？" class="headerlink" title="你觉得这篇paper创新与贡献是（不一定如作者所说）？为什么？"></a>你觉得这篇paper创新与贡献是（不一定如作者所说）？为什么？</h3><h3 id="有没有进一步深入的价值？为什么？"><a href="#有没有进一步深入的价值？为什么？" class="headerlink" title="有没有进一步深入的价值？为什么？"></a>有没有进一步深入的价值？为什么？</h3><h3 id="列出该文弱点（或者是你觉得应该是什么问题，他解决的不好，你会如何解决？）"><a href="#列出该文弱点（或者是你觉得应该是什么问题，他解决的不好，你会如何解决？）" class="headerlink" title="列出该文弱点（或者是你觉得应该是什么问题，他解决的不好，你会如何解决？）"></a>列出该文弱点（或者是你觉得应该是什么问题，他解决的不好，你会如何解决？）</h3><h3 id="该文对你的启发是？"><a href="#该文对你的启发是？" class="headerlink" title="该文对你的启发是？"></a>该文对你的启发是？</h3><h3 id="列出其中有价值的需要进一步阅读的参考文献"><a href="#列出其中有价值的需要进一步阅读的参考文献" class="headerlink" title="列出其中有价值的需要进一步阅读的参考文献"></a>列出其中有价值的需要进一步阅读的参考文献</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Semantic-Role-Labeling-for-Learner-Chinese–the-Importance-of-Syntactic-Parsing-and-L2-L1-Parallel-Data&quot;&gt;&lt;a href=&quot;#Semantic-Role-Labe
      
    
    </summary>
    
      <category term="Papers" scheme="https://xiaoxiaoaurora.github.io/categories/Papers/"/>
    
      <category term="Semantic Role Labeling(SRL)" scheme="https://xiaoxiaoaurora.github.io/categories/Papers/Semantic-Role-Labeling-SRL/"/>
    
    
      <category term="Papers" scheme="https://xiaoxiaoaurora.github.io/tags/Papers/"/>
    
      <category term="Semantic Role Labeling" scheme="https://xiaoxiaoaurora.github.io/tags/Semantic-Role-Labeling/"/>
    
      <category term="SRL" scheme="https://xiaoxiaoaurora.github.io/tags/SRL/"/>
    
      <category term="语义角色标注" scheme="https://xiaoxiaoaurora.github.io/tags/%E8%AF%AD%E4%B9%89%E8%A7%92%E8%89%B2%E6%A0%87%E6%B3%A8/"/>
    
      <category term="Annotation Guidelines" scheme="https://xiaoxiaoaurora.github.io/tags/Annotation-Guidelines/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch Bugs</title>
    <link href="https://xiaoxiaoaurora.github.io/2019/05/11/Pytorch-Bugs/"/>
    <id>https://xiaoxiaoaurora.github.io/2019/05/11/Pytorch-Bugs/</id>
    <published>2019-05-11T01:30:23.422Z</published>
    <updated>2019-05-11T01:46:15.549Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pytorch-Bugs"><a href="#Pytorch-Bugs" class="headerlink" title="Pytorch Bugs"></a>Pytorch Bugs</h1><p><strong>本篇用于记录使用Pytorch过程中遇到的bug</strong>  </p><h2 id="AttributeError-module-‘torch-nn-functions’-has-no-attribute-‘dropout’"><a href="#AttributeError-module-‘torch-nn-functions’-has-no-attribute-‘dropout’" class="headerlink" title="AttributeError: module ‘torch.nn._functions’ has no attribute ‘dropout’"></a>AttributeError: module ‘torch.nn._functions’ has no attribute ‘dropout’</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"src/main.py"</span>, line <span class="number">17</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    <span class="keyword">import</span> parse_nk</span><br><span class="line">  File <span class="string">"/home/workspace/self-attentive-parser/src/parse_nk.py"</span>, line <span class="number">74</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">FeatureDropoutFunction</span><span class="params">(nn.functional._functions.dropout.InplaceFunction)</span>:</span></span><br><span class="line">AttributeError: module <span class="string">'torch.nn._functions'</span> has no attribute <span class="string">'dropout'</span></span><br></pre></td></tr></table></figure><p><strong>将class FeatureDropoutFunction(torch.nn.functional._functions.dropout.InplaceFunction)更改为class FeatureDropoutFunction(torch.autograd.function.InplaceFunction)</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Pytorch-Bugs&quot;&gt;&lt;a href=&quot;#Pytorch-Bugs&quot; class=&quot;headerlink&quot; title=&quot;Pytorch Bugs&quot;&gt;&lt;/a&gt;Pytorch Bugs&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;本篇用于记录使用Pytorch过程中遇到的b
      
    
    </summary>
    
      <category term="Pytorch" scheme="https://xiaoxiaoaurora.github.io/categories/Pytorch/"/>
    
      <category term="Bugs" scheme="https://xiaoxiaoaurora.github.io/categories/Pytorch/Bugs/"/>
    
    
      <category term="Pytorch" scheme="https://xiaoxiaoaurora.github.io/tags/Pytorch/"/>
    
      <category term="Bugs" scheme="https://xiaoxiaoaurora.github.io/tags/Bugs/"/>
    
      <category term="version_aspect" scheme="https://xiaoxiaoaurora.github.io/tags/version-aspect/"/>
    
  </entry>
  
  <entry>
    <title>Paper：Deep contextualized word representations</title>
    <link href="https://xiaoxiaoaurora.github.io/2019/04/30/Paper%EF%BC%9ADeep-contextualized-word-representations/"/>
    <id>https://xiaoxiaoaurora.github.io/2019/04/30/Paper：Deep-contextualized-word-representations/</id>
    <published>2019-04-30T01:17:36.000Z</published>
    <updated>2019-05-10T12:30:14.172Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Deep-contextualized-word-representations-ELMO"><a href="#Deep-contextualized-word-representations-ELMO" class="headerlink" title="Deep contextualized word representations (ELMO)"></a>Deep contextualized word representations (ELMO)</h1><hr><p>[TOC]</p><p><img src="https://i.loli.net/2019/05/10/5cd511b6ba704.jpg" alt="image"></p><h2 id="1-题目与摘要"><a href="#1-题目与摘要" class="headerlink" title="1. 题目与摘要"></a>1. 题目与摘要</h2><h3 id="本文针对什么任务？任务简要介绍下。"><a href="#本文针对什么任务？任务简要介绍下。" class="headerlink" title="本文针对什么任务？任务简要介绍下。"></a>本文针对什么任务？任务简要介绍下。</h3><ul><li>任务： <a href="http://licstar.net/archives/328" target="_blank" rel="noopener">词表征(Word Representation)</a></li><li>任务介绍： <strong>句子是序列化</strong>，里面携带了大量的信息。在NLP发展的进程里面， 采用了<strong>one-hot vector</strong>的形式来表示一个句子里面的词是一种方式。表示这个句子的方式如下： <ul><li>首先是创建一张词汇表(Vocabulary)，然后每个词都有对应的位置，假设现在我们有10000个单词。本例子来自于吴恩达的Deeplearningai。图中所示的词汇表大小就是10000。<br><img src="https://upload-images.jianshu.io/upload_images/11011890-6589018636d37cd8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1000/format/webp" alt="image"></li><li>将句子里面的每个单词转换为one-hot vector。one-hot vector从字面上来理解就是向量里面只有1个1，也就是找到这个词在词汇表里面的位置处填上1，然后其他的位置都是0。可以看出one-hot vector的模长为1。<br><strong>one-hot vector表示向量的缺陷：<br>这种表示把每个词都孤立出来了，使得算法对相关词的泛化能力弱</strong>。两个不同人名的one-hot vector的内积为0，这样来表示就不能体现出词汇在某些方面的相关性。</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(x\_Harry)^T * x\_Hermione = 0</span><br></pre></td></tr></table></figure><hr><h3 id="本文发现了什么问题？该文大体是怎么解决的？解决得如何？"><a href="#本文发现了什么问题？该文大体是怎么解决的？解决得如何？" class="headerlink" title="本文发现了什么问题？该文大体是怎么解决的？解决得如何？"></a>本文发现了什么问题？该文大体是怎么解决的？解决得如何？</h3><p><strong>本文提出了一种新的深层上下文的word reresentation。这种模型不仅能够【1】表征词汇使用的复杂特性（例如句法和语义层面的特征，syntax and semantics）；还能【2】表征出如何随着上下文语境的变化而改变，例如一词多义（polysemy）。</strong></p><ul><li>发现问题&amp;解决：作者认为认为好的word representation模型应该同时兼顾【1】【2】两个特点，因此提出了deep contextualized word representation方法来解决以上两个问题。</li><li>本文中的模型本质上是一个基于大规模语料训练后的深层双向语言模型(biLM)内部隐状态的学习函数。</li><li>实验证明，新的词向量模型能够很轻松的与NLP的现有主流模型相结合，并且在六大NLP任务的结果上有着显著的提升。同时，作者也发现对模型的预训练是十分关键的，能够让下游模型去融合不同类型的半监督训练出的特征。</li></ul><hr><h3 id="解释下题目。题目起得如何？能概括内容并吸引人吗？"><a href="#解释下题目。题目起得如何？能概括内容并吸引人吗？" class="headerlink" title="解释下题目。题目起得如何？能概括内容并吸引人吗？"></a><del>解释下题目。题目起得如何？能概括内容并吸引人吗？</del></h3><h2 id="2-介绍"><a href="#2-介绍" class="headerlink" title="2. 介绍"></a>2. 介绍</h2><h3 id="这个任务以往是如何解决的？作者沿着哪条路径继续研究的？为什么？"><a href="#这个任务以往是如何解决的？作者沿着哪条路径继续研究的？为什么？" class="headerlink" title="这个任务以往是如何解决的？作者沿着哪条路径继续研究的？为什么？"></a>这个任务以往是如何解决的？作者沿着哪条路径继续研究的？为什么？</h3><p>之前提出的方法从以下两个方面克服了传统词向量的一些缺点：</p><pre><code>- 通过subword信息丰富词向量- 为每个单词的词义学习单独的向量</code></pre><ul><li><p>最近的工作也主要集中在学习与上下文相关的表示：</p><ul><li>context2vec (Melamud et al., 2016)：使用BiLSTM对一个中心词的上下文进行编码。</li><li><p>学习contextual embeddings的其他方法包括表示中的中心词本身，并使用监督神经机器翻译(MT)系统或者无监督语言模型(Peters et al.， 2017)的编码器进行计算。</p><p>但是上面这两种方法都比较适用于大型数据集，但是MT方法受到并行语料库大小的限制。<br>在本文中，作者充分利用了丰富的单语数据，将biLM训练在一个包含约3000万个句子的语料库上(Chelba et al.，2014)。还将这些方法推广到深层上下文表示，这些方法可以很处理各种NLP任务。</p></li></ul></li></ul><ul><li><p>本文提出了一种新的、能解决当前两个问题的深层上下文的word reresentation，每个word representation是整个输入语句的函数。在大型文本语料库上，以language model为目标训练出bidirectional LSTM模型从而产生词语的表征。</p></li><li><p>以前的工作也表明，不同层次的深度biRNNs编码不同类型的信息。</p></li></ul><h3 id="目前作者研究存在什么问题？为什么？你觉得可能还存在什么其他问题？为什么？"><a href="#目前作者研究存在什么问题？为什么？你觉得可能还存在什么其他问题？为什么？" class="headerlink" title="目前作者研究存在什么问题？为什么？你觉得可能还存在什么其他问题？为什么？"></a><del>目前作者研究存在什么问题？为什么？你觉得可能还存在什么其他问题？为什么？</del></h3><h3 id="该文准备如何解决这个问题？为什么可以这样解决？你觉得该文解决这个问题的方法如何？为什么？你觉得可以如何-或更好的解决这个问题？为什么？"><a href="#该文准备如何解决这个问题？为什么可以这样解决？你觉得该文解决这个问题的方法如何？为什么？你觉得可以如何-或更好的解决这个问题？为什么？" class="headerlink" title="该文准备如何解决这个问题？为什么可以这样解决？你觉得该文解决这个问题的方法如何？为什么？你觉得可以如何/或更好的解决这个问题？为什么？"></a>该文准备如何解决这个问题？为什么可以这样解决？<del>你觉得该文解决这个问题的方法如何？为什么？你觉得可以如何/或更好的解决这个问题？为什么？</del></h3><ul><li><p><strong>本文中的Word Representation不同于传统的word type embeddings</strong>：</p><ul><li>作者提出的word representation：每个词都被分配了一个表示，每个word representation是整个输入语句的函数。</li><li>在大型文本语料库上，以language model为目标训练出bidirectional LSTM模型，然后利用BiLSTM产生词语的表征。<strong>ELMo故而得名(Embeddings from Language Models)</strong>。</li><li>与以前学习上下文词向量的方法不同，ELMo表征是“深层”的，也就是说它们是biLM的所有内部层表征的函数。</li><li>本文还通过使用字符卷积从子单词单元中获益，并且还无缝地将多义信息(multi-sense)合并到下游任务中，而不需要显式地训练来预测预定义的义类</li></ul></li><li><p>这样做的好处是能够产生丰富的词语表征。高层的LSTM的状态可以捕捉词义中和语境相关的那方面的特征(比如可以用来做语义的消歧)，而低层的LSTM可以捕捉到句法方面的特征(比如可以做词性标注)。如果把它们结合在一起，在下游的NLP任务中会体现优势。</p></li></ul><h3 id="列出该文贡献（该文自己觉得的）"><a href="#列出该文贡献（该文自己觉得的）" class="headerlink" title="列出该文贡献（该文自己觉得的）"></a>列出该文贡献（该文自己觉得的）</h3><ul><li>ELMO可以很容易地添加到现有的模型中</li><li>用于解决6个不同且具有挑战性的语言理解问题（包括文本蕴涵、问题回答和情感分析），仅添加ELMo表示就可以显著提高每种任务的结果，包括减少20%的相对错误。</li><li>对于可能进行直接比较的任务，ELMo的性能优于CoVe (McCann et al.， 2017)， CoVe使用神经机器翻译编码器计算上下文表示。</li><li>最后，对ELMo和CoVe的分析表明，深度表示优于仅来自LSTM顶层的表示。</li></ul><hr><h2 id="3-模型：ELMo-Embeddings-from-Language-Models"><a href="#3-模型：ELMo-Embeddings-from-Language-Models" class="headerlink" title="3. 模型：ELMo(Embeddings from Language Models)"></a>3. 模型：ELMo(Embeddings from Language Models)</h2><p>与大多数广泛使用的词嵌入(Pennington et al., 2014)不同，<strong>ELMo词表示是整个输入语句的函数</strong>。<br>它们是在具有字符卷积的两层biLMs之上计算的(第3.1节)，作为内部网络状态的线性函数(第3.2节)。这种设置使得可以进行半监督学习，其中biLM被大规模地预先训练(第3.4节)，并且很容易地融入到现有的广泛的神经NLP体系结构中(第3.3节)。</p><h3 id="整体介绍（主要是图）"><a href="#整体介绍（主要是图）" class="headerlink" title="整体介绍（主要是图）"></a>整体介绍（主要是图）</h3><p><img src="https://i.loli.net/2019/05/10/5cd533b7e6bc1.jpg" alt="ELMO"></p><p>ELMO（Embeddings from Language Models）模型本质是从语言模型而来的。</p><ol><li><strong>Language Model</strong><br><img src="https://i.loli.net/2019/05/10/5cd534478803e.jpg" alt="Language Model"></li></ol><p>  给定一个句子($t_1, t_2, t_N$) 构建的语言模型就是通过一个词汇的上下文去预测一个词$t_k$：</p><p><img src="https://i.loli.net/2019/05/10/5cd53de4a9292.jpg" alt="image"></p><p>最新的神经语言模型(Jozefowicz et al. ´ , 2016; Melis et al., 2017; Merity et al., 2017)先 计算一个与上下文无关的字符表示 ${x_k^{LM}}$ (通过token embedding或者一个CNN覆盖字符)，然后将它传递进L层前向LSTM。在每个位置k，每个LSTM层输出一个上下文相关的表示$\overrightarrow{h}_{k,j}^{LM}$，其中j = 1，…，L。顶层LSTM输出，$\overrightarrow{h}_{k,j}^{LM}$通过一个Softmax层用于预测下一个token$t_{k+1}$。</p><p>反向LM与前向LM类似，只是它以相反的方式遍历序列，根据未来上下文预测前面的token。</p><p><img src="https://i.loli.net/2019/05/10/5cd550adece17.jpg" alt></p><p>biLM结合了前向LM和反向LM。我们的公式共同最大化了前向和反向的对数似然性：<br>$$<br>\sum_{k=1}^N(\log p(t_k | t_1,…,t_{k-i}; \circleddash_x, \overrightarrow{\circleddash}_{LSTM}, \circleddash_s) + \log p(t_k | t_{k+1},…,t_{N}; \circleddash_x, \overleftarrow{\circleddash}_{LSTM}, \circleddash_s)))<br>$$</p><ol start="2"><li><p><strong>Bidirectional Language Model</strong></p><p>其中两个方向的LSTM的参数不是共享的，$\circleddash_x$就是一开始输入的词向量，$\circleddash_s$就是softmax层参数。因此，双向语言模型的结构图，可以表达如下:</p><p><img src="https://i.loli.net/2019/05/10/5cd534712bb3c.jpg" alt="Bidirectional Language Models"></p></li></ol><ol start="3"><li>ELMO：一种学习单词表示的新方法，该方法是biLM层的线性组合。具体的描述在<a href="模型创新点">模型创新点</a>。</li></ol><h3 id="模型创新点"><a href="#模型创新点" class="headerlink" title="模型创新点"></a>模型创新点</h3><h3 id="（仅对要进一步跟进的paper）详细介绍模型，从输入到输出，输入矩阵维度，公式等"><a href="#（仅对要进一步跟进的paper）详细介绍模型，从输入到输出，输入矩阵维度，公式等" class="headerlink" title="（仅对要进一步跟进的paper）详细介绍模型，从输入到输出，输入矩阵维度，公式等"></a>（仅对要进一步跟进的paper）详细介绍模型，从输入到输出，输入矩阵维度，公式等</h3><p>ELMo is a task specific combination of the intermediate layer representations in the biLM.</p><p>对于每一个词$t_k$，一个L层的biLM可以计算出$2*L+1$个表达如下：</p><p><img src="https://i.loli.net/2019/05/10/5cd55747f158c.jpg" alt></p><p>其中$h_{k,0}^{LM}$来表示第一层向量，$h_{k,j}^{LM} = [\overrightarrow{h}_{k,j}^{LM}; {\overleftarrow{h}}_{k,j}^{LM}]$。\</p><p>为了包含在下游模型中，ELMo将R中的所有层折叠成一个向量，$ELMO_k = E(R_k; \circleddash_e) $。</p><p>在最简单的情况下，ELMo只选择顶层，$E(R_k) = h_{k,L}^{LM}$，和TagLM和CoVe一样。</p><p>ELMO本质上就是一个任务导向的、双向语言模型（biLM）内部的隐状态层的组合。通用的表达式如下：</p><p><img src="C:\Users\uliux\AppData\Roaming\Typora\typora-user-images\1557486044972.png" alt></p><p>在（1）中，s^task^是softmax-normalized权重，标量参数$\gamma^{task}$允许任务模型缩放整个ELMo向量。其中$\gamma $有助于进行优化。考虑到每个biLM层的激活度分布不同，在某些情况下，在加权前对每个biLM层应用归一化层(Ba et al.， 2016)也有所帮助。</p><blockquote><p>$ \gamma $是用来控制ELMO模型生成的向量大小，原文中说该系数对于后续的模型优化过程有好处（在附件中，作者强调了这个参数的重要性，因为biLM的内核表达，与任务需要表达，存在一定的差异性，所以需要这么一个参数去转换。并且，这个参数对于，last-only的情况（就是只取最后一层，ELMO的特殊情况），尤其重要），另一个参数s，原论文只说了softmax-normalized weights，所以我的理解，其实它的作用等同于层间的归一化处理。（<a href="https://www.jianshu.com/p/d93912d5280e" target="_blank" rel="noopener">引用自</a>)</p></blockquote><h3 id="Using-biLMs-for-supervised-NLP-tasks"><a href="#Using-biLMs-for-supervised-NLP-tasks" class="headerlink" title="Using biLMs for supervised NLP tasks"></a>Using biLMs for supervised NLP tasks</h3><p>在给定目标NLP任务的预训练biLM和监督体系结构的情况下，利用biLM改进任务模型是一个简单的过程。我们只需运行biLM并记录每个单词的所有层表示。然后，我们让终端任务模型学习这些表示的线性组合，如下所述：</p><ul><li>首先考虑无biLM的监督模型的最低层。大部分的NLP的模型都会有一层词向量层，而我们要做的无非就是用ELMO与词向量层结合。然后，模型生成一个上下文敏感的表示$h_k$，通常使用双向RNNs、CNNs或前馈网络。</li><li>为了将ELMo添加到监督模型中，我们首先冻结biLM的权值，然后将ELMo向量$ELMo_k^{task}$与 $x_k$ 连接，并将ELMo增强表示$[ x_k ; ELMo_k^{task}]$c传递到RNN中。</li><li>发现：在ELMo中添加合适的dropout和某些情况下在loss中添加$\lambda ||w||_2^2$正则化ELMo权重都是有益的。这对ELMo权重施加了一个归纳偏差，使其接近所有biLM层的平均值。</li></ul><h3 id="Pre-trained-bidirectional-language-model-architecture"><a href="#Pre-trained-bidirectional-language-model-architecture" class="headerlink" title="Pre-trained bidirectional language model architecture"></a>Pre-trained bidirectional language model architecture</h3><p>本文预训练的biLMs与Jozefowicz等人(2016)和Kim等人 (2015) 的体系结构相似，但加以修改以便支持两个方向的联合训练，并在LSTM层之间添加了剩余连接。在这项工作中，作者重点关注大规模的biLMs，正如Peters等人(2017)强调了使用biLMs比只使用前向LMs和大规模训练更重要。</p><p>最终作者用于实验的预训练模型，为了平衡语言模型之间的困惑度以及后期NLP模型的计算复杂度，采用了2层Bi-Big-Lstm，共计4096个单元，输入及输出纬度为512，并且在第一层和第二层之间有残差连接，包括最初的那一层文本向量（用了2048个过滤器, 进行基于字符的卷积计算，详细可查看字符卷积的原论文），整个ELMO会为每一个词提供一个3层的输出，而下游模型学习的就是这3层输出的组合。另外，作者强调了一下，对该模型进行FINE-TUNE训练的话，对具体的NLP任务会有提升的作用。</p><h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. <strong>实验</strong></h2><p><img src="https://i.loli.net/2019/05/10/5cd55ff7788d5.jpg" alt></p><p>论文的实验部分，具体展示了ELMO在六大nlp任务上的表现（如Tabel 1），证实了该模型的有效性。此处未具体展开，只罗列了SRL。</p><h3 id="数据集及评价标准介绍"><a href="#数据集及评价标准介绍" class="headerlink" title="数据集及评价标准介绍"></a>数据集及评价标准介绍</h3><p><strong>Semantic Role Labeling(SRL)</strong>是对句子的predicate-argument结构进行解析的过程，通常被描述为“回答谁对谁做了什么(Who did what to whom )”。He(2017)等人将SRL作为一个BIO标记问题，并使用了一个8层的前后方向交错的深层biLSTM。</p><h3 id="Baseline介绍"><a href="#Baseline介绍" class="headerlink" title="Baseline介绍"></a><del>Baseline介绍</del></h3><h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><ul><li><p>在下游任务中使用深层上下文表示比仅使用顶层的先前工作提高了性能，无论它们是由biLM或MT编码器生成的，而且ELMo表示提供了最佳的总体性。整体趋势与CoVe相似，但在基线上增幅较小。</p><p><img src="https://i.loli.net/2019/05/10/5cd56750d6a7f.jpg" alt></p></li></ul><ul><li>探索了在biLMs中捕获的不同类型的上下文信息，并使用了两个内在的评估来表明，语法信息在较低的层中更好地表示，而语义信息在较高的层中捕获，这与MT编码器一致。它还表明，本文中的biLM始终提供比CoVe更丰富的表示。ELMO不同的层，能从不同的纬度表达一个词，作者经过实验发现，低层的输出能更好的从句法语法的层面表达一个词，而高层的输出能更好的从语意的层面表达一个词</li></ul><ul><li><p>ELMo在任务模型(第5.2节)、训练集大小(第5.4节)中的敏感性，并可视化了ELMo在任务中学习到的权重(第5.5节)。</p><p><img src="https://i.loli.net/2019/05/10/5cd568549540c.jpg" alt></p><ul><li>加ELMO的位置：是与之前的词向量层一起，在最开始的地方。作者还实验了一把，发现在特定于任务的体系结构中，在biRNN的输出中包含ELMo可以改进某些任务的总体结果。如表3所示，在SNLI和SQuAD的输入层和输出层都包含ELMo，这比仅仅在输入层有所改进，但是对于SRL(以及没有显示的协引用分辨率)，当它只包含在输入层时，性能是最高的。一个可能的解释是SNLI和小队架构在biRNN之后都使用了注意层，所以在这一层引入ELMo允许模型直接关注biLM的内部表示。</li></ul></li></ul><h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h2><p>本论文主要集中在于阐述ELMO结构及其预训练的思想，并且用具体NLP实验证明了使用这一套结构的原理，并证实了其可行性。</p><h3 id="你觉得这篇paper创新与贡献是（不一定如作者所说）？为什么？"><a href="#你觉得这篇paper创新与贡献是（不一定如作者所说）？为什么？" class="headerlink" title="你觉得这篇paper创新与贡献是（不一定如作者所说）？为什么？"></a>你觉得这篇paper创新与贡献是（不一定如作者所说）？为什么？</h3><ul><li><p>本文介绍了一种从biLMs学习高质量的深度上下文相关表示的通用方法，并在将ELMo应用于广泛的NLP任务时显示了很大的改进。</p></li><li><p>通过一些实验也证实了biLM层可以有效地编码关于语境用词的不同类型的语法和语义信息，并且使用所有层可以提高整体任务性能。</p></li></ul><h3 id="有没有进一步深入的价值？为什么？"><a href="#有没有进一步深入的价值？为什么？" class="headerlink" title="有没有进一步深入的价值？为什么？"></a>有没有进一步深入的价值？为什么？</h3><p>目前只停留在使用的过程中。</p><h3 id="列出该文弱点（或者是你觉得应该是什么问题，他解决的不好，你会如何解决？）"><a href="#列出该文弱点（或者是你觉得应该是什么问题，他解决的不好，你会如何解决？）" class="headerlink" title="列出该文弱点（或者是你觉得应该是什么问题，他解决的不好，你会如何解决？）"></a><del>列出该文弱点（或者是你觉得应该是什么问题，他解决的不好，你会如何解决？）</del></h3><h3 id="该文对你的启发是？"><a href="#该文对你的启发是？" class="headerlink" title="该文对你的启发是？"></a>该文对你的启发是？</h3><p>将ELMO用于Sytactic Parsing中。</p><h3 id="列出其中有价值的需要进一步阅读的参考文献"><a href="#列出其中有价值的需要进一步阅读的参考文献" class="headerlink" title="列出其中有价值的需要进一步阅读的参考文献"></a>列出其中有价值的需要进一步阅读的参考文献</h3><p>1、BERT: Pre-training of Deep Bidirectional Transformers for<br>Language Understanding </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Deep-contextualized-word-representations-ELMO&quot;&gt;&lt;a href=&quot;#Deep-contextualized-word-representations-ELMO&quot; class=&quot;headerlink&quot; title=&quot;De
      
    
    </summary>
    
      <category term="Papers" scheme="https://xiaoxiaoaurora.github.io/categories/Papers/"/>
    
      <category term="论文阅读笔记" scheme="https://xiaoxiaoaurora.github.io/categories/Papers/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Papers" scheme="https://xiaoxiaoaurora.github.io/tags/Papers/"/>
    
      <category term="论文阅读笔记" scheme="https://xiaoxiaoaurora.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="ELMO" scheme="https://xiaoxiaoaurora.github.io/tags/ELMO/"/>
    
  </entry>
  
  <entry>
    <title>Deep Biaffine Attention for Neural Dependency Parsing</title>
    <link href="https://xiaoxiaoaurora.github.io/2019/04/11/Deep-Biaffine-Attention-for-Neural-Dependency-Parsing/"/>
    <id>https://xiaoxiaoaurora.github.io/2019/04/11/Deep-Biaffine-Attention-for-Neural-Dependency-Parsing/</id>
    <published>2019-04-11T01:13:25.000Z</published>
    <updated>2019-05-10T12:26:58.424Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Dependency-Biaffine-Attention-Neural-Dependency-Parsing"><a href="#Dependency-Biaffine-Attention-Neural-Dependency-Parsing" class="headerlink" title="Dependency Biaffine Attention Neural Dependency Parsing"></a>Dependency Biaffine Attention Neural Dependency Parsing</h1><p><img src="https://i.loli.net/2019/04/11/5cae9707cace5.jpg" alt="Title"></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文主要针对 Dependency Parsing 提出了一个新的模型，该文以Kiperwasser &amp; Goldberg (2016)  最近的工作为基础，在graph-based的dependency parser的基础上引入了 neural attention。</p><blockquote><p>We use a larger but more thoroughly <code>regularized parser</code> than other recent BiLSTM-based approaches,<code>with biaffine classifiers to predict arcs and labels</code>.</p></blockquote><h3 id="本文针对任务：Dependency-Parsing"><a href="#本文针对任务：Dependency-Parsing" class="headerlink" title="本文针对任务：Dependency Parsing"></a>本文针对任务：Dependency Parsing</h3><p>【(1) 本文针对什么任务？任务简要介绍下。】</p><p>依存分析 (Dependency Parsing, DP) 通过分析语言单位内成分之间的依存关系揭示其句法结构。 直观来讲，依存句法分析识别句子中的“主谓宾”、“定状补”这些语法成分，并分析各成分之间的关系。再通俗地讲：它将句子分析成一颗依存句法树，描述出各个词语之间的依存关系。 </p><ul><li>依存语法的结构没有非终结点，词与词之间直接发生依存关系，构成一个依存对，其中一个是核心词，也叫支配词，另一个叫修饰词，也叫从属词。 </li><li>依存关系用一个有向弧表示，叫做依存弧。依存弧的方向为由从属词指向支配词，当然反过来也是可以的，按个人习惯统一表示即可。</li><li>依存句法分析的5个条件<ul><li>一个句子中只有一个成分是独立的 </li><li>句子的其他成分都从属于某一成分 </li><li>任何一个成分都不能依存于两个或两个以上的成分 </li><li>如果成分A直接从属成分B，而成分C在句子中位于A和B之间，那么，成分C或者从属于A，或者从属于B，或者从属于A和B之间的某一成分 </li><li>中心成分左右两边的其他成分相互不发生关系</li></ul></li></ul><p><a href="http://stp.lingfil.uu.se/~nivre/docs/ACLslides.pdf" target="_blank" rel="noopener">详细介绍：https://cl.lingfil.uu.se/~nivre/docs/ACLslides.pdf</a></p><h3 id="本文发现了什么问题？该文大体是怎么解决的？解决得如何？"><a href="#本文发现了什么问题？该文大体是怎么解决的？解决得如何？" class="headerlink" title="本文发现了什么问题？该文大体是怎么解决的？解决得如何？"></a>本文发现了什么问题？该文大体是怎么解决的？解决得如何？</h3><p><strong>本文作者提出自己的问题</strong>：目前性能最优的基于转移（transition-based）的神经依存解析模型大大优于许多更简单的基于图（graph-based）的神经解析器。</p><p><strong>解决方案：</strong>在Kiperwasser &amp; Goldberg (2016) 所提出的模型的基础上加以改进：</p><ul><li>建立了一个更大的网络，但也使用了更多的正则化</li><li>用双仿射分类器（Biaffine Classifier）代替传统的基于MLP的注意机制和仿射标签分类器（Affile Label Classifier），而不是在双仿射变换中使用LSTM的顶部递归状态</li><li>在两种不同的模型架构和超参上训练模型，加以比较和改进</li></ul><p><strong>结果：</strong>得到的解析器保持了Graph-based的方法的大部分简单性，同时又接近了SOTA Transition-based的方法的性能。</p><h3 id="解释下题目。题目起得如何？能概括内容并吸引人吗？"><a href="#解释下题目。题目起得如何？能概括内容并吸引人吗？" class="headerlink" title="解释下题目。题目起得如何？能概括内容并吸引人吗？"></a>解释下题目。题目起得如何？能概括内容并吸引人吗？</h3><p><strong>Biaffine Attention</strong>  – <em>简单明了</em> </p><hr><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><h3 id="这个任务以往是如何解决的？作者沿着哪条路径继续研究的？为什么？"><a href="#这个任务以往是如何解决的？作者沿着哪条路径继续研究的？为什么？" class="headerlink" title="这个任务以往是如何解决的？作者沿着哪条路径继续研究的？为什么？"></a>这个任务以往是如何解决的？作者沿着哪条路径继续研究的？为什么？</h3><p>Dependency Parsing主要有两种方法：Transition-based 和Graph-based。作者沿着Graph-based这个角度对Parser的结构加以改进（引入Biaffine Attention）从而达到提升性能的目的。目的即上文提出的当前Graph-based的Parser性能低于Transition-based的性能。</p><p>下面从对Paser的两种结构的研究过程进行介绍：</p><h4 id="Transition-based-Parser"><a href="#Transition-based-Parser" class="headerlink" title="Transition-based Parser"></a>Transition-based Parser</h4><ul><li><p>About</p><ul><li><p>基于转移的解析器<strong>从左到右按顺序处理</strong>句子中的每一个词，每次选择一条弧来构成一颗解析树。包含一个尚未解析的单词的“缓冲区（buffer）”和一个未看到head或者依赖项尚未被全部解析的单词的“堆栈（stack）”。</p></li><li><p>在每个步骤中，Transition-based解析器都可以访问和操作堆栈和缓冲区，并将弧从一个单词分配到另一个单词</p></li><li><p>然后，可以根据从堆栈、缓冲区和以前的arc操作中提取的特征训练任何多类机器学习分类器，以预测下一步操作。</p></li><li><p><u>Transition-based的解析器不使用机器学习来直接预测边;他们用它来预测转移算法的操作。</u></p><p><img src="https://i.loli.net/2019/04/12/5cb076598e808.jpg" alt></p></li></ul></li><li>Modifies<ul><li>Chen &amp; Manning(2014)首次成功地尝试将<strong>深度学习</strong>集成到Transition-based依存解析器中。<ul><li>每个step中，(前馈)网络根据来自堆栈和缓冲区中特定单词的单词、标记和标签嵌入，为解析器可以采取的每个操作分配一个概率</li></ul></li><li>Weiss et al. (2015) and Andor et al. (2016)  ：增加了一个<strong>波束搜索（beam search）</strong>和<strong>条件随机场（CRF）</strong>的损失目标，使解析器一旦发现以前的操作可能是不正确的苗头便可以“撤消”这个操作。</li><li>Dyer et al. (2015) and (Kuncoro et al., 2016) ：使用<strong>LSTMs</strong>来表示堆栈和缓冲区，通过以组合已解析的短语的方式构建来获得最优性能。</li></ul></li></ul><h4 id="Graph-based-Parser"><a href="#Graph-based-Parser" class="headerlink" title="Graph-based Parser"></a>Graph-based Parser</h4><ul><li><p>About</p><p><u>Graph-based的解析器使用机器学习方法为每条可能的边分配一个权重或概率，然后从这些加权边中构造一棵最大生成树(Maximum Spaning Tree, MST)</u></p><p><strong>基于图的依存句法分析就是为每个要分析的句子生成一个有向图</strong>，其中：节点是句子中的单词，边是单词之间的依存关系，因为依存句法中规定每个句子都有其核心成分，所以加入了须根节点。依存句法中还规定了句子中除了须根节点，每个词必须依存于其他词，因此，句子依存图中边的个数和单词的个数相等。</p></li><li><p>Modifies</p><ul><li><p><strong>Kiperwasser &amp; Goldberg (2016)</strong> ：提出一种基于神经图的解析器(除了Transition-based的解析器之外)，它使用与Bahdanau等人(2014)所提出的相同的注意机制来进行机器翻译。每个单词的(双向)LSTM递归输出向量与它每个可能的head递归向量串联拼接，并将结果用作MLP的输入，MLP对每个生成的弧进行评分。训练时预测的树结构是每个单词都依赖于其得分最高的头部。类似地生成标签，在多分类MLP中使用每个单词的循环输出向量及其gold或预测的head word的循环向量。</p><blockquote><p>Labels are generated analogously, with each word’s recurrent output vector and its gold or predicted head word’s recurrent vector being used in a multi-class MLP </p></blockquote></li><li><p>Hashimoto et al. (2016)  ：在它们的多任务神经模型中包含一个基于图的依存解析器。用双线性（bilinear）(但仍然使用MLP标签分类器)替代了Kiperwasser和Goldberg(2016)使用的传统的基于MLP的注意力机制。</p></li><li><p>Cheng et al. (2016) ：在某种程度上，试图绕过其他Graph-based的神经解析器的限制，这些解析器无法根据以前的解析结果决定每个可能弧的得分。除了用一个双向循环神经网络来计算每个词的循环隐层状态向量以外，还有附加的单向循环神经网络（left-to-right和 right-to left）来记录之前每条arc的概率，然后用这些概率一起来预测下一个arc。</p></li></ul></li></ul><h3 id="目前摘要存在什么问题？为什么？你觉得可能还存在什么其他问题？为什么？"><a href="#目前摘要存在什么问题？为什么？你觉得可能还存在什么其他问题？为什么？" class="headerlink" title="目前摘要存在什么问题？为什么？你觉得可能还存在什么其他问题？为什么？"></a>目前摘要存在什么问题？为什么？你觉得可能还存在什么其他问题？为什么？</h3><p>摘要中没发现提出问题，只表述了当前论文的工作和取得的成绩。暂时还未发现什么特殊的问题。</p><h3 id="该文准备如何解决这个问题？为什么可以这样解决？你觉得该文解决这个问题的方法如何？为什么？你觉得可以如何-或更好的解决这个问题？为什么？"><a href="#该文准备如何解决这个问题？为什么可以这样解决？你觉得该文解决这个问题的方法如何？为什么？你觉得可以如何-或更好的解决这个问题？为什么？" class="headerlink" title="该文准备如何解决这个问题？为什么可以这样解决？你觉得该文解决这个问题的方法如何？为什么？你觉得可以如何/或更好的解决这个问题？为什么？"></a>该文准备如何解决这个问题？为什么可以这样解决？你觉得该文解决这个问题的方法如何？为什么？你觉得可以如何/或更好的解决这个问题？为什么？</h3><p>通过对以往在Graph-based Parser的结构进行性能分析，加以整合改进，提出了Biaffine Attention机制。有效的提高了当前Graph-based Parser性能，几乎与当前SOTA的Transition-based Parser效果相当。</p><h3 id="列出该文贡献（该文自己觉得的）"><a href="#列出该文贡献（该文自己觉得的）" class="headerlink" title="列出该文贡献（该文自己觉得的）"></a>列出该文贡献（该文自己觉得的）</h3><ul><li>提升了基于简单的Graph-based神经解析器的性能，该解析器在六种不同语言的标准树库上获得了最先进或接近最先进的性能，在英语PTB上实现了95.7%的UAS和94.1%的LAS，在Chinese Penn Treebank中实现了UAS=89.30%，LAS=88.23%。</li><li><strong>目前Graph-based解析器中性能最高的</strong>。</li><li>分析了超参的选择对于解析结果的准确性。</li></ul><hr><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>将Kiperwasser &amp; Goldberg (2016),Hashimoto et al. (2016), and Cheng et al. (2016)提出的模型加以变形：</p><p><img src="https://i.loli.net/2019/04/11/5caf34ed899a1.jpg" alt></p><h3 id="模型创新点"><a href="#模型创新点" class="headerlink" title="模型创新点"></a>模型创新点</h3><ul><li><strong>使用<u>双仿射注意力机制</u></strong>(Biaffine Attention)代替双线性（bilinear）或传统的MLP-based的注意力机制, <u>结构更加简单</u>，运用了一个双线性层而不是两个线性层和一个非线性层—<u>选择双仿射使得我们模型中的分类器类似于传统的仿射分类器</u>，后者在单个LSTM输出状态$$ \mathbf{r}_i$$(或其他向量输入)上使用仿射变换来预测所有类的得分向量$$ \mathbf{s}_i  $$。</li><li>使用Biaffine依存项标签分类器</li><li>在应用双仿射变换(Biaffine transformation)之前，我们将降维MLPs应用于每个递归输出向量 $$  \mathbf{r}_i  $$。</li></ul><h3 id="详细介绍模型，从输入到输出，输入矩阵维度，公式等"><a href="#详细介绍模型，从输入到输出，输入矩阵维度，公式等" class="headerlink" title="详细介绍模型，从输入到输出，输入矩阵维度，公式等"></a>详细介绍模型，从输入到输出，输入矩阵维度，公式等</h3><p>可将双仿射注意力机制看作一个传统的放射分类器，但是使用<u>堆叠</u><strong>LSTM</strong>的输出$ \mathbf{R} \mathbf{U}^{(1)}$的一个 (d×d)线性变换代替权重矩阵<em>W</em>，用一个(d×1)变换$ \mathbf{R} \mathbf{u}^{(2)} $来替代偏置项<em>b</em>。</p><p><img src="https://i.loli.net/2019/04/11/5caf4bdba664c.jpg" alt></p><p>​    这在概念上的优势是，可以直接在对单词 <em>j</em>  在术语$$ \mathbf{r}_j^T u$$中接收任何dependents的先验概率 和 j 在$$ \mathbf{r}_j^T  \mathbf{U}^{(1)} \mathbf{r}_i $$中接受特定依存项 i 的可能性之间进行建模。还使用双仿射分类器来预测给定的head或预测head $ {y}_i $的依存标签(3)。</p><p><img src="https://i.loli.net/2019/04/11/5caf5134cb098.jpg" alt></p><p>同样地为每个类的每个先验概率建模：</p><ul><li><p>只给出一个词 i 的可能性(一个词带有一个特定的标签的概率为多少)，</p></li><li><p>只给出head word $$ y_i $$的可能性(一个词带有一个特定标签的依存项的概率为多少)，</p></li><li>给出单词 i 和它的head的类的可能性(给定一个单词的head，且该单词带有某个特定标签的概率为多少)。</li></ul><p>在Biaffine分类器之前将较小的MLPs应用于循环输出状态，其优点是可以剥离与当前决策无关的信息。</p><p><img src="https://i.loli.net/2019/04/12/5cb03d62d75e8.jpg" alt></p><p>在标签分类器中使用递归状态之前，还将MLP应用于递归状态。和其它Graph-based模型一样，在训练时,预测的解析树是每一个单词都依存于其得分最高的head（虽然在测试时也会通过MST算法确保解析树是一个格式良好的树）。</p><p>除了和其他Graph-based解析器之间存在架构差异之外，作者还做出了一些超参数选择，使得解析器性能优于其他解析器。</p><p><img src="https://i.loli.net/2019/04/11/5caf541f85a36.jpg" alt></p><table><thead><tr><th>Hyperparameters</th><th>Decriptions</th></tr></thead><tbody><tr><td>外部词向量</td><td>计算一个在训练数据集中至少出现两次的单词组成的“训练过的”embedding矩阵，并将这些embedding添加到它们相应的预先训练过的embedding中。任何不在这两种嵌入矩阵中出现的单词都将由一个OOV符号替换。100维</td></tr><tr><td>POS tag vectors</td><td>100d</td></tr><tr><td>three BiLSTM layers</td><td>400 dimensions in each direction</td></tr><tr><td>ReLU MLP Layers</td><td>500 and 100 dimensions</td></tr></tbody></table><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="数据集及评价标准介绍"><a href="#数据集及评价标准介绍" class="headerlink" title="数据集及评价标准介绍"></a>数据集及评价标准介绍</h3><ul><li><p>English Penn Treebank</p><ul><li>采用版本为3.3.0和3.5.0的Stanford Dependency转换工具将PTB数据集转换为<strong>PTB-SD 3.3.0</strong>和<strong>PTB-SD 3.5.0</strong>两个版本。</li><li>POS标签：Stanford POS tagger 生成</li></ul></li><li><p>Chinese Penn Treebank</p><ul><li>POS tags：Gold tags</li></ul></li><li>CoNLL2009 shared task dataset<ul><li>POS tags：提供的预测标签</li></ul></li></ul><p>注意：在评估PTB-SD和CTB时省略标点符号</p><p>通过PTB-SD 3.5.0验证集来实现超参搜索，为最小化到PTB-SD 3.3.0基准测试的过度拟合。在PTB-SD 3.5.0测试集上来进行超参分析。</p><p><img src="https://i.loli.net/2019/04/12/5cb058fe84822.jpg" alt></p><h4 id="超参选择"><a href="#超参选择" class="headerlink" title="超参选择"></a>超参选择</h4><ul><li>Attention机制</li></ul><p>研究了不同分类器对准确度和性能的影响，发现：<strong>深层双线性模型在速度和精度方面都优于其他模型</strong>。</p><p>其中，浅层双线性的arc和label分类器在与深层双线性模型设置相同时，两者UAS性能相当（deep: 95.75%, shallow：95.74%）。但是由于label分类器较大 ($$ {801 &lt; c &lt; 801} $$)，所以解析过程中速度慢且<strong>发生过拟合</strong>。解决过拟合的方法有两种：</p><p>（1）增大MLP的dropout – 对解析速度并无改善；</p><p>（2）将recurrent cell大小降到300 – 这将降低UAS的准确性，也不会提升解析速度到论文中提出的deep模型中。</p><p><strong>结果：</strong>实现了Kiperwasser &amp; Goldberg (2016) 提出的基于MLP的注意力和分类方法，发现在LAS和UAS的准确性方面都明显低于deep biaffine方法。</p><h4 id="Network-Size"><a href="#Network-Size" class="headerlink" title="Network Size"></a>Network Size</h4><p>那么网络大小是如何影响速度和准确性的？</p><ul><li>Kiperwasser &amp; Goldberg’s 2016：两层维度为125的BiLSTM</li><li>Hashimoto et al.’s 2016：一层100维的BiLSTM用于解析过程(两个较低的层也被训练用于其他目标)</li><li>Cheng et al.’s 2016：一层368维的GRU</li></ul><p><strong>结论：</strong>使用三层或四层比使用两层可以获得更好的性能，并将LSTM大小从200增加到300或400个维度同样显著地提高了性能。<strong>BUT：</strong>具有400维递归状态的模型在验证集上的性能显著优于300维模型，但在测试集上则不是这样。</p><h4 id="Recurrent-Cell"><a href="#Recurrent-Cell" class="headerlink" title="Recurrent Cell"></a>Recurrent Cell</h4><ul><li><p>虽然GRU( Cheng et al, 2016)比LSTM的结构简单速度更快，但是在作者提出的模型中，GRU的表现不如LSTM的好。</p></li><li><p>Greff et al. (2015) 提出的Cif-LSTM(the coupled input-forget gate LSTM cells)模型的性能仍然略低于LSTM，但两者之间的差异要小得多，但是速度比GRU快。</p></li></ul><p><strong>猜想：</strong>Cif-LSTM模型中的输出门能够保证稀疏的递归输出状态，这有助于设定较高的dropout，以防止GRU无法做到的过度拟合。</p><h4 id="Embedding-Dropout"><a href="#Embedding-Dropout" class="headerlink" title="Embedding Dropout"></a>Embedding Dropout</h4><p>由于解析性能的提升，所以还需增加正则化(regularization)。</p><ul><li>输入层进行了正则化</li><li>在训练过程中减少了33%的单词和33%的标签</li><li>只用单词或标签dropout来训练的模型，两者都没有明显地过拟合，后者使得label准确度和attachment准确度都有所降低。</li></ul><p><strong>结论：</strong>不使用任何tags却比使用tags dropout的性能更好。</p><h4 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h4><ul><li><p>选择Adam优化器</p></li><li><p>$$ \mathbf{β}_2 = 0.9$$ 的性能优于  $$ \mathbf{β}_2 = 0.999$$</p><h4 id="评价标准"><a href="#评价标准" class="headerlink" title="评价标准"></a>评价标准</h4></li></ul><p>对每个单词标上序号(从1开始，0为“ROOT”)，列出每个依存关系对应的序号和类型。分为两种正确率的计算。</p><ul><li>UAS：不考虑标签只考虑弧。</li><li>LAS：同时考虑标签和弧。</li></ul><p><img src="https://i.loli.net/2019/04/12/5cb076b139c0f.jpg" alt></p><h3 id="Baseline介绍"><a href="#Baseline介绍" class="headerlink" title="Baseline介绍"></a>Baseline介绍</h3><p><img src="https://i.loli.net/2019/04/12/5cb05cc84d0ba.jpg" alt></p><h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><ul><li><p>虽然Deep Biaffine模型结构要简单得多，但是在PTB-SD 3.3.0上的UAS性能几乎与Kuncoro等人(2016)提出的模型相同。</p></li><li><p>在CTB 5.17和CoNLL 09所有语言上的UAS也是最佳性能。</p></li><li>但是，CoNLL 09数据集包含许多非投射(non-projective )依存项，这些依存项很难或不可能通过基于转移的解析器预测出来。所以这也解释了本文提出的模型和Andor等人(2016) 的模型在这个数据集上差异较大的情况。</li></ul><p><strong>针对上述所提出的对于非投射情况的解析，Graph-Based具有优势，而在中介语的依存句法解析中，显然Graph-Based更适用。</strong>这也是接下来需要跟进的地方。目前对该模型进行了复现，在CTB数据上经过测试，UAS达到88+（忘记具体数字了，后面补上）。</p><p>LAS的结果较低：</p><blockquote><p>Firstly, it may be the result of inefficiencies or errors in the GloVe embeddings or POS tagger, in which case using alternative pretrained embeddings or a more accurate tagger might improve label classification. </p><p>Secondly, the SOTA model is specifically designed to capture phrasal compositionality; so another possibility is that ours doesn t capture this compositionality as effectively, and that this results in a worse label score. Similarly, it may be the result of a more general limitation of graph-based parsers, which have access to less explicit syntactic information than transition-based parsers when making decisions. </p></blockquote><p>要想解决以上两种限制性因素，需要一个相较于当前结构简单的Graph-based神经解析器更具创新性的模型。不过在我对于中介语的解析中，不考虑标签类别，所以对LAS无要求….</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><h3 id="你觉得这篇paper创新与贡献是（不一定如作者所说）？为什么？"><a href="#你觉得这篇paper创新与贡献是（不一定如作者所说）？为什么？" class="headerlink" title="你觉得这篇paper创新与贡献是（不一定如作者所说）？为什么？"></a>你觉得这篇paper创新与贡献是（不一定如作者所说）？为什么？</h3><ul><li>在神经依存解析器加入BIaffine Attention，希望在不影响性能的前提下提高解析速）。</li><li>Larger and more Regularied Network, and SOTA Performance</li></ul><h3 id="有没有进一步深入的价值？为什么？”-gt-lt-a-href-”-有没有进一步深入的价值？为什么？"><a href="#有没有进一步深入的价值？为什么？”-gt-lt-a-href-”-有没有进一步深入的价值？为什么？" class="headerlink" title="有没有进一步深入的价值？为什么？”&gt;&lt;a href=”#有没有进一步深入的价值？为什么？"></a>有没有进一步深入的价值？为什么？”&gt;&lt;a href=”#有没有进一步深入的价值？为什么？</h3><p>虽然Graph-based Parser目前与 SOTA Transition-based Parser的性能相当，在English Penn Treebank上达到UAS 95.7%，但是在LAS上还有差距，另外在Chinese Penn Treebank上UAS=89.3%，都有进一步研究的价值。</p><h3 id="列出该文弱点（或者是你觉得应该是什么问题，他解决的不好，你会如何解决？）"><a href="#列出该文弱点（或者是你觉得应该是什么问题，他解决的不好，你会如何解决？）" class="headerlink" title="列出该文弱点（或者是你觉得应该是什么问题，他解决的不好，你会如何解决？）"></a>列出该文弱点（或者是你觉得应该是什么问题，他解决的不好，你会如何解决？）</h3><p>没有发现，没有什么想法…..</p><h3 id="该文对你的启发是？"><a href="#该文对你的启发是？" class="headerlink" title="该文对你的启发是？"></a>该文对你的启发是？</h3><ul><li>超参的设置与选择进行了比较详细的分析，为我接下来的实验提供了一些经验帮助。</li><li>我将使用该模型应用于对中介语依存结构的解析中，虽然希望改进模型，但是对于模型改进毫无思路。。。。。。。。</li></ul><h3 id="列出其中有价值的需要进一步阅读的参考文献"><a href="#列出其中有价值的需要进一步阅读的参考文献" class="headerlink" title="列出其中有价值的需要进一步阅读的参考文献"></a>列出其中有价值的需要进一步阅读的参考文献</h3><ul><li>Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuzman Ganchev, Slav Petrov, and Michael Collins. Globally normalized transitionbased neural networks. In Association for Computational Linguistics, 2016.  URL <a href="https://arxiv.org/abs/1603.06042" target="_blank" rel="noopener">https://arxiv.org/abs/1603.06042</a></li><li>Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng Kong, Chris Dyer, Graham Neubig, and Noah A. Smith. What do recurrent neural network grammars learn about syntax? CoRR, abs/1611.05774, 2016.  URL <a href="http://arxiv.org/abs/1611.05774" target="_blank" rel="noopener">http://arxiv.org/abs/1611.05774</a></li><li>Danqi Chen and Christopher D Manning. A fast and accurate dependency parser using neural networks. In Proceedings of the conference on empirical methods in natural language processing, pp. 740–750, 2014</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Dependency-Biaffine-Attention-Neural-Dependency-Parsing&quot;&gt;&lt;a href=&quot;#Dependency-Biaffine-Attention-Neural-Dependency-Parsing&quot; class=&quot;h
      
    
    </summary>
    
      <category term="Papers" scheme="https://xiaoxiaoaurora.github.io/categories/Papers/"/>
    
      <category term="Dependency Parsing" scheme="https://xiaoxiaoaurora.github.io/categories/Papers/Dependency-Parsing/"/>
    
    
      <category term="Papers" scheme="https://xiaoxiaoaurora.github.io/tags/Papers/"/>
    
      <category term="论文阅读笔记" scheme="https://xiaoxiaoaurora.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Biaffine Attention" scheme="https://xiaoxiaoaurora.github.io/tags/Biaffine-Attention/"/>
    
      <category term="Dependency Parsing" scheme="https://xiaoxiaoaurora.github.io/tags/Dependency-Parsing/"/>
    
  </entry>
  
  <entry>
    <title>Python之range()在Python3中与python2中的区别</title>
    <link href="https://xiaoxiaoaurora.github.io/2019/04/02/Python%E4%B9%8Brange-%E5%9C%A8Python3%E4%B8%AD%E4%B8%8Epython2%E4%B8%AD%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>https://xiaoxiaoaurora.github.io/2019/04/02/Python之range-在Python3中与python2中的区别/</id>
    <published>2019-04-02T12:35:11.000Z</published>
    <updated>2019-04-04T09:34:19.440Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python-range-在Python3中与python2中的区别"><a href="#Python-range-在Python3中与python2中的区别" class="headerlink" title="Python | range()在Python3中与python2中的区别"></a>Python | range()在Python3中与python2中的区别</h1><p>python2中的range返回的是一个列表</p><p>python3中的range返回的是一个迭代值</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Python-range-在Python3中与python2中的区别&quot;&gt;&lt;a href=&quot;#Python-range-在Python3中与python2中的区别&quot; class=&quot;headerlink&quot; title=&quot;Python | range()在Python3
      
    
    </summary>
    
      <category term="Python" scheme="https://xiaoxiaoaurora.github.io/categories/Python/"/>
    
    
      <category term="Python" scheme="https://xiaoxiaoaurora.github.io/tags/Python/"/>
    
      <category term="range" scheme="https://xiaoxiaoaurora.github.io/tags/range/"/>
    
  </entry>
  
  <entry>
    <title>Python | @classmethod @staticmethod区别</title>
    <link href="https://xiaoxiaoaurora.github.io/2018/08/20/Python%E4%B8%AD@classmethod%20@staticmethod%E5%8C%BA%E5%88%AB/"/>
    <id>https://xiaoxiaoaurora.github.io/2018/08/20/Python中@classmethod @staticmethod区别/</id>
    <published>2018-08-20T11:48:21.000Z</published>
    <updated>2019-04-04T09:32:41.891Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python-classmethod-staticmethod区别"><a href="#Python-classmethod-staticmethod区别" class="headerlink" title="Python | @classmethod @staticmethod区别"></a>Python | @classmethod @staticmethod区别</h1><p>Python中有三种方式定义类方法：  </p><ul><li>常规方法；   </li><li>@classmethod修饰方法；  </li><li>@staticmathod修饰方式。    </li></ul><p><img src="https://i.loli.net/2018/08/20/5b7ab046dd11c.jpg" alt>  </p><p>执行：<br> <img src="https://i.loli.net/2018/08/20/5b7ab06b10d9a.jpg" alt></p><p>输出：</p><p><img src="https://i.loli.net/2018/08/20/5b7ab0aea03a5.jpg" alt></p><h1 id="1-定义方式"><a href="#1-定义方式" class="headerlink" title="1. 定义方式"></a>1. 定义方式</h1><p>普通的类方法foo()需要通过self参数隐式的传递当前类对象的实例。@classmethod修饰的方法class_foo()需要通过cls参数传递当前的类对象。@staticmethod修饰的方法定义与普通函数是一样的。  </p><p>self和cls的区别不是强制的，只是PEP8中一种编程风格，<strong>self通常用作实例方法的第一参数，cls通常用作类方法的第一参数。即通常用self来传递当前类对象的实例，cls传递当前类对象。</strong>  </p><h1 id="2-绑定对象"><a href="#2-绑定对象" class="headerlink" title="2. 绑定对象"></a>2. 绑定对象</h1><pre><code># foo方法绑定对象A的实例，class_foo方法绑定对象A，static_foo没有参数绑定&gt;&gt;&gt; print(a.foo)  </code></pre><p>输出：<br><img src="https://i.loli.net/2018/08/20/5b7ab28f14d91.jpg" alt></p><pre><code>&gt;&gt;&gt; print(a.class_foo)&lt;bound method A.class_foo of &lt;class &apos;__main__.A&apos;&gt;&gt;&gt;&gt;&gt; print(a.static_foo)&lt;function A.static_foo at 0x000001A1026F5840&gt;</code></pre><h1 id="3-调用方式"><a href="#3-调用方式" class="headerlink" title="3. 调用方式"></a>3. 调用方式</h1><p>foo可通过实例a调用，类对象A直接调用会参数错误。  </p><pre><code>&gt;&gt;&gt; a.foo(l)  </code></pre><p>输出：<br><img src="https://i.loli.net/2018/08/20/5b7ab4a5cfca5.jpg" alt>  </p><pre><code>A.foo(1)</code></pre><p>输出：<br><img src="https://i.loli.net/2018/08/20/5b7ab4c251d20.jpg" alt>  </p><p>但foo如下方式可以使用正常，显式的传递实例参数a。  </p><pre><code>A.foo(a, 1)</code></pre><p>输出：<br><img src="https://i.loli.net/2018/08/20/5b7ab55e50545.jpg" alt></p><p>class_foo通过类对象或对象实例调用。  </p><pre><code>&gt;&gt;&gt; A.class_foo(1)  </code></pre><p>输出：<br><img src="https://i.loli.net/2018/08/20/5b7ab80f58bf6.jpg" alt>  </p><pre><code>&gt;&gt;&gt; a.class_foo(1)</code></pre><p>输出：<br><img src="https://i.loli.net/2018/08/20/5b7ab849f3b91.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Python-classmethod-staticmethod区别&quot;&gt;&lt;a href=&quot;#Python-classmethod-staticmethod区别&quot; class=&quot;headerlink&quot; title=&quot;Python | @classmethod @sta
      
    
    </summary>
    
      <category term="Python" scheme="https://xiaoxiaoaurora.github.io/categories/Python/"/>
    
    
      <category term="Python" scheme="https://xiaoxiaoaurora.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>阅读笔记 | 语言和认知中的跨语言影响</title>
    <link href="https://xiaoxiaoaurora.github.io/2018/08/10/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9A%E8%AF%AD%E8%A8%80%E5%92%8C%E8%AE%A4%E7%9F%A5%E4%B8%AD%E7%9A%84%E8%B7%A8%E8%AF%AD%E8%A8%80%E5%BD%B1%E5%93%8D/"/>
    <id>https://xiaoxiaoaurora.github.io/2018/08/10/阅读笔记：语言和认知中的跨语言影响/</id>
    <published>2018-08-10T01:48:21.000Z</published>
    <updated>2019-04-04T10:05:17.486Z</updated>
    
    <content type="html"><![CDATA[<h1 id="阅读笔记-语言和认知中的跨语言影响"><a href="#阅读笔记-语言和认知中的跨语言影响" class="headerlink" title="阅读笔记 | 语言和认知中的跨语言影响"></a>阅读笔记 | 语言和认知中的跨语言影响</h1><p><img src="https://i.loli.net/2018/08/10/5b6d49edc57d6.jpg" alt>  </p><ul><li>Crosslinguistic Influence 跨语言影响,CLI</li></ul><h2 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h2><p>我们对于CLI所了解的内容与那些尚待发现的内容相比起来无疑是相形见绌的，但是，自1989年起，关于CLI的研究早已经进入了一个新的时代。与之前在transfer上的工作相比，新的时代表现出了四种特征。  </p><ul><li>第一，CLI不再是一种没有原则标准来确定其在语言数据中存在或不存在的现象(cf. Meisel, 1983)。  </li><li><p>第二，CLI不再被研究者们简单地视为一个背景（background）、中介（mediating）或者中介变量（intervening variable），只是作为一种解释（例如，对所讨论的语言行为的解释），但从来不作为一个待解释的项（例如，要解释的现象）。 <font color="E066FF">这里有点绕，贴出原文</font><br><img src="https://i.loli.net/2018/08/10/5b6d4e79ab112.jpg" alt>  </p></li><li><p>第三，在Transfer研究的新时代下，CLI不再是一个缺乏明确的理论地位的模糊概念。的确，本书的主要目标是探索、突出和详细地阐述其跨学科理论的发展现状</p></li><li>最后，现在对CLI的本质的研究已经超出了语言知识的范畴，并深入探讨了CLI的认知基础。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;阅读笔记-语言和认知中的跨语言影响&quot;&gt;&lt;a href=&quot;#阅读笔记-语言和认知中的跨语言影响&quot; class=&quot;headerlink&quot; title=&quot;阅读笔记 | 语言和认知中的跨语言影响&quot;&gt;&lt;/a&gt;阅读笔记 | 语言和认知中的跨语言影响&lt;/h1&gt;&lt;p&gt;&lt;img s
      
    
    </summary>
    
      <category term="Papers" scheme="https://xiaoxiaoaurora.github.io/categories/Papers/"/>
    
      <category term="阅读笔记" scheme="https://xiaoxiaoaurora.github.io/categories/Papers/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Papers" scheme="https://xiaoxiaoaurora.github.io/tags/Papers/"/>
    
      <category term="CLI" scheme="https://xiaoxiaoaurora.github.io/tags/CLI/"/>
    
  </entry>
  
  <entry>
    <title>NLP|中英文名词对照附录</title>
    <link href="https://xiaoxiaoaurora.github.io/2018/07/23/NLP%E4%B9%8B%E4%B8%AD%E8%8B%B1%E6%96%87%E5%90%8D%E8%AF%8D%E5%AF%B9%E7%85%A7/"/>
    <id>https://xiaoxiaoaurora.github.io/2018/07/23/NLP之中英文名词对照/</id>
    <published>2018-07-23T08:12:18.000Z</published>
    <updated>2019-04-04T10:00:53.138Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NLP-中英文名词对照附录"><a href="#NLP-中英文名词对照附录" class="headerlink" title="NLP|中英文名词对照附录"></a><center>NLP|中英文名词对照附录</center></h1><font size="1" color="#9A32CD">英语不好，记性差，碰到专业术语总是抓狂（@_@）。主要是一些缩略词、语言学常用词。</font>  <h2 id="句法标注"><a href="#句法标注" class="headerlink" title="句法标注"></a>句法标注</h2><ol><li>TLE: Treebank of Learner English  学习者英语树库  </li><li>SALLE: Syntacticlly Annotating Learner Language of English 英语学习者语言的句法标注（也是一个treebank）  </li><li>UD: Universal Dependencies 通用依存  </li><li>POS： Part of Speech  词性</li><li>Literal Annotation  字面标注</li><li>Lemma 词元</li><li>Dependency Annotation 依存标注</li><li>Word Segmentation 分词；中文切词（即把一句话分成一个词的序列。）  </li><li>Morphological 形态学的; 构形<ul><li>morphologically 词法地；形态学上地  </li><li>morphology 形态学，形态论；[语] 词法，[语] 词态学  </li><li>morph 词态；语素；变体  </li><li>morpheme [语] 词素；形态素  </li></ul></li></ol><ol start="10"><li><p>Distributional     分布式的； 分布</p><ul><li>distributive     [语] 分配词    </li></ul></li><li><p>Word formation     构词 词性转换 单词构成  </p></li><li>linguistic form     语言形式(如后缀、词素、单词、短词、句子等); 语言形态; 语言结构  </li></ol><h2 id="other"><a href="#other" class="headerlink" title="other"></a><strong>other</strong></h2><p>activation 激活值<br>activation function 激活函数<br>additive noise 加性噪声<br>autoencoder 自编码器<br>Autoencoders 自编码算法<br>average firing rate 平均激活率<br>average sum-of-squares error 均方差<br>backpropagation 后向传播<br>basis 基<br>basis feature vectors 特征基向量<br>batch gradient ascent 批量梯度上升法<br>Bayesian regularization method 贝叶斯规则化方法<br>Bernoulli random variable 伯努利随机变量<br>bias term 偏置项<br>binary classfication 二元分类<br>class labels 类型标记<br>concatenation 级联<br>conjugate gradient 共轭梯度<br>contiguous groups 联通区域<br>convex optimization software 凸优化软件<br>convolution 卷积<br>cost function 代价函数<br>covariance matrix 协方差矩阵<br>DC component 直流分量<br>decorrelation 去相关<br>degeneracy 退化<br>demensionality reduction 降维<br>derivative 导函数<br>diagonal 对角线<br>diffusion of gradients 梯度的弥散<br>eigenvalue 特征值<br>eigenvector 特征向量<br>error term 残差<br>feature matrix 特征矩阵<br>feature standardization 特征标准化<br>feedforward architectures 前馈结构算法<br>feedforward neural network 前馈神经网络<br>feedforward pass 前馈传导<br>fine-tuned 微调<br>first-order feature 一阶特征<br>forward pass 前向传导<br>forward propagation 前向传播<br>Gaussian prior 高斯先验概率<br>generative model 生成模型<br>gradient descent 梯度下降<br>Greedy layer-wise training 逐层贪婪训练方法<br>grouping matrix 分组矩阵<br>Hadamard product 阿达马乘积<br>Hessian matrix Hessian 矩阵<br>hidden layer 隐含层<br>hidden units 隐藏神经元<br>Hierarchical grouping 层次型分组<br>higher-order features 更高阶特征<br>highly non-convex optimization problem 高度非凸的优化问题<br>histogram 直方图<br>hyperbolic tangent 双曲正切函数<br>hypothesis 估值，假设<br>identity activation function 恒等激励函数<br>IID 独立同分布<br>illumination 照明<br>inactive 抑制<br>independent component analysis 独立成份分析<br>input domains 输入域<br>input layer 输入层<br>intensity 亮度/灰度<br>intercept term 截距<br>KL divergence 相对熵<br>KL divergence KL分散度<br>k-Means K-均值<br>learning rate 学习速率<br>least squares 最小二乘法<br>linear correspondence 线性响应<br>linear superposition 线性叠加<br>line-search algorithm 线搜索算法<br>local mean subtraction 局部均值消减<br>local optima 局部最优解<br>logistic regression 逻辑回归<br>loss function 损失函数<br>low-pass filtering 低通滤波<br>magnitude 幅值<br>MAP 极大后验估计<br>maximum likelihood estimation 极大似然估计<br>mean 平均值<br>MFCC Mel 倒频系数<br>multi-class classification 多元分类<br>neural networks 神经网络<br>neuron 神经元<br>Newton’s method 牛顿法<br>non-convex function 非凸函数<br>non-linear feature 非线性特征<br>norm 范式<br>norm bounded 有界范数<br>norm constrained 范数约束<br>normalization 归一化<br>numerical roundoff errors 数值舍入误差<br>numerically checking 数值检验<br>numerically reliable 数值计算上稳定<br>object detection 物体检测<br>objective function 目标函数<br>off-by-one error 缺位错误<br>orthogonalization 正交化<br>output layer 输出层<br>overall cost function 总体代价函数<br>over-complete basis 超完备基<br>over-fitting 过拟合<br>parts of objects 目标的部件<br>part-whole decompostion 部分-整体分解<br>PCA 主元分析<br>penalty term 惩罚因子<br>per-example mean subtraction 逐样本均值消减<br>pooling 池化<br>pretrain 预训练<br>principal components analysis 主成份分析<br>quadratic constraints 二次约束<br>RBMs 受限Boltzman机<br>reconstruction based models 基于重构的模型<br>reconstruction cost 重建代价<br>reconstruction term 重构项<br>redundant 冗余<br>reflection matrix 反射矩阵<br>regularization 正则化<br>regularization term 正则化项<br>rescaling 缩放<br>robust 鲁棒性<br>run 行程<br>second-order feature 二阶特征<br>sigmoid activation function S型激励函数<br>significant digits 有效数字<br>singular value 奇异值<br>singular vector 奇异向量<br>smoothed L1 penalty 平滑的L1范数惩罚<br>Smoothed topographic L1 sparsity penalty 平滑地形L1稀疏惩罚函数<br>smoothing 平滑<br>Softmax Regresson Softmax回归<br>sorted in decreasing order 降序排列<br>source features 源特征<br>sparse autoencoder 消减归一化<br>Sparsity 稀疏性<br>sparsity parameter 稀疏性参数<br>sparsity penalty 稀疏惩罚<br>square function 平方函数<br>squared-error 方差<br>stationary 平稳性（不变性）<br>stationary stochastic process 平稳随机过程<br>step-size 步长值<br>supervised learning 监督学习<br>symmetric positive semi-definite matrix 对称半正定矩阵<br>symmetry breaking 对称失效<br>tanh function 双曲正切函数<br>the average activation 平均活跃度<br>the derivative checking method 梯度验证方法<br>the empirical distribution 经验分布函数<br>the energy function 能量函数<br>the Lagrange dual 拉格朗日对偶函数<br>the log likelihood 对数似然函数<br>the pixel intensity value 像素灰度值<br>the rate of convergence 收敛速度<br>topographic cost term 拓扑代价项<br>topographic ordered 拓扑秩序<br>transformation 变换<br>translation invariant 平移不变性<br>trivial answer 平凡解<br>under-complete basis 不完备基<br>unrolling 组合扩展<br>unsupervised learning 无监督学习<br>variance 方差<br>vecotrized implementation 向量化实现<br>vectorization 矢量化<br>visual cortex 视觉皮层<br>weight decay 权重衰减<br>weighted average 加权平均值<br>whitening 白化<br>zero-mean 均值为零</p><p>Letter A<br>Accumulated error backpropagation 累积误差逆传播<br>Activation Function 激活函数<br>Adaptive Resonance Theory/ART 自适应谐振理论<br>Addictive model 加性学习<br>Adversarial Networks 对抗网络<br>Affine Layer 仿射层<br>Affinity matrix 亲和矩阵<br>Agent 代理 / 智能体<br>Algorithm 算法<br>Alpha-beta pruning α-β剪枝<br>Anomaly detection 异常检测<br>Approximation 近似<br>Area Under ROC Curve／AUC Roc 曲线下面积<br>Artificial General Intelligence/AGI 通用人工智能<br>Artificial Intelligence/AI 人工智能<br>Association analysis 关联分析<br>Attention mechanism 注意力机制<br>Attribute conditional independence assumption 属性条件独立性假设<br>Attribute space 属性空间<br>Attribute value 属性值<br>Autoencoder 自编码器<br>Automatic speech recognition 自动语音识别<br>Automatic summarization 自动摘要<br>Average gradient 平均梯度<br>Average-Pooling 平均池化</p><p>Letter B<br>Backpropagation Through Time 通过时间的反向传播<br>Backpropagation/BP 反向传播<br>Base learner 基学习器<br>Base learning algorithm 基学习算法<br>Batch Normalization/BN 批量归一化<br>Bayes decision rule 贝叶斯判定准则<br>Bayes Model Averaging／BMA 贝叶斯模型平均<br>Bayes optimal classifier 贝叶斯最优分类器<br>Bayesian decision theory 贝叶斯决策论<br>Bayesian network 贝叶斯网络<br>Between-class scatter matrix 类间散度矩阵<br>Bias 偏置 / 偏差<br>Bias-variance decomposition 偏差-方差分解<br>Bias-Variance Dilemma 偏差 – 方差困境<br>Bi-directional Long-Short Term Memory/Bi-LSTM 双向长短期记忆<br>Binary classification 二分类<br>Binomial test 二项检验<br>Bi-partition 二分法<br>Boltzmann machine 玻尔兹曼机<br>Bootstrap sampling 自助采样法／可重复采样／有放回采样<br>Bootstrapping 自助法<br>Break-Event Point／BEP 平衡点</p><p>Letter C<br>Calibration 校准<br>Cascade-Correlation 级联相关<br>Categorical attribute 离散属性<br>Class-conditional probability 类条件概率<br>Classification and regression tree/CART 分类与回归树<br>Classifier 分类器<br>Class-imbalance 类别不平衡<br>Closed -form 闭式<br>Cluster 簇/类/集群<br>Cluster analysis 聚类分析<br>Clustering 聚类<br>Clustering ensemble 聚类集成<br>Co-adapting 共适应<br>Coding matrix 编码矩阵<br>COLT 国际学习理论会议<br>Committee-based learning 基于委员会的学习<br>Competitive learning 竞争型学习<br>Component learner 组件学习器<br>Comprehensibility 可解释性<br>Computation Cost 计算成本<br>Computational Linguistics 计算语言学<br>Computer vision 计算机视觉<br>Concept drift 概念漂移<br>Concept Learning System /CLS 概念学习系统<br>Conditional entropy 条件熵<br>Conditional mutual information 条件互信息<br>Conditional Probability Table／CPT 条件概率表<br>Conditional random field/CRF 条件随机场<br>Conditional risk 条件风险<br>Confidence 置信度<br>Confusion matrix 混淆矩阵<br>Connection weight 连接权<br>Connectionism 连结主义<br>Consistency 一致性／相合性<br>Contingency table 列联表<br>Continuous attribute 连续属性<br>Convergence 收敛<br>Conversational agent 会话智能体<br>Convex quadratic programming 凸二次规划<br>Convexity 凸性<br>Convolutional neural network/CNN 卷积神经网络<br>Co-occurrence 同现<br>Correlation coefficient 相关系数<br>Cosine similarity 余弦相似度<br>Cost curve 成本曲线<br>Cost Function 成本函数<br>Cost matrix 成本矩阵<br>Cost-sensitive 成本敏感<br>Cross entropy 交叉熵<br>Cross validation 交叉验证<br>Crowdsourcing 众包<br>Curse of dimensionality 维数灾难<br>Cut point 截断点<br>Cutting plane algorithm 割平面法</p><p>Letter D<br>Data mining 数据挖掘<br>Data set 数据集<br>Decision Boundary 决策边界<br>Decision stump 决策树桩<br>Decision tree 决策树／判定树<br>Deduction 演绎<br>Deep Belief Network 深度信念网络<br>Deep Convolutional Generative Adversarial Network/DCGAN 深度卷积生成对抗网络<br>Deep learning 深度学习<br>Deep neural network/DNN 深度神经网络<br>Deep Q-Learning 深度 Q 学习<br>Deep Q-Network 深度 Q 网络<br>Density estimation 密度估计<br>Density-based clustering 密度聚类<br>Differentiable neural computer 可微分神经计算机<br>Dimensionality reduction algorithm 降维算法<br>Directed edge 有向边<br>Disagreement measure 不合度量<br>Discriminative model 判别模型<br>Discriminator 判别器<br>Distance measure 距离度量<br>Distance metric learning 距离度量学习<br>Distribution 分布<br>Divergence 散度<br>Diversity measure 多样性度量／差异性度量<br>Domain adaption 领域自适应<br>Downsampling 下采样<br>D-separation （Directed separation） 有向分离<br>Dual problem 对偶问题<br>Dummy node 哑结点<br>Dynamic Fusion 动态融合<br>Dynamic programming 动态规划</p><p>Letter E<br>Eigenvalue decomposition 特征值分解<br>Embedding 嵌入<br>Emotional analysis 情绪分析<br>Empirical conditional entropy 经验条件熵<br>Empirical entropy 经验熵<br>Empirical error 经验误差<br>Empirical risk 经验风险<br>End-to-End 端到端<br>Energy-based model 基于能量的模型<br>Ensemble learning 集成学习<br>Ensemble pruning 集成修剪<br>Error Correcting Output Codes／ECOC 纠错输出码<br>Error rate 错误率<br>Error-ambiguity decomposition 误差-分歧分解<br>Euclidean distance 欧氏距离<br>Evolutionary computation 演化计算<br>Expectation-Maximization 期望最大化<br>Expected loss 期望损失<br>Exploding Gradient Problem 梯度爆炸问题<br>Exponential loss function 指数损失函数<br>Extreme Learning Machine/ELM 超限学习机</p><p>Letter F<br>Factorization 因子分解<br>False negative 假负类<br>False positive 假正类<br>False Positive Rate/FPR 假正例率<br>Feature engineering 特征工程<br>Feature selection 特征选择<br>Feature vector 特征向量<br>Featured Learning 特征学习<br>Feedforward Neural Networks/FNN 前馈神经网络<br>Fine-tuning 微调<br>Flipping output 翻转法<br>Fluctuation 震荡<br>Forward stagewise algorithm 前向分步算法<br>Frequentist 频率主义学派<br>Full-rank matrix 满秩矩阵<br>Functional neuron 功能神经元</p><p>Letter G<br>Gain ratio 增益率<br>Game theory 博弈论<br>Gaussian kernel function 高斯核函数<br>Gaussian Mixture Model 高斯混合模型<br>General Problem Solving 通用问题求解<br>Generalization 泛化<br>Generalization error 泛化误差<br>Generalization error bound 泛化误差上界<br>Generalized Lagrange function 广义拉格朗日函数<br>Generalized linear model 广义线性模型<br>Generalized Rayleigh quotient 广义瑞利商<br>Generative Adversarial Networks/GAN 生成对抗网络<br>Generative Model 生成模型<br>Generator 生成器<br>Genetic Algorithm/GA 遗传算法<br>Gibbs sampling 吉布斯采样<br>Gini index 基尼指数<br>Global minimum 全局最小<br>Global Optimization 全局优化<br>Gradient boosting 梯度提升<br>Gradient Descent 梯度下降<br>Graph theory 图论<br>Ground-truth 真相／真实</p><p>Letter H<br>Hard margin 硬间隔<br>Hard voting 硬投票<br>Harmonic mean 调和平均<br>Hesse matrix 海塞矩阵<br>Hidden dynamic model 隐动态模型<br>Hidden layer 隐藏层<br>Hidden Markov Model/HMM 隐马尔可夫模型<br>Hierarchical clustering 层次聚类<br>Hilbert space 希尔伯特空间<br>Hinge loss function 合页损失函数<br>Hold-out 留出法<br>Homogeneous 同质<br>Hybrid computing 混合计算<br>Hyperparameter 超参数<br>Hypothesis 假设<br>Hypothesis test 假设验证</p><p>Letter I<br>ICML 国际机器学习会议<br>Improved iterative scaling/IIS 改进的迭代尺度法<br>Incremental learning 增量学习<br>Independent and identically distributed/i.i.d. 独立同分布<br>Independent Component Analysis/ICA 独立成分分析<br>Indicator function 指示函数<br>Individual learner 个体学习器<br>Induction 归纳<br>Inductive bias 归纳偏好<br>Inductive learning 归纳学习<br>Inductive Logic Programming／ILP 归纳逻辑程序设计<br>Information entropy 信息熵<br>Information gain 信息增益<br>Input layer 输入层<br>Insensitive loss 不敏感损失<br>Inter-cluster similarity 簇间相似度<br>International Conference for Machine Learning/ICML 国际机器学习大会<br>Intra-cluster similarity 簇内相似度<br>Intrinsic value 固有值<br>Isometric Mapping/Isomap 等度量映射<br>Isotonic regression 等分回归<br>Iterative Dichotomiser 迭代二分器</p><p>Letter K<br>Kernel method 核方法<br>Kernel trick 核技巧<br>Kernelized Linear Discriminant Analysis／KLDA 核线性判别分析<br>K-fold cross validation k 折交叉验证／k 倍交叉验证<br>K-Means Clustering K – 均值聚类<br>K-Nearest Neighbours Algorithm/KNN K近邻算法<br>Knowledge base 知识库<br>Knowledge Representation 知识表征</p><p>Letter L<br>Label space 标记空间<br>Lagrange duality 拉格朗日对偶性<br>Lagrange multiplier 拉格朗日乘子<br>Laplace smoothing 拉普拉斯平滑<br>Laplacian correction 拉普拉斯修正<br>Latent Dirichlet Allocation 隐狄利克雷分布<br>Latent semantic analysis 潜在语义分析<br>Latent variable 隐变量<br>Lazy learning 懒惰学习<br>Learner 学习器<br>Learning by analogy 类比学习<br>Learning rate 学习率<br>Learning Vector Quantization/LVQ 学习向量量化<br>Least squares regression tree 最小二乘回归树<br>Leave-One-Out/LOO 留一法<br>linear chain conditional random field 线性链条件随机场<br>Linear Discriminant Analysis／LDA 线性判别分析<br>Linear model 线性模型<br>Linear Regression 线性回归<br>Link function 联系函数<br>Local Markov property 局部马尔可夫性<br>Local minimum 局部最小<br>Log likelihood 对数似然<br>Log odds／logit 对数几率<br>Logistic Regression Logistic 回归<br>Log-likelihood 对数似然<br>Log-linear regression 对数线性回归<br>Long-Short Term Memory/LSTM 长短期记忆<br>Loss function 损失函数</p><p>Letter M<br>Machine translation/MT 机器翻译<br>Macron-P 宏查准率<br>Macron-R 宏查全率<br>Majority voting 绝对多数投票法<br>Manifold assumption 流形假设<br>Manifold learning 流形学习<br>Margin theory 间隔理论<br>Marginal distribution 边际分布<br>Marginal independence 边际独立性<br>Marginalization 边际化<br>Markov Chain Monte Carlo/MCMC 马尔可夫链蒙特卡罗方法<br>Markov Random Field 马尔可夫随机场<br>Maximal clique 最大团<br>Maximum Likelihood Estimation/MLE 极大似然估计／极大似然法<br>Maximum margin 最大间隔<br>Maximum weighted spanning tree 最大带权生成树<br>Max-Pooling 最大池化<br>Mean squared error 均方误差<br>Meta-learner 元学习器<br>Metric learning 度量学习<br>Micro-P 微查准率<br>Micro-R 微查全率<br>Minimal Description Length/MDL 最小描述长度<br>Minimax game 极小极大博弈<br>Misclassification cost 误分类成本<br>Mixture of experts 混合专家<br>Momentum 动量<br>Moral graph 道德图／端正图<br>Multi-class classification 多分类<br>Multi-document summarization 多文档摘要<br>Multi-layer feedforward neural networks 多层前馈神经网络<br>Multilayer Perceptron/MLP 多层感知器<br>Multimodal learning 多模态学习<br>Multiple Dimensional Scaling 多维缩放<br>Multiple linear regression 多元线性回归<br>Multi-response Linear Regression ／MLR 多响应线性回归<br>Mutual information 互信息</p><p>Letter N<br>Naive bayes 朴素贝叶斯<br>Naive Bayes Classifier 朴素贝叶斯分类器<br>Named entity recognition 命名实体识别<br>Nash equilibrium 纳什均衡<br>Natural language generation/NLG 自然语言生成<br>Natural language processing 自然语言处理<br>Negative class 负类<br>Negative correlation 负相关法<br>Negative Log Likelihood 负对数似然<br>Neighbourhood Component Analysis/NCA 近邻成分分析<br>Neural Machine Translation 神经机器翻译<br>Neural Turing Machine 神经图灵机<br>Newton method 牛顿法<br>NIPS 国际神经信息处理系统会议<br>No Free Lunch Theorem／NFL 没有免费的午餐定理<br>Noise-contrastive estimation 噪音对比估计<br>Nominal attribute 列名属性<br>Non-convex optimization 非凸优化<br>Nonlinear model 非线性模型<br>Non-metric distance 非度量距离<br>Non-negative matrix factorization 非负矩阵分解<br>Non-ordinal attribute 无序属性<br>Non-Saturating Game 非饱和博弈<br>Norm 范数<br>Normalization 归一化<br>Nuclear norm 核范数<br>Numerical attribute 数值属性</p><p>Letter O<br>Objective function 目标函数<br>Oblique decision tree 斜决策树<br>Occam’s razor 奥卡姆剃刀<br>Odds 几率<br>Off-Policy 离策略<br>One shot learning 一次性学习<br>One-Dependent Estimator／ODE 独依赖估计<br>On-Policy 在策略<br>Ordinal attribute 有序属性<br>Out-of-bag estimate 包外估计<br>Output layer 输出层<br>Output smearing 输出调制法<br>Overfitting 过拟合／过配<br>Oversampling 过采样</p><p>Letter P<br>Paired t-test 成对 t 检验<br>Pairwise 成对型<br>Pairwise Markov property 成对马尔可夫性<br>Parameter 参数<br>Parameter estimation 参数估计<br>Parameter tuning 调参<br>Parse tree 解析树<br>Particle Swarm Optimization/PSO 粒子群优化算法<br>Part-of-speech tagging 词性标注<br>Perceptron 感知机<br>Performance measure 性能度量<br>Plug and Play Generative Network 即插即用生成网络<br>Plurality voting 相对多数投票法<br>Polarity detection 极性检测<br>Polynomial kernel function 多项式核函数<br>Pooling 池化<br>Positive class 正类<br>Positive definite matrix 正定矩阵<br>Post-hoc test 后续检验<br>Post-pruning 后剪枝<br>potential function 势函数<br>Precision 查准率／准确率<br>Prepruning 预剪枝<br>Principal component analysis/PCA 主成分分析<br>Principle of multiple explanations 多释原则<br>Prior 先验<br>Probability Graphical Model 概率图模型<br>Proximal Gradient Descent/PGD 近端梯度下降<br>Pruning 剪枝<br>Pseudo-label 伪标记</p><p>Letter Q<br>Quantized Neural Network 量子化神经网络<br>Quantum computer 量子计算机<br>Quantum Computing 量子计算<br>Quasi Newton method 拟牛顿法</p><p>Letter R<br>Radial Basis Function／RBF 径向基函数<br>Random Forest Algorithm 随机森林算法<br>Random walk 随机漫步<br>Recall 查全率／召回率<br>Receiver Operating Characteristic/ROC 受试者工作特征<br>Rectified Linear Unit/ReLU 线性修正单元<br>Recurrent Neural Network 循环神经网络<br>Recursive neural network 递归神经网络<br>Reference model 参考模型<br>Regression 回归<br>Regularization 正则化<br>Reinforcement learning/RL 强化学习<br>Representation learning 表征学习<br>Representer theorem 表示定理<br>reproducing kernel Hilbert space/RKHS 再生核希尔伯特空间<br>Re-sampling 重采样法<br>Rescaling 再缩放<br>Residual Mapping 残差映射<br>Residual Network 残差网络<br>Restricted Boltzmann Machine/RBM 受限玻尔兹曼机<br>Restricted Isometry Property/RIP 限定等距性<br>Re-weighting 重赋权法<br>Robustness 稳健性/鲁棒性<br>Root node 根结点<br>Rule Engine 规则引擎<br>Rule learning 规则学习</p><p>Letter S<br>Saddle point 鞍点<br>Sample space 样本空间<br>Sampling 采样<br>Score function 评分函数<br>Self-Driving 自动驾驶<br>Self-Organizing Map／SOM 自组织映射<br>Semi-naive Bayes classifiers 半朴素贝叶斯分类器<br>Semi-Supervised Learning 半监督学习<br>semi-Supervised Support Vector Machine 半监督支持向量机<br>Sentiment analysis 情感分析<br>Separating hyperplane 分离超平面<br>Sigmoid function Sigmoid 函数<br>Similarity measure 相似度度量<br>Simulated annealing 模拟退火<br>Simultaneous localization and mapping 同步定位与地图构建<br>Singular Value Decomposition 奇异值分解<br>Slack variables 松弛变量<br>Smoothing 平滑<br>Soft margin 软间隔<br>Soft margin maximization 软间隔最大化<br>Soft voting 软投票<br>Sparse representation 稀疏表征<br>Sparsity 稀疏性<br>Specialization 特化<br>Spectral Clustering 谱聚类<br>Speech Recognition 语音识别<br>Splitting variable 切分变量<br>Squashing function 挤压函数<br>Stability-plasticity dilemma 可塑性-稳定性困境<br>Statistical learning 统计学习<br>Status feature function 状态特征函<br>Stochastic gradient descent 随机梯度下降<br>Stratified sampling 分层采样<br>Structural risk 结构风险<br>Structural risk minimization/SRM 结构风险最小化<br>Subspace 子空间<br>Supervised learning 监督学习／有导师学习<br>support vector expansion 支持向量展式<br>Support Vector Machine/SVM 支持向量机<br>Surrogat loss 替代损失<br>Surrogate function 替代函数<br>Symbolic learning 符号学习<br>Symbolism 符号主义<br>Synset 同义词集</p><p>Letter T<br>T-Distribution Stochastic Neighbour Embedding/t-SNE T – 分布随机近邻嵌入<br>Tensor 张量<br>Tensor Processing Units/TPU 张量处理单元<br>The least square method 最小二乘法<br>Threshold 阈值<br>Threshold logic unit 阈值逻辑单元<br>Threshold-moving 阈值移动<br>Time Step 时间步骤<br>Tokenization 标记化<br>Training error 训练误差<br>Training instance 训练示例／训练例<br>Transductive learning 直推学习<br>Transfer learning 迁移学习<br>Treebank 树库<br>Tria-by-error 试错法<br>True negative 真负类<br>True positive 真正类<br>True Positive Rate/TPR 真正例率<br>Turing Machine 图灵机<br>Twice-learning 二次学习</p><p>Letter U<br>Underfitting 欠拟合／欠配<br>Undersampling 欠采样<br>Understandability 可理解性<br>Unequal cost 非均等代价<br>Unit-step function 单位阶跃函数<br>Univariate decision tree 单变量决策树<br>Unsupervised learning 无监督学习／无导师学习<br>Unsupervised layer-wise training 无监督逐层训练<br>Upsampling 上采样</p><p>Letter V<br>Vanishing Gradient Problem 梯度消失问题<br>Variational inference 变分推断<br>VC Theory VC维理论<br>Version space 版本空间<br>Viterbi algorithm 维特比算法<br>Von Neumann architecture 冯 · 诺伊曼架构</p><p>Letter W<br>Wasserstein GAN/WGAN Wasserstein生成对抗网络<br>Weak learner 弱学习器<br>Weight 权重<br>Weight sharing 权共享<br>Weighted voting 加权投票法<br>Within-class scatter matrix 类内散度矩阵<br>Word embedding 词嵌入<br>Word sense disambiguation 词义消歧</p><p>Letter Z<br>Zero-data learning 零数据学习<br>Zero-shot learning 零次学习</p><p>A<br>approximations近似值<br>arbitrary随意的<br>affine仿射的<br>arbitrary任意的<br>amino acid氨基酸<br>amenable经得起检验的<br>axiom公理，原则<br>abstract提取<br>architecture架构，体系结构；建造业<br>absolute绝对的<br>arsenal军火库<br>assignment分配<br>algebra线性代数<br>asymptotically无症状的<br>appropriate恰当的</p><p>B<br>bias偏差<br>brevity简短，简洁；短暂<br>broader广泛<br>briefly简短的<br>batch批量</p><p>C<br>convergence 收敛，集中到一点<br>convex凸的<br>contours轮廓<br>constraint约束<br>constant常理<br>commercial商务的<br>complementarity补充<br>coordinate ascent同等级上升<br>clipping剪下物；剪报；修剪<br>component分量；部件<br>continuous连续的<br>covariance协方差<br>canonical正规的，正则的<br>concave非凸的<br>corresponds相符合；相当；通信<br>corollary推论<br>concrete具体的事物，实在的东西<br>cross validation交叉验证<br>correlation相互关系<br>convention约定<br>cluster一簇<br>centroids 质心，形心<br>converge收敛<br>computationally计算(机)的<br>calculus计算</p><p>D<br>derive获得，取得<br>dual二元的<br>duality二元性；二象性；对偶性<br>derivation求导；得到；起源<br>denote预示，表示，是…的标志；意味着，[逻]指称<br>divergence 散度；发散性<br>dimension尺度，规格；维数<br>dot小圆点<br>distortion变形<br>density概率密度函数<br>discrete离散的<br>discriminative有识别能力的<br>diagonal对角<br>dispersion分散，散开<br>determinant决定因素<br>disjoint不相交的</p><p>E<br>encounter遇到<br>ellipses椭圆<br>equality等式<br>extra额外的<br>empirical经验；观察<br>ennmerate例举，计数<br>exceed超过，越出<br>expectation期望<br>efficient生效的<br>endow赋予<br>explicitly清楚的<br>exponential family指数家族<br>equivalently等价的</p><p>F<br>feasible可行的<br>forary初次尝试<br>finite有限的，限定的<br>forgo摒弃，放弃<br>fliter过滤<br>frequentist最常发生的<br>forward search前向式搜索<br>formalize使定形</p><p>G<br>generalized归纳的<br>generalization概括，归纳；普遍化；判断（根据不足）<br>guarantee保证；抵押品<br>generate形成，产生<br>geometric margins几何边界<br>gap裂口<br>generative生产的；有生产力的</p><p>H<br>heuristic启发式的；启发法；启发程序<br>hone怀恋；磨<br>hyperplane超平面</p><p>L<br>initial最初的<br>implement执行<br>intuitive凭直觉获知的<br>incremental增加的<br>intercept截距<br>intuitious直觉<br>instantiation例子<br>indicator指示物，指示器<br>interative重复的，迭代的<br>integral积分<br>identical相等的；完全相同的<br>indicate表示，指出<br>invariance不变性，恒定性<br>impose把…强加于<br>intermediate中间的<br>interpretation解释，翻译</p><p>J<br>joint distribution联合概率</p><p>L<br>lieu替代<br>logarithmic对数的，用对数表示的<br>latent潜在的<br>Leave-one-out cross validation留一法交叉验证</p><p>M<br>magnitude巨大<br>mapping绘图，制图；映射<br>matrix矩阵<br>mutual相互的，共同的<br>monotonically单调的<br>minor较小的，次要的<br>multinomial多项的<br>multi-class classification二分类问题</p><p>N<br>nasty讨厌的<br>notation标志，注释<br>naïve朴素的</p><p>O<br>obtain得到<br>oscillate摆动<br>optimization problem最优化问题<br>objective function目标函数<br>optimal最理想的<br>orthogonal(矢量，矩阵等)正交的<br>orientation方向<br>ordinary普通的<br>occasionally偶然的</p><p>P<br>partial derivative偏导数<br>property性质<br>proportional成比例的<br>primal原始的，最初的<br>permit允许<br>pseudocode伪代码<br>permissible可允许的<br>polynomial多项式<br>preliminary预备<br>precision精度<br>perturbation 不安，扰乱<br>poist假定，设想<br>positive semi-definite半正定的<br>parentheses圆括号<br>posterior probability后验概率<br>plementarity补充<br>pictorially图像的<br>parameterize确定…的参数<br>poisson distribution柏松分布<br>pertinent相关的</p><p>Q<br>quadratic二次的<br>quantity量，数量；分量<br>query疑问的</p><p>R<br>regularization使系统化；调整<br>reoptimize重新优化<br>restrict限制；限定；约束<br>reminiscent回忆往事的；提醒的；使人联想…的（of）<br>remark注意<br>random variable随机变量<br>respect考虑<br>respectively各自的；分别的<br>redundant过多的；冗余的</p><p>S<br>susceptible敏感的<br>stochastic可能的；随机的<br>symmetric对称的<br>sophisticated复杂的<br>spurious假的；伪造的<br>subtract减去；减法器<br>simultaneously同时发生地；同步地<br>suffice满足<br>scarce稀有的，难得的<br>split分解，分离<br>subset子集<br>statistic统计量<br>successive iteratious连续的迭代<br>scale标度<br>sort of有几分的<br>squares平方</p><p>T<br>trajectory轨迹<br>temporarily暂时的<br>terminology专用名词<br>tolerance容忍；公差<br>thumb翻阅<br>threshold阈，临界<br>theorem定理<br>tangent正弦</p><p>U<br>unit-length vector单位向量</p><p>V<br>valid有效的，正确的<br>variance方差<br>variable变量；变元<br>vocabulary词汇<br>valued经估价的；宝贵的</p><p>W<br>wrapper包装</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;NLP-中英文名词对照附录&quot;&gt;&lt;a href=&quot;#NLP-中英文名词对照附录&quot; class=&quot;headerlink&quot; title=&quot;NLP|中英文名词对照附录&quot;&gt;&lt;/a&gt;&lt;center&gt;NLP|中英文名词对照附录&lt;/center&gt;&lt;/h1&gt;&lt;font size=&quot;
      
    
    </summary>
    
      <category term="NLP" scheme="https://xiaoxiaoaurora.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="https://xiaoxiaoaurora.github.io/tags/NLP/"/>
    
      <category term="名词解释" scheme="https://xiaoxiaoaurora.github.io/tags/%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/"/>
    
  </entry>
  
  <entry>
    <title>学习记录之句法结构标注(一)</title>
    <link href="https://xiaoxiaoaurora.github.io/2018/07/09/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E4%B9%8B%E5%8F%A5%E6%B3%95%E7%BB%93%E6%9E%84%E6%A0%87%E6%B3%A8%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://xiaoxiaoaurora.github.io/2018/07/09/学习记录之句法结构标注（一）/</id>
    <published>2018-07-09T01:34:57.000Z</published>
    <updated>2019-04-04T10:03:09.774Z</updated>
    
    <content type="html"><![CDATA[<h1 id="学习记录之句法结构标注（一）"><a href="#学习记录之句法结构标注（一）" class="headerlink" title="学习记录之句法结构标注（一）"></a>学习记录之句法结构标注（一）</h1><hr><p><strong><font color="#836FFF">Note:</font></strong> </p><ul><li>关于学习者文本做句法标注  </li><li>针对汉语学习者尚且没有太多工作，汉语上，香港城市大学有一些相关工作。  </li><li>大致上是<strong>为中介语标注句法结构</strong>，进而可以做以下工作，例如给教师提供语言用例的检索、做中介语NLP等。  </li><li>该篇文章仅是在阅读论文中所做的基础记录，大部分是论文翻译内容。</li></ul><p><strong><font color="#98F5FF">Information：</font></strong>  </p><ul><li>中介语（Interlanguage）：也有人译为”过渡语”或”语际语”，是指在第二语言习得过程中，学习者通过一定的学习策略，在目的语输入的基础上所形成的一种既不同于其第一语言也不同于目的语，随着学习的进展向目的语逐渐过渡的动态的语言系统。  </li></ul><h2 id="一、关于句法标注（Syntactically-Annotating）"><a href="#一、关于句法标注（Syntactically-Annotating）" class="headerlink" title="一、关于句法标注（Syntactically Annotating）"></a>一、关于句法标注（Syntactically Annotating）</h2><p><strong>SALLE（Syntactically Annotating Learner Language of English）</strong></p><p>从英语学习者文本的句法标注相关内容进行了解。该任务主要就是对把英语作为第二语言的学习者所写的文本进行句法结构标注。该项目中的目标是对给定句子中存在的语言属性进行注释（annotate linguistic properties present），而不是对学习者的意思做出太多解释（interpretation），或者正确的形式应该是什么。为了达到这个目的，我们的注释方案根据句子中的上下文，并基于英语规则（目标语言），添加了几个关于每个单词的语言信息。  我们通过对依存关系进行注解来标记句子中的单词之间的句法关系，例如，一个词是另一个词的主语。  </p><p>A beta version of the guidelines we are using are available <a href="http://cl.indiana.edu/~salle/resources/salle-guidelines0.1.pdf" target="_blank" rel="noopener">here</a>.The decisions we have made (certainly needing refinement in some cases) point out many of the essential questions that need to be addressed for linguistically annotating learner data, and we hope they can stimulate discussion.</p><center>—-以上摘自（<a href="http://cl.indiana.edu/~salle/" target="_blank" rel="noopener">http://cl.indiana.edu/~salle/</a>）</center>  <h2 id="二、论文研读–《Universal-Dependencies-for-Learner-English》"><a href="#二、论文研读–《Universal-Dependencies-for-Learner-English》" class="headerlink" title="二、论文研读–《Universal Dependencies for Learner English》"></a>二、论文研读–《Universal Dependencies for Learner English》</h2><p><img src="https://i.loli.net/2018/07/04/5b3c7b17624a6.jpg" alt>  </p><p><img src="https://i.loli.net/2018/07/03/5b3af1a72a4fe.jpg" alt>  </p><p>Abstract: 本篇论文主要介绍了英语学习者的树库（TLE），是一个将英语作为第二语言（ESL）的第一个公共可用的语法树库（syntactic treebank）。<strong>TLE为5124个句子提供了手动注释的POS标签和通用依存（Universal Dependency，UD）树，</strong>这些句子来源于剑桥大学FCE（First Certificate in English ）语料库。UD注解和FCE中已存在的错误注解结合在一起，从而为每个句子的原始版本和错误修正后的版本提供完整的语法分析。进一步的描述了ESL注释指南，该指南允许对不符合语法的英语进行一致的句法处理。最后，<strong>在TLE数据集上对POS标记和依存关系解析性能进行基准测试，并测量语法错误对解析准确性的影响</strong>。我们设想树库支持第二语言习得的广泛的语言和计算研究以及自动处理不符合语法的语言。</p><h3 id="1、Introduction"><a href="#1、Introduction" class="headerlink" title="1、Introduction"></a>1、Introduction</h3><p>大多数世界范围内可用的英语文本都是由非母语者所产生的。这些文本中最明显的挑战就是语法错误，解决这些问题对于语言习得的科学研究和自然语言处理都是至关重要的。<strong>尽管非母语英语无处不在，但是目前还没有公共可用的ESL句法树库。<br>为了解决这个问题，我们提出了英语学习者树库（TLE，the<br>Treebank of Learner English），这是非英语母语的第一个资源，树库包含用POS标签和依存树手动注释的5,124个句子。</strong></p><ul><li>TLE句子是从FCE数据集中获取，这些句子是由来自10个不同母语背景的英语学习者所写。  </li><li>treebank使用了通用依存(UD)形式主义，提供跨不同语言的统一注释框架，且面向多语种NLP。</li></ul><p>以上两种使得treebank能够支持ESL的计算分析，它不仅使用基于英语的，而且还使用多种语言的方法，试图将ESL现象与本地语言语法联系起来。</p><ul><li>以学习者语言分析之前的工作为基础，制定了一套附加的注解约定，旨在对不符合语法的学习者语言进行统一处理。采用一个two-layer 分析，在这个分析中，每个句子的原始版本和纠正版本都提供了不同的语法注释。该方法是通过FCE语料中已存在的错误注释而启用的，该注释用于生成数据集的错误修正变体。</li></ul><p><strong>总的来说这篇论文主要有以下三点贡献：</strong>  </p><ul><li><ol><li>为ESL引入了第一个大型语法树库，该树库包括手工注释的POS标签和通用依存（UD）。  </li></ol></li><li><ol start="2"><li>为不符合语法的学习者英语描述了一种受语言驱动的注释方案，并通过注释者间协议分析为其一致性提供经验支持。  </li></ol></li><li><ol start="3"><li>作者在自己的数据集上对性能最佳的解析器进行了基准测试，并且评估了自动进行POS标注和依存分析的精确度对语法错误的影响。</li></ol></li></ul><p><strong><font color="4B0082">论文结构说明：</font></strong>    </p><ul><li>Section 2：Present an overview of the treebank.  </li><li>Section 3 and Section 4:Provide background information on the annotation project, and review the main annotation stages leading to the current form of the dataset.  </li><li><strong>Section 5: To Summary the ESL annotation guidelines.</strong>  </li><li>Section 6: Present the Inter-annotator agreement analysis.  </li><li><strong>Section 7: Parsing Experiments.</strong>  </li><li>Section 8: Related Work </li><li>Section 9: Conclusion </li></ul><h3 id="2、Treebank概述"><a href="#2、Treebank概述" class="headerlink" title="2、Treebank概述"></a>2、Treebank概述</h3><blockquote><p>使用的是NLTL句子分词器（<a href="http://www.nltk.org/api/nltk.tokenize.html" target="_blank" rel="noopener">http://www.nltk.org/api/nltk.tokenize.html</a>）的改编版进行句子级别切分。<br>Word level tokenization was generated using the Stanford PTB word tokenizer（<a href="http://nlp.stanford.edu/software/tokenizer.shtml" target="_blank" rel="noopener">http://nlp.stanford.edu/software/tokenizer.shtml</a>）。</p></blockquote><p>TLE是第一个以完全手动注释的方式构建的大规模英语树库。  </p><ol><li>TLE包含5124个英语通用依赖关系(UD)形式中带有POS标记和依存注释的句子。这些句子是从FCE语料得到，这是一组中高级英语学习者所写的文章，包含75个错误分类的错误注释。  </li><li>树库中的学习者来自10个母语不同的语言背景：汉语、法语、德语、意大利语、日语、韩语、葡萄牙语、西班牙语、俄语和土耳其语。每种母语背景下都随机挑选500个自动分段的句子，除此之外所选的句子必须至少包含一种不是符号或拼写的语法错误。  </li><li><p>TLE注解有两个版本：  </p><ul><li>学习者写的原始句子（有语法错误）；  </li><li>纠正的句子，是原始句子的语法变体，根据FCE数据集提供的手动错误注释纠正了句子中的所有语法错误。</li></ul></li></ol><p>由此产生的正确句子构成了标准英语的平行语料库。  </p><center><img src="https://i.loli.net/2018/07/04/5b3c88616775d.jpg" alt></center>    <h3 id="3、Annotator-Training"><a href="#3、Annotator-Training" class="headerlink" title="3、Annotator Training"></a>3、Annotator Training</h3><p>手动注释人员的相关培训  </p><h3 id="4、Annotation-Procedure"><a href="#4、Annotation-Procedure" class="headerlink" title="4、Annotation Procedure"></a>4、Annotation Procedure</h3><p>树库的形成分注释、审核、分歧解决和有针对性的调试四个步骤。  </p><h4 id="4-1、Annotation"><a href="#4-1、Annotation" class="headerlink" title="4.1、Annotation"></a>4.1、Annotation</h4><ul><li>annotation from scratch.  </li><li><p>我们使用一个基于CoNLL的文本模板(textual template)，其中每个单词在单独的行（line）中进行注释，每一行包含六列(根据英语UD指南 Version 1)：  </p><ul><li>index(IND)  </li><li>the word itself(WORD)  </li><li>a Universal POS tag(UPOS)  </li><li>a Penn Treebank POS tag(POS)  </li><li>a head word index(HIND)  </li><li>a dependency relation(REL)  </li></ul></li></ul><center><img src="https://i.loli.net/2018/07/04/5b3cb9e2c0fbc.jpg" alt></center>  <p>下图示例是展示给注释者的预先注释的原始句子：  </p><center><img src="https://i.loli.net/2018/07/04/5b3cb974d9d15.jpg" alt></center>   <h4 id="4-2、review"><a href="#4-2、review" class="headerlink" title="4.2、review"></a>4.2、review</h4><p>所有带注释的句子以双盲的方式被随机分配给第二个注释器(也称 reviewer)。reviewer的任务是标记所有注释不同的注释。</p><p>为方便review的工作，编辑了一个常见的注解错误列表（可在已发布的注释手册中找到）。</p><p>引入了一个主动编辑注释的方案来进行review，该方案可以避免reviewer由于passive approval而忽略注释问题。具体来讲：</p><center><img src="https://i.loli.net/2018/07/11/5b45da027d4a3.jpg" alt></center>  <h4 id="4-3、Disagreement-Resolution"><a href="#4-3、Disagreement-Resolution" class="headerlink" title="4.3、Disagreement Resolution"></a>4.3、Disagreement Resolution</h4><p>在注解过程的最后一步里，所有的annotator-reviewer分歧对由第三个注释器解决(annotator，henceforth judge)，其主要任务是在annotator和reviewer之间做出选择。主要是以下两个任务：</p><ul><li>评审人员可以用第三种替代方法解决注释或评论意见的分歧，并为审阅人员所忽略的注释问题引入新的更正。</li><li>另一项任务是为通过审查异议或出现在句子中的歧义结构标记可接受的替代注释（mark acceptable alternative annotations）。</li></ul><h4 id="4-4、Final-Debugging"><a href="#4-4、Final-Debugging" class="headerlink" title="4.4、Final Debugging"></a>4.4、Final Debugging</h4><p>在采用过judge给出的解决方案后，我们用特定的语言学结构的调试测试对语料库进行了查询。附件的测试阶段进一步减少了注释错误的数量和treebank中的不一致性。  </p><h3 id="5、Annotation-Scheme-for-ESL"><a href="#5、Annotation-Scheme-for-ESL" class="headerlink" title="5、Annotation Scheme for ESL"></a>5、Annotation Scheme for ESL</h3><p>注解使用英语的UD POS标记和依存关系的现有目录，并遵循英语的标准UD注释指南。这些指导方针是以英语的语法用法来制定的，不包括由于语法错误而产生的非标准句法结构。<br>指导方针遵循字面阅读（literal reading）的一般原则，强调根据观察到的语言使用进行句法分析。该策略延续了SLA中的一项工作，主张围绕形态语法表面依据对学习者语言进行集中分析。   </p><p>我们的框架包含了对已改正的句子的并行注释，这种策略经常出现在多层注释方案的上下文中，这些注释方案也解释了错误修正的句子形式。   </p><p>在UD中部署一个字面注释策略，这是一种加强注释跨语言一致性的形式主义，将使作者在英语中的非规范结构和作者的母语中的规范结构之间进行有意义的比较。因此，我们的树库的一个重要的新特性是它支持学习者语言的交叉语言研究的能力。  </p><h4 id="5-1-Literal-Annotation"><a href="#5-1-Literal-Annotation" class="headerlink" title="5.1 Literal Annotation"></a>5.1 Literal Annotation</h4><p>关于词性标注，文字标注意味着尽可能地遵循观察到的词形形式。<br>从句法上讲，参数结构是根据单词的用法进行注释的，而不是根据相关上下文中的典型分布进行注释的。<br>下面的惯例列表定义了一些常见的与语法错误相关的非规范结构的字面阅读的概念: 参数结构、时态、词性转换（构词法？）、  </p><ul><li><p><strong>Argument Structure</strong>  </p><ul><li>Extraneous prepositions:我们注释所有由外来的介词作为名词修饰词引入的名词依赖性。在下面的句子中，“his”被标记为一个名词修饰语(nmod)而不是“give”的间接宾语(iobj)。  <center> <img src="https://i.loli.net/2018/07/13/5b4869c8d3c8a.jpg" alt></center>    </li><li>Ommited prepossitions:我们把一个缺少介词的谓语词的名词依赖项作为参数，而不是名词修饰词。例如下面的例子中，将“money”作为一个直接宾语（dobj）而不是视为一个名词修饰语（nmod）,“you”则根据上下文视为一个间接宾语（iobj）而不是一个dobj。</li></ul><center><img src="https://i.loli.net/2018/07/18/5b4f051d7d573.jpg" alt></center>  </li></ul><ul><li><strong>Tense</strong></li></ul><p>根据动词的形态学时态，标注错误的时态用法。例如下图中，“shopping”的时态注释为现在分词(present participle)VBG，而校正后的“shop”则被注解为VBP。</p>  <center><img src="https://i.loli.net/2018/07/18/5b4f1c42837b3.jpg" alt></center>   <ul><li><strong>Word Formation</strong></li></ul><p>字面上注释了在语境上合理并且可以分配PTB标签的错误单词形式。下面例子中“stuffs”就是被视为一个复数名词处理。  </p><center><img src="https://i.loli.net/2018/07/19/5b500d0eba8ed.jpg" alt></center>  <p>类似地，在下面的示例中我们将“necessaryiest”注释为最高级(superlative)。  </p><center><img src="https://i.loli.net/2018/07/19/5b500dfe69501.jpg" alt></center>  <h4 id="5-2-Exceptions-to-Literal-Annotation"><a href="#5-2-Exceptions-to-Literal-Annotation" class="headerlink" title="5.2 Exceptions to Literal Annotation"></a>5.2 Exceptions to Literal Annotation</h4><p>虽然ESL的一般注解策略遵循literal sentence readings，但是有几种类型的构词错误使这种阅读没有信息或不可能，本质上迫使某些词必须用某种程度的解释进行注释。<br>因此，根据从FCE错误纠正中获得的对预定义词的解释，在原句中注释下列情况。   </p><ul><li><strong>Spelling</strong><br>拼写错误是根据单词的正确拼写形式进行注解的。为了支持自动注释工具的错误分析，关于拼写错误的单词形式的最常见用法，在POS标签的元数据字段TYPO中注释恰好形成有效单词的拼写错误的单词。下图的例子中，TYPO字段包含“where”的典型的POS注释，这在句子的上下文中显然是无意的。  </li></ul><center><img src="https://i.loli.net/2018/07/20/5b51350a77503.jpg" alt></center>  <ul><li><strong>Word Formation</strong><br>不能使用现有的PTB标签分配的错误的词结构会被标注为正确的词形式。  <center><img src="https://i.loli.net/2018/07/20/5b51368d57520.jpg" alt></center>  </li></ul><p>有复数后缀的畸形形容词会接收到一个标准的形容词POS标签。当适用时，这种情况还会在使用属性“ua”的错误注释中获得不必要的协议的附加标记。</p><center><img src="https://i.loli.net/2018/07/20/5b51386e77f42.jpg" alt></center>  <center><img src="https://i.loli.net/2018/07/20/5b5138b000c11.jpg" alt></center>  <p>错误的构词法导致有效的，但上下文不可信的构词形式也根据词的更正进行注解。在下面的例子中，名词形式的“sale”很可能是一个畸形动词的意外结果。 与导致有效单词的拼写错误类似，我们在TYPO元数据字段中标记典型的文字POS注释。  </p><center><img src="https://i.loli.net/2018/07/20/5b513b78bd3df.jpg" alt></center> <h4 id="6-Editing-Agreement"><a href="#6-Editing-Agreement" class="headerlink" title="6 Editing Agreement"></a>6 Editing Agreement</h4><p><img src="https://i.loli.net/2018/07/20/5b5171180c80d.jpg" alt></p><h4 id="7-Parsing-Experiments"><a href="#7-Parsing-Experiments" class="headerlink" title="7 Parsing Experiments"></a>7 Parsing Experiments</h4><p>TLE可以学习解析学习者语言并探索语法错误和解析性能之间的关系。因此，在数据集上提出了解析基准，并提供了几种评估语法错误降低了自动POS标记和依存解析质量的范围。  </p><ul><li>第一个实验：  <ul><li>measures tagging and parsing accuracy on the TLE and approximates the gloabal impact of grammatical errors on automatic annotation via performance comparison between the original and error corrected sentence versions.</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;学习记录之句法结构标注（一）&quot;&gt;&lt;a href=&quot;#学习记录之句法结构标注（一）&quot; class=&quot;headerlink&quot; title=&quot;学习记录之句法结构标注（一）&quot;&gt;&lt;/a&gt;学习记录之句法结构标注（一）&lt;/h1&gt;&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;&lt;font col
      
    
    </summary>
    
      <category term="Papers" scheme="https://xiaoxiaoaurora.github.io/categories/Papers/"/>
    
      <category term="Syntactically Annotating" scheme="https://xiaoxiaoaurora.github.io/categories/Papers/Syntactically-Annotating/"/>
    
    
      <category term="Papers" scheme="https://xiaoxiaoaurora.github.io/tags/Papers/"/>
    
      <category term="论文阅读笔记" scheme="https://xiaoxiaoaurora.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="Syntactically Annotating" scheme="https://xiaoxiaoaurora.github.io/tags/Syntactically-Annotating/"/>
    
      <category term="Universal Dependencies" scheme="https://xiaoxiaoaurora.github.io/tags/Universal-Dependencies/"/>
    
  </entry>
  
  <entry>
    <title>中文句法结构</title>
    <link href="https://xiaoxiaoaurora.github.io/2018/07/03/%E4%B8%AD%E6%96%87%E5%8F%A5%E6%B3%95%E7%BB%93%E6%9E%84/"/>
    <id>https://xiaoxiaoaurora.github.io/2018/07/03/中文句法结构/</id>
    <published>2018-07-03T06:11:33.000Z</published>
    <updated>2019-04-04T10:00:39.357Z</updated>
    
    <content type="html"><![CDATA[<h1 id="中文句法结构"><a href="#中文句法结构" class="headerlink" title="中文句法结构  "></a>中文句法结构  </h1><p>参考：<a href="https://blog.csdn.net/sinat_26917383/article/details/55682996" target="_blank" rel="noopener">NLP+句法结构（三）︱中文句法结构（CIPS2016、依存句法、文法）</a>  </p><p>自然语言处理中的自然语言句子级分析技术，可以大致分为词法分析、句法分析、语义分析三个层面。<br>第二个层面的句法分析（syntactic parsing）是对输入的文本句子进行分析以得到句子的句法结构的处理过程。对句法结构进行分析，一方面是语言理解的自身需求，句法分析是语言理解的重要一环，另一方面也为其它自然语言处理任务提供支持。例如句法驱动的统计机器翻译需要对源语言或目标语言（或者同时两种语言）进行句法分析；语义分析通常以句法分析的输出结果作为输入以便获得更多的指示信息。</p><p>根据句法结构的表示形式不同，最常见的句法分析任务可以分为以下三种：<br> (1) 短语结构句法分析（phrase-structure syntactic parsing），该任务也被称作成分句法分析（constituent syntactic parsing），作用是识别出句子中的短语结构以及短语之间的层次句法关系；<br> (2) 依存句法分析（dependency syntactic parsing），作用是识别句子中词汇与词汇之间的相互依存关系；<br> (3) 深层文法句法分析，即利用深层文法，例如词汇化树邻接<br>文法（Lexicalized Tree Adjoining Grammar， LTAG）、词汇功能文法（Lexical Functional Grammar， LFG）、组合范畴文法（Combinatory Categorial Grammar， CCG）等，对句子进行深层的句法以及语义分析。  </p><p>上述几种句法分析任务比较而言，<strong>依存句法分析属于浅层句法分析</strong>。其实现过程相对简单，比较适合在多语言环境下的应用，但是依存句法分析所能提供的信息也相对较少。深层文法句法分析可以提供丰富的句法和语义信息，但是采用的文法相对复杂，分析器的运行复杂度也较高，这使得深层句法分析当前不适合处理大规模数据。<strong>短语结构句法分析介于依存句法分析和深层文法句法分析之间。</strong></p><p><strong>词法分析是将输入句子从字序列转化为词和词性序列，句法分析将输入句子从词序列形式转化为树状结构，从而刻画句子的词法和句法结构。</strong>  </p><p><img src="https://i.loli.net/2018/07/03/5b3b64866a7b3.jpg" alt></p><blockquote><p>摘录自：CIPS2016 中文信息处理报告《第一章 词法和句法分析研究进展、现状及趋势》P8 -P11<br>CIPS2016&gt; 中文信息处理报告下载链接：<a href="http://cips-upload.bj.bcebos.com/cips2016.pdf" target="_blank" rel="noopener">http://cips-upload.bj.bcebos.com/cips2016.pdf</a>    </p></blockquote><p><img src="https://i.loli.net/2018/07/03/5b3b64c19411a.jpg" alt>  </p><p><img src="https://i.loli.net/2018/07/03/5b3b64dd25914.jpg" alt>   </p><p><img src="https://i.loli.net/2018/07/03/5b3b6c2ee21b5.jpg" alt>  </p><p>不同类型的句法分析体现在句法结构的表示形式不同，实现过程的复杂程度也有所不同。因此，科研人员采用不同的方法构建符合各个语法特点的句法分析系统。下文主要对句法分析技术方法和研究现状进行总结分析。  </p><h2 id="一、依存句法分析"><a href="#一、依存句法分析" class="headerlink" title="一、依存句法分析"></a>一、依存句法分析</h2><p>依存句法存在一个共同的基本假设：句法结构本质上包含词和词之间的依存（修饰）关系。<strong>一个依存关系连接两个词，分别是核心词（head）和依存词（dependent）。依存关系可以细分为不同的类型，表示两个词之间的具体句法关系。</strong>   </p><p>依存句法分析的形式化目标是针对给定输入句子<img src="https://i.loli.net/2018/07/03/5b3b6f0e7664a.jpg" alt>，寻找分值（或概率）最大的依存树<img src="https://i.loli.net/2018/07/03/5b3b6f3356c16.jpg" alt>  </p><p><img src="https://i.loli.net/2018/07/03/5b3b6f5e9639b.jpg" alt>  </p><p>其中， Y(x)表示输入句子x对应的合法依存树集合，即搜索空间； θ 为模型参数， 即特征权重向量。</p><p>目前研究主要集中在数据驱动的依存句法分析方法，即在训练实例集合上学习得到依存句法分析器，而不涉及依存语法理论的研究。数据驱动的方法的主要优势在于给定较大规模的训练数据，不需要过多的人工干预，就可以得到比较好的模型。因此，这类方法很容易应用到新领域和新语言环境。数据驱动的依存句法分析方法主要有两种主流方法：基于图（graph-based）的分析方法和基于转移（transition-based）的分析方法。    </p><h3 id="1、基于图的依存句法分析方法"><a href="#1、基于图的依存句法分析方法" class="headerlink" title="1、基于图的依存句法分析方法"></a>1、基于图的依存句法分析方法</h3><p><strong>基于图的方法将依存句法分析问题看成从完全有向图中寻找最大生成树的问题。</strong>一棵依存树的分值由构成依存树的几种子树的分值累加得到。根据依存树分值中包含的子树的复杂度，基于图的依存分析模型可以简单区分为一阶和高阶模型。高阶模型可以使用更加复杂的子树特征，因此分析准确率更高，但是解码算法的效率也会下降。基于图的方法通常采用基于动态规划的解码算法，也有一些学者采用柱搜索 (beamsearch)来提高效率。学习特征权重时，通常采用在线训练算法，如平均感知（averaged<br>perceptron）。  </p><h3 id="2、基于转移的依存句法分析方法"><a href="#2、基于转移的依存句法分析方法" class="headerlink" title="2、基于转移的依存句法分析方法"></a>2、基于转移的依存句法分析方法</h3><p><strong>基于转移的方法将依存树的构成过程建模为一个动作序列，将依存分析问题转化为寻找最优动作序列的问题</strong>。 早期，研究者们使用局部分类器（如支持向量机等）决定下一个动作。近年来，研究者们采用全局线性模型来决定下一个动作，一个依存树的分值由其对应的动作序列中每一个动作的分值累加得到。特征表示方面，基于转移的方法可以充分利用已形成的子树信息，从而形成丰富的特征，以指导模型决策下一个动作。模型通过贪心搜索或者柱搜索等解码算法找到近似最优的依存树。和基于图的方法类似，基于转移的方法通常也采用在线训练算法学习特征权重。</p><h3 id="3、多模型融合的依存句法分析方法"><a href="#3、多模型融合的依存句法分析方法" class="headerlink" title="3、多模型融合的依存句法分析方法"></a>3、多模型融合的依存句法分析方法</h3><p><strong>基于图和基于转移的方法从不同的角度解决问题，各有优势。</strong>基于图的模型进行全局搜索但只能利用有限的子树特征，而基于转移的模型搜索空间有限但可以充分利用已构成的子树信息构成丰富的特征。详细比较发现，这两种方法存在不同的错误分布。因此，<strong>研究者们使用不同的方法融合两种模型的优势，常见的方法有：stacked learning；对多个模型的结果加权后重新解码(re-parsing)；从训练语料中多次抽样训练多个模型(bagging)。</strong></p><h2 id="二、短语结构句法分析"><a href="#二、短语结构句法分析" class="headerlink" title="二、短语结构句法分析"></a>二、短语结构句法分析</h2><p>短语结构句法分析的研究基于上下文无关文法（Context Free Grammar,CEG）。上下文无关文法可以定义为四元组&lt;T,N,S,R&gt;，其中T表示终结符的集合（即词的集合），N表示非终结符的集合（即文法标注和词性标记的集合），S表示充当句法树根节点的特殊非终结符，而 R 表示文法规则的集合，其中每条文法规则可以表示为 <img src="https://i.loli.net/2018/07/03/5b3b743d0f43b.jpg" alt> ，这里的<img src="https://i.loli.net/2018/07/03/5b3b745418202.jpg" alt> 表示由非终结符与终结符组成的一个序列（允许为空）。  </p><p><img src="https://i.loli.net/2018/07/04/5b3c24d048328.jpg" alt>   </p><p><img src="https://i.loli.net/2018/07/04/5b3c24f0cd318.jpg" alt>  </p><p>当前主流的句法分析模型，无论底层的机器学习方法（生成模型或者判别模型）或是所采用的系统框架（单系统、多系统融合或者两阶段的重排序方法），本质上都可以归到基于词汇化方法或者基于符号重标记方法的句法分析器。</p><h2 id="三、深层文法句法分析"><a href="#三、深层文法句法分析" class="headerlink" title="三、深层文法句法分析"></a>三、深层文法句法分析</h2><p>相对前两种句法分析，深层文法句法分析的研究相对较少。本节简要介绍词汇化树邻接文法（Lexicalized Tree Adjoining Grammar， LTAG）、词汇功能文法（Lexical Functional<br>Grammar， LFG）和组合范畴文法（Combinatory Categorial Grammar， CCG）。</p><p>1、<strong>词汇化树邻接文法，简称LTAG</strong>，是对树邻接文法（TAG）进行词汇化扩展得到的。  </p><ul><li>树邻接文法包含两种基本树（Elementary Tree）：初始树（Initial Tree）和辅助树（Auxiliary Tree）。  </li><li>在树邻接文法中，有两种子树操作：替换（ Substitution）和插接（ Adjunction）。   </li><li>词汇化语法是给所有基本树都和具体词关联起来，使得树更加具有个性化。  </li></ul><p>2、<strong>词汇功能文法，简称 LFG，是一种短语结构文法。</strong>  </p><ul><li>LFG 把语言看成是由多维结构组成的，每一维都用特殊规则、概念和格式表示成一个特殊结构。  </li><li>LFG 包含两种最基本的结构：  <ul><li>1） F-结构，用于表示语法功能；  </li><li>2） C-结构，用于表示句法功能。  </li><li>除此之外还有一些其他结构，用于表示浅层信息，例如谓词论元关系等。</li></ul></li></ul><p>3、<strong>组合范畴文法，简称CCG，一种类型驱动的词汇化文法，通过词汇范畴显式地提供从句法到语义的接口，属于短语结构文法。</strong>  </p><ul><li>CCG 的基本操作包括：  <ul><li>1）原子范畴（Atomic Category），用于表达基本的词汇类别和句法功能；  </li><li>2）组合范畴（Function Category），由原子范畴构<br>成，通常用 X/Y 或 X\Y 来表示可以向左或者向右寻找变元 Y 来获得组合 X。</li></ul></li></ul><p>基于深层文法的句法分析器也取得一些进展。例如，Boullier 和 Sagot 构建基于LFG的分析器-SxLFG。 WenduanXu 等人借鉴依存分析模型，采用 Shift-reduce 框架构建高效的<br>CCG 分析器取得很好的效果。在树库语料方面，大多数深层文法树库是通过从 PTB 自动转换得到的，而黄昌宁老师在清华中文树库基础上结合中文特点，探索如何构建中文 CCG 树库。</p><h2 id="四、基于深度学习的句法分析"><a href="#四、基于深度学习的句法分析" class="headerlink" title="四、基于深度学习的句法分析"></a>四、基于深度学习的句法分析</h2><p>深度学习（Deep Learning）在句法分析课题上主要研究工作集中在特征表示方面。<strong>深度学习把原子特征进行向量化，在利用多层神经元网络提取特征</strong>。所谓向量化就是把词、词性等用低维、连续实数空间上的向量来表示，从而便于寻找特征组合与表示，同时容易进行计算。   </p><p><img src="https://i.loli.net/2018/07/03/5b3b7301e40e9.jpg" alt>  </p><p>在图 1 中，把词、词性、类别标签等原子特征表示为向量，然后利用多层网络进行特征提取。深度学习在特征表示方面有如下优点：  </p><ul><li>1）<strong>只需要原子特征</strong>。这些原子特征以前是通过人工的自由组合形成最终的一元特征、二元特征、三元特征、四元特征甚至更多元的组合。这种人工组合最后取得较好的效果，但是事实上我们不知道怎么组合能形成最佳的特征集合。<strong>深度学习将所有的原子特征向量化之后，直接采用向量乘法以及非线性等各种运算从理论上能实现任意元的特征组合</strong>。  </li></ul><ul><li>2）<strong>能使用更多的原子特征</strong>。比如基于图的模型中，在建立弧时，不仅仅使用左边第一个词、右边第一个词等原子特征，还可以使用左边整个词序列、右边整个词序列的特征。研究人员把这种基于深度学习的特征表示方法分别应用在基于图的句法分析模型和基于转移的句法分析模型上，实验结果表明深度学习方法开始在句法中发挥作用。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;中文句法结构&quot;&gt;&lt;a href=&quot;#中文句法结构&quot; class=&quot;headerlink&quot; title=&quot;中文句法结构  &quot;&gt;&lt;/a&gt;中文句法结构  &lt;/h1&gt;&lt;p&gt;参考：&lt;a href=&quot;https://blog.csdn.net/sinat_26917383/a
      
    
    </summary>
    
      <category term="NLP" scheme="https://xiaoxiaoaurora.github.io/categories/NLP/"/>
    
      <category term="句法分析" scheme="https://xiaoxiaoaurora.github.io/categories/NLP/%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/"/>
    
    
      <category term="NLP" scheme="https://xiaoxiaoaurora.github.io/tags/NLP/"/>
    
      <category term="Syntactically Annotating" scheme="https://xiaoxiaoaurora.github.io/tags/Syntactically-Annotating/"/>
    
      <category term="句法分析" scheme="https://xiaoxiaoaurora.github.io/tags/%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90/"/>
    
      <category term="中文句法结构" scheme="https://xiaoxiaoaurora.github.io/tags/%E4%B8%AD%E6%96%87%E5%8F%A5%E6%B3%95%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>Python | 处理json文件</title>
    <link href="https://xiaoxiaoaurora.github.io/2018/06/21/python%E4%B9%8Bjson%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96/"/>
    <id>https://xiaoxiaoaurora.github.io/2018/06/21/python之json文件读取/</id>
    <published>2018-06-21T11:08:21.000Z</published>
    <updated>2019-04-04T09:32:20.871Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python-处理json文件"><a href="#Python-处理json文件" class="headerlink" title="Python | 处理json文件"></a>Python | 处理json文件</h1><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">json文件内容：  </span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"stations"</span>:[</span><br><span class="line">            &#123;</span><br><span class="line">            <span class="attr">"sta_name"</span>:<span class="string">"北京北"</span>,</span><br><span class="line">            <span class="attr">"sta_ename"</span>:<span class="string">"beijingbei"</span>,</span><br><span class="line">            <span class="attr">"sta_code"</span>:<span class="string">"VAP"</span>,</span><br><span class="line">            <span class="attr">"text"</span>:<span class="string">"自三峡七百里中，两岸连山，略无阙处。"</span>,</span><br><span class="line">            &#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="1-需要加载一个Json文件，并将Json中的某些项进行修改，然后写回到一个新的Json文件中去。"><a href="#1-需要加载一个Json文件，并将Json中的某些项进行修改，然后写回到一个新的Json文件中去。" class="headerlink" title="1. 需要加载一个Json文件，并将Json中的某些项进行修改，然后写回到一个新的Json文件中去。"></a>1. 需要加载一个Json文件，并将Json中的某些项进行修改，然后写回到一个新的Json文件中去。</h2><p>Model代码：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3.6</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Python-处理json文件&quot;&gt;&lt;a href=&quot;#Python-处理json文件&quot; class=&quot;headerlink&quot; title=&quot;Python | 处理json文件&quot;&gt;&lt;/a&gt;Python | 处理json文件&lt;/h1&gt;&lt;figure class=&quot;hi
      
    
    </summary>
    
      <category term="Python" scheme="https://xiaoxiaoaurora.github.io/categories/Python/"/>
    
    
      <category term="Python" scheme="https://xiaoxiaoaurora.github.io/tags/Python/"/>
    
      <category term="json" scheme="https://xiaoxiaoaurora.github.io/tags/json/"/>
    
  </entry>
  
  <entry>
    <title>Attention机制的理解</title>
    <link href="https://xiaoxiaoaurora.github.io/2018/06/12/Attention%E6%9C%BA%E5%88%B6%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://xiaoxiaoaurora.github.io/2018/06/12/Attention机制的理解/</id>
    <published>2018-06-12T11:34:55.000Z</published>
    <updated>2019-04-04T09:38:43.381Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Attention机制的理解"><a href="#Attention机制的理解" class="headerlink" title="Attention机制的理解"></a>Attention机制的理解</h1><p><strong>Attention Model</strong>类似于人脑的注意力模型，说到底是一种资源分配模型，在某个特定时刻，你的注意力总是集中在画面中的某个焦点部分，而对其它部分视而不见。  </p><h2 id="Encoder-Decoder框架"><a href="#Encoder-Decoder框架" class="headerlink" title="|Encoder-Decoder框架"></a>|Encoder-Decoder框架</h2><p>文本处理领域的AM模型，因为目前绝大多数文献中出现的AM模型是附着在Encoder-Decoder框架下的，当然，其实AM模型可以看作一种通用的思想，本身并不依赖于Encoder-Decoder模型，这点需要注意。  </p><p>Encoder-Decoder框架可以看作是一种文本处理领域的研究模式，应用场景异常广泛，下图是文本处理领域里常用的Encoder-Decoder框架最抽象的一种表示：<br><img src="https://i.loli.net/2018/06/10/5b1c982303cc0.jpg" alt>  </p><p>Encoder-Decoder框架可以这么直观地去理解：<strong>可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。</strong>对于句子对&lt;X,Y&gt;，我们的目标是给定输入句子X，期待通过Encoder-Decoder框架来生成目标句子Y。X和Y可以是同一种语言，也可以是两种不同的语言。而X和Y分别由各自的单词序列构成：<br><img src="https://img-blog.csdn.net/20160120181636077" alt>  </p><p>Encoder顾名思义就是对输入句子X进行编码，将输入句子通过非线性变换转化为中间语义表示C：<br><img src="https://img-blog.csdn.net/20160120181707734" alt><br>对于解码器Decoder来说，其任务是根据句子X的中间语义表示C和之前已经生成的历史信息y1,y2….yi-1来生成i时刻要生成的单词yi ：<br><img src="https://img-blog.csdn.net/20160120181744247" alt>  </p><p>Encoder-Decoder框架是个通用的计算机框架，可以有各种不同的模型结合，具体用什么模型由研究者自己决定，常见的比如CNN/RNN/BiRNN/GRU/LSTM/Deep LSTM 。  </p><h2 id="Attention-Model"><a href="#Attention-Model" class="headerlink" title="Attention Model"></a>Attention Model</h2><p>图1中展示的Encoder-Decoder模型是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Y中每个单词的生成过程如下：  </p><p><img src="https://img-blog.csdn.net/20160120181816122" alt>  </p><p>其中：  </p><ul><li>f是decoder的非线性变换函数。由此可看出，在生成目标句子的单词时，不论生成哪个单词（y1,y2还是y3），他们使用的句子X的语义编码C都是一样的，没有任何区别。而<strong>语义编码C是由句子X的每个单词经过Encoder 编码产生的</strong>，这意味着不论是生成哪个单词，y1,y2还是y3，<strong>其实句子X中任意单词对生成某个目标单词yi来说影响力都是相同的</strong>，没有任何区别（其实如果Encoder是RNN的话，理论上越是后输入的单词影响越大，并非等权的，估计这也是为何Google提出Sequence to Sequence模型时发现把输入句子逆序输入做翻译效果会更好的小Trick的原因）。这就是为何说这个模型没有体现出注意力的缘由。这类似于你看到眼前的画面，但是没有注意焦点一样。如果拿机器翻译来解释这个分心模型的Encoder-Decoder框架更好理解，比如输入的是英文句子：Tom chase Jerry，Encoder-Decoder框架逐步生成中文单词：“汤姆”，“追逐”，“杰瑞”。在翻译“杰瑞”这个中文单词的时候，分心模型里面的每个英文单词对于翻译目标单词“杰瑞”贡献是相同的，很明显这里不太合理，显然“Jerry”对于翻译成“杰瑞”更重要，但是分心模型是无法体现这一点的，这就是为何说它没有引入注意力的原因。<strong>没有引入注意力的模型在输入句子比较短的时候估计问题不大，但是如果输入句子比较长，此时所有语义完全通过一个中间语义向量来表示，单词自身的信息已经消失，可想而知会丢失很多细节信息，这也是为何要引入注意力模型的重要原因。</strong></li></ul><p>上面的例子中，如果引入AM模型的话，应该在翻译“杰瑞”的时候，体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：  </p><blockquote><p>（Tom,0.3）(Chase,0.2)(Jerry,0.5)  </p></blockquote><p><strong>每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小</strong>。这对于正确翻译目标语单词肯定是有帮助的，因为引入了新的信息。同理，目标句子中的每个单词都应该学会其对应的源语句子中单词的注意力分配概率信息。这意味着在生成每个单词Yi的时候，原先都是相同的中间语义表示C会替换成根据当前生成单词而不断变化的Ci。理解AM模型的关键就是这里，即由固定的中间语义表示C换成了根据当前输出单词来调整成加入注意力模型的变化的Ci。增加了AM模型的Encoder-Decoder框架理解起来如图2所示。<br><img src="https://i.loli.net/2018/06/10/5b1d1d0eb5ffa.jpg" alt><br>即生成目标句子单词的过程成了下面的形式：<br><img src="https://img-blog.csdn.net/20160120181916688" alt></p><p><img src="https://i.loli.net/2018/06/10/5b1d28efcdf63.jpg" alt>  </p><p><img src="https://i.loli.net/2018/06/10/5b1d290d4fc8b.jpg" alt></p><p><img src="https://i.loli.net/2018/06/10/5b1d292a36763.jpg" alt>  </p><p><img src="https://i.loli.net/2018/06/10/5b1d29743380a.jpg" alt>  </p><p><img src="https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1529315464&amp;di=789cd7ead94e9fcf267cb0a676f424de&amp;imgtype=jpg&amp;er=1&amp;src=http%3A%2F%2Fimage.bubuko.com%2Finfo%2F201805%2F20180526131547138766.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Attention机制的理解&quot;&gt;&lt;a href=&quot;#Attention机制的理解&quot; class=&quot;headerlink&quot; title=&quot;Attention机制的理解&quot;&gt;&lt;/a&gt;Attention机制的理解&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Attention Mode
      
    
    </summary>
    
      <category term="DeepLearning" scheme="https://xiaoxiaoaurora.github.io/categories/DeepLearning/"/>
    
      <category term="Attention" scheme="https://xiaoxiaoaurora.github.io/categories/DeepLearning/Attention/"/>
    
    
      <category term="DeepLearning" scheme="https://xiaoxiaoaurora.github.io/tags/DeepLearning/"/>
    
      <category term="Attention" scheme="https://xiaoxiaoaurora.github.io/tags/Attention/"/>
    
      <category term="Encoder-Decoder" scheme="https://xiaoxiaoaurora.github.io/tags/Encoder-Decoder/"/>
    
      <category term="Neural Network" scheme="https://xiaoxiaoaurora.github.io/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>Fast and Robust Neural Network Joint Models for Statistical Machine.md</title>
    <link href="https://xiaoxiaoaurora.github.io/2018/06/09/Fast-and-Robust-Neural-Network-Joint-Models-for-Statistical-Machine-md/"/>
    <id>https://xiaoxiaoaurora.github.io/2018/06/09/Fast-and-Robust-Neural-Network-Joint-Models-for-Statistical-Machine-md/</id>
    <published>2018-06-09T01:40:24.000Z</published>
    <updated>2019-04-04T10:02:29.076Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Fast-and-Robust-Neural-Network-Joint-Models-for-Statistical-Machine-Translation"><a href="#Fast-and-Robust-Neural-Network-Joint-Models-for-Statistical-Machine-Translation" class="headerlink" title="Fast and Robust Neural Network Joint Models for Statistical Machine Translation"></a>Fast and Robust Neural Network Joint Models for Statistical Machine Translation</h2><p><strong>NNJM:</strong>通过一个源上下文窗口扩展NNLM（which augments the NNJM with a source context window）。该模型是纯词汇化(purely lexicalized)的，可以集成到任何MT的Decoder中。具体来说，该模型利用m-word源窗口扩展一个n-gram目标语言模型。和以往的联合模型不同，该模型能够很容易作为一个feature被整合到任何SMT解码器中。</p><p><img src="https://i.loli.net/2018/06/11/5b1dde6c03fe7.jpg" alt>    </p><p><strong>NNJM近似地估计了以源句子S为条件的目标假设T的概率。遵循目标的标准n-gram LM分解，其中每个目标字ti都受前面的n- 1个目标字的制约。为了使这个模型成为一个联合模型，对源上下文向量 <img src="https://i.loli.net/2018/06/11/5b1de290abead.jpg" alt>进行了条件分析：</strong>  </p><p><img src="https://i.loli.net/2018/06/11/5b1de1e00531f.jpg" alt>  </p><p>每一个目标词ti都直接对应着一个在位置ai的源词，<img src="https://i.loli.net/2018/06/11/5b1de290abead.jpg" alt>是在以ai为中心的m-word的源窗口。<br><img src="https://i.loli.net/2018/06/11/5b1de8f27cf9e.jpg" alt>  </p><p>这种从属（affiliation）概念源自单词对齐，但与单词对齐不同，每个目标单词必须与一个非空(non-NULL)源单词相关联。  </p><p><img src="https://i.loli.net/2018/06/11/5b1de8f27cf9e.jpg" alt>  </p><p><img src="https://i.loli.net/2018/06/11/5b1dec1261a97.jpg" alt>  </p><p>中文-英语平行句子的NNJM上下文模型例子如下图：  </p><p><img src="https://i.loli.net/2018/06/11/5b1deb3bb7995.jpg" alt></p><p>论文中采用的是n=4、m=11的15-gram LM 模型（神经网络语言模型能够优雅地扩展并利用任意大的上下文大小）。  论文中的神经网络结构与Bengio et.al等人的前溃神经网络语言模型结构基本相似，如下图。<br><img src="https://i.loli.net/2018/06/11/5b1dfed06cf34.jpg" alt></p><h4 id="1-NNJM中的神经网络结构"><a href="#1-NNJM中的神经网络结构" class="headerlink" title="1. NNJM中的神经网络结构"></a>1. NNJM中的神经网络结构</h4><p>NNJM中的神经网络架构与Bengio et al.(2003)所描述的原始前馈NNLM体系结构（feed-forward NNJM architecture）几乎相同。<br>隐藏层大小、词汇表大小和源窗口大小选择了这些值:  </p><p><img src="https://i.loli.net/2018/06/11/5b1e2e496fa49.jpg" alt>    </p><h4 id="2-神经网络训练"><a href="#2-神经网络训练" class="headerlink" title="2. 神经网络训练"></a>2. 神经网络训练</h4><p>除了使用平行语料库代替单语语料库外，训练过程与NNLM相同。在形式上，我们寻求使训练数据的逻辑可能性最大化：    </p><p><img src="https://i.loli.net/2018/06/11/5b1e2ed00920a.jpg" alt>  </p><pre><code>* 优化（Optimization）: 带SGD的标准后向传播。  * 权重（Weights）：[-0.05,0.05]之间进行随机初始化  * 学习率： 10^-3* minibatch size: 128* 20,000 minibatches/each epoch, 计算验证集的可能性。* 40 epochs* 我们在没有L2正则化或动量的情况下执行基本的权值更新。* Training is performed on a single Tesla K10 GPU, with each epoch (128*20k = 2.6M samples)</code></pre><h4 id="3-Self-Normalized-Neural-Network"><a href="#3-Self-Normalized-Neural-Network" class="headerlink" title="3. Self-Normalized Neural Network"></a>3. Self-Normalized Neural Network</h4><p>NNLM的计算成本在解码中是一个重要的问题，并且这个成本由整个目标词汇表上的输出softmax所支配。<br>我们的目标是能够使用相当大的且没有词类（word-classes）的词汇表，并且简单地避免在解码时计算整个输出层。为此，我们提出了自规一化（self-normalization）的新技术，其中输出层分数是接近于没有显示执行softmax的概率。<br>我们所定义的标准softmax对数似然函数如下：  </p><p><img src="https://i.loli.net/2018/06/11/5b1e3547018e5.jpg" alt>  </p><p>由上看出，在解码阶段当log(Z(x))等于0时（Z(x)=1）我们就只需要计算输出层的r行而不是计算整个矩阵，但是很难保证用这个来训练神经网络，所以可以通过增加训练目标函数来明确鼓励log-softmax正态化器（explicitly encourage the log-softmax normalizer）尽可能接近0：  </p><p><img src="https://i.loli.net/2018/06/11/5b1e382e98fe0.jpg" alt>  </p><p>在这种情况下，输出层的偏置权值初始化为log(1/|V|)，因此初始网络是自归一化的。在解码时，使用<img src="https://i.loli.net/2018/06/11/5b1e38db8ce39.jpg" alt>作为特征得分，而不是选用log(P(x))。在本篇论文中NNNJM结构中，在解码过程中，self-normalization将查找速度提高了15倍。  </p><p><img src="https://i.loli.net/2018/06/11/5b1e39f09cfac.jpg" alt></p><p>在用噪声对比估计（ Noise Contrastive Estimation ，NCE）训练自归一化的NNLMs时虽然加速了训练时间，但是没有机制能控制自归一化程度。</p><h4 id="4-Pre-Computing-the-Hidden-Layer"><a href="#4-Pre-Computing-the-Hidden-Layer" class="headerlink" title="4. Pre-Computing the Hidden Layer"></a>4. Pre-Computing the Hidden Layer</h4><p>自归一化显著提高了NNJM查找的速度，该模型仍比 back-off LM慢几个数量级。在这里，我们展示了预计算(Pre-Computing)第一个隐藏层的技巧，它进一步将NNJM查找速度提高了1000倍。<br>请注意，这种技术只会导致自归一化，前向反馈，有一个隐藏层的NNLM-style网络的显著加速。</p><h4 id="5-Decoding-with-the-NNJM"><a href="#5-Decoding-with-the-NNJM" class="headerlink" title="5. Decoding with the NNJM"></a>5. Decoding with the NNJM</h4><p><strong>论文所提出的NNJM本质上是一个带有附加源上下文的n-gram NNLM</strong>，所以可以很容易地集成到任何SMT解码器中。  </p><blockquote><p>NNJM is fundamentally an n-gram NNLM with additional source context, it can easily be integrated into any SMT decoder。  </p></blockquote><p>主要介绍将NNJM集成到分层解码器时必须考虑的事项。  </p><ul><li>Hierarchical Parsing（分层句法分析）  </li><li>Affiliation Heuristic（加入启发式）</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Fast-and-Robust-Neural-Network-Joint-Models-for-Statistical-Machine-Translation&quot;&gt;&lt;a href=&quot;#Fast-and-Robust-Neural-Network-Joint-Mode
      
    
    </summary>
    
      <category term="Papers" scheme="https://xiaoxiaoaurora.github.io/categories/Papers/"/>
    
      <category term="SMT" scheme="https://xiaoxiaoaurora.github.io/categories/Papers/SMT/"/>
    
    
      <category term="Papers" scheme="https://xiaoxiaoaurora.github.io/tags/Papers/"/>
    
      <category term="论文阅读笔记" scheme="https://xiaoxiaoaurora.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="GEC" scheme="https://xiaoxiaoaurora.github.io/tags/GEC/"/>
    
      <category term="SMT" scheme="https://xiaoxiaoaurora.github.io/tags/SMT/"/>
    
      <category term="NNJM" scheme="https://xiaoxiaoaurora.github.io/tags/NNJM/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning with PyTorch(60 Minute)</title>
    <link href="https://xiaoxiaoaurora.github.io/2018/06/09/Pytorch-Deep-Learning-with-PyTorch-A-60-Minute-Blitz/"/>
    <id>https://xiaoxiaoaurora.github.io/2018/06/09/Pytorch-Deep-Learning-with-PyTorch-A-60-Minute-Blitz/</id>
    <published>2018-06-09T01:04:57.000Z</published>
    <updated>2019-04-04T10:06:32.596Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Deep-Learning-with-PyTorch-A-60-Minute-Blitz"><a href="#Deep-Learning-with-PyTorch-A-60-Minute-Blitz" class="headerlink" title="Deep Learning with PyTorch:A 60 Minute Blitz"></a>Deep Learning with PyTorch:A 60 Minute Blitz</h1><hr><h2 id="一、PyTorch-是什么"><a href="#一、PyTorch-是什么" class="headerlink" title="一、PyTorch 是什么"></a>一、PyTorch 是什么</h2><p>它是一个基于Python的科学计算包，目标用户有两类：</p><ul><li>为了使用GPU来替代numpy。</li><li>一个深度学习援救平台：提供最大的灵活性和速度。</li></ul><h2 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h2><h3 id="张量（Tensors"><a href="#张量（Tensors" class="headerlink" title="张量（Tensors)"></a>张量（Tensors)</h3><p>张量类似于Numpy的ndarrays，不同之处在于张量可以使用GPU来加快计算。  </p><pre><code>from __future__ import print_functionimport torch  </code></pre><p>构建一个未初始化的5*3的矩阵：  </p><pre><code>x = torch.empty(5, 3)print(x)  </code></pre><p>构建一个随机初始化的矩阵：  </p><pre><code>x = torch.rand(5, 3)print(x)  </code></pre><p>构建一个以零填充且数据类型为long的矩阵：  </p><pre><code>x = torch.zeros(5, 3, dtype=torch.long)  print(x)  </code></pre><p>直接从数据构造张量：</p><p>x = torch.tensor([5.5, 3])<br>print(x)  </p><p>也可以根据现有张量创建张量。这些方法将重用输入张量的属性，例如dtype，除非用户提供了新的值：  </p><pre><code>x = x.new_ones(5, 3, dtype=torch.double)   #new_* methods 获取大小 print(x)  x = torch.randn_like(x, dtype=torch.float)   # 重写dtypeprint(x)             # 结果具有相同的大小</code></pre><p>获取张量大小：  </p><pre><code>print(x.size())</code></pre><p>注意：torch.Size实际上是一个元组，所以它支持所有的元组操作。  </p><h3 id="操作（Operations）"><a href="#操作（Operations）" class="headerlink" title="操作（Operations）"></a>操作（Operations）</h3><p>张量上的操作有多重语法形式，下面我们一加法为例进行讲解。  </p><p><strong>加法：语法1</strong>  </p><pre><code>print(&quot;x: &quot;, x)y = torch.rand(5, 3)print(x + y)  </code></pre><p><strong>加法：语法2</strong>  </p><pre><code>print(&quot;x: &quot;, x)print(torch.add(x, y))   </code></pre><p><strong>加法：提供输出张量作为参数</strong>   </p><pre><code>result = torch.empty(5, 3)torch.add(x, y, out=result)print(result)</code></pre><p><strong>加法: in-place</strong>  </p><pre><code>#adds x to yy.add_(x)print(y)  </code></pre><p>注意：任何使张量在原位发生变异的操作都是用<em>， 例如: x.copy</em>(y), x.t_(),都将会改变x。</p><p>可以任意使用标准Numpy-like索引：</p><pre><code>print(x)print(x[:, 1])  print(x[1, :])print(x[2, 4])  </code></pre><p><strong>调整大小(Resizing)：如果您想调整大小/重塑张量，可以使用torch.view</strong>  </p><pre><code>x = torch.randn(4, 4)y = x.view(16)z = x.view(-1, 8)print(x.size(), y.size(), z.size())  </code></pre><p>如果有一个单元张量(a one element tensor)，请使用.item（）将该值作为Python数字来获取  </p><pre><code>x = torch.rand(1)print(x)print(x.item())  </code></pre><p><a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener">这里</a>描述了100+张量操作，包括转置，索引，切片，数学运算，线性代数，随机数等。  </p><h2 id="NumPy-Bridge"><a href="#NumPy-Bridge" class="headerlink" title="NumPy Bridge"></a>NumPy Bridge</h2><p>把一个torch张量转换为numpy数组或者反过来都是很简单的。</p><p>Torch张量和numpy数组将共享潜在的内存，改变其中一个也将改变另一个。  </p><p>将Torch张量转换成一个NumPy数组 ：</p><pre><code>&gt;&gt;&gt; a = torch.ones(5)&gt;&gt;&gt; print(a)Out: tensor([ 1.,  1.,  1.,  1.,  1.])&gt;&gt;&gt; b = a.numpy()&gt;&gt;&gt; print(b)  Out: [1. 1. 1. 1. 1.]  </code></pre><p>numpy数组的值如何在改变？</p><pre><code>a.add_(1)print(a)print(b)</code></pre><p>把NumPy数组转换成Torch张量：<br>看看改变numpy数组如何自动改变torch张量。</p><pre><code>import numpy as npa = np.ones(5)b = torch.from_numpy(a)np.add(a, 1, out=a)print(a)print(b)  </code></pre><p>所有在CPU上的张量，除了字符张量，都支持在numpy之间转换。</p><h2 id="CUDA-张量"><a href="#CUDA-张量" class="headerlink" title="CUDA 张量"></a>CUDA 张量</h2><p>使用  <strong>.to</strong>  函数可以将张量移动到GPU上。  </p><pre><code># let us run this cell only if CUDA is available# We will use ``torch.device`` objects to move tensors in and out of GPUif torch.cuda.is_available():    device = torch.device(&quot;cuda&quot;)  # a CUDA device object    y = torch.ones_like(x, device=device)    x = x.to(device)    z = x + y    print(z)    print(z.to(&quot;cpu&quot;, torch.double))  </code></pre><p>Out:  </p><blockquote><p>tensor([ 0.5921], device=’cuda:0’)<br>tensor([ 0.5921], dtype=torch.float64)  </p></blockquote><h1 id="Autograd-自动求导（automatic-differentiation）"><a href="#Autograd-自动求导（automatic-differentiation）" class="headerlink" title="Autograd: 自动求导（automatic differentiation）"></a>Autograd: 自动求导（automatic differentiation）</h1><p>PyTorch中所有神经网络的核心是autograd包。我们首先简单介绍一下这个包,然后训练我们的第一个神经网络。  </p><p>autograd包为张量上的所有操作提供了自动求导.它是一个运行时定义的框架,这意味着反向传播是根据你的代码如何运行来定义,并且每次迭代可以不同.</p><p>接下来我们用一些简单的示例来看这个包。  </p><h2 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h2><p><strong>torch.Tensor是包的核心类，如果将其属性requires_grad设置为true,它开始跟踪它上面的所有操作。</strong> 当完成计算时可以调用.backward()并自动计算所有的梯度。<strong>该张量的梯度被计算放入搭配到.grad属性中</strong>。<br>阻止跟踪历史的张量，可以通过调用.detch()将其从计算历史记录中分离出来，并防止跟踪将来的计算。<br>为了防止跟踪历史记录（和使用内存），您还可以在 with torch.no_grad（）包装代码块。这在评估模型时特别有用，因为该模型可能具有requires_grad = True的可训练参数，但我们不需要梯度。  </p><p><strong>对自动求导的实现还有一个非常重要的类,即函数(Function)</strong>    </p><p><strong>张量（Tensor）和函数(Function)是相互联系的,并形成一个非循环图来构建一个完整的计算过程.每个变量有一个.grad_fn属性,它指向创建该变量的一个Function(用户自己创建的变量除外,它的grad_fn属性为None)。</strong>  </p><p>如果你想计算导数,可以在一个张量上调用.backward().如果一个Tensor是一个标量(它只有一个元素值),你不必给backward()指定任何的参数,但是该Variable有多个值,你需要指定一个和该张量相同形状的的gradient参数(查看API发现实际为gradients参数)。  </p><pre><code>import torch  # 创建一个张量，饼设置reuqires_grad=True来跟踪计算过程x = torch.ones(2, 2, reuqires_grad=True)print(x)Out: tensor([[ 1.,  1.],        [ 1.,  1.]])  </code></pre><p>在张量上执行操作:  </p><pre><code>y = x + 2  print(y)Out:    tensor([[ 3.,  3.],         [ 3.,  3.]])</code></pre><p>因为y是通过一个操作创建的,所以它有grad_fn,而x是由用户创建,所以它的grad_fn为None.  </p><pre><code>print(y.grad_fn)Out:  &lt;AddBackward0 object at 0x0000020D2A5CC048&gt;  </code></pre><p>在张量y上执行更多操作：</p><pre><code>z = y * y * 3  out = z.mean()print(&quot;z : &quot;, z, &quot;, out: &quot;,  out) Out: z :  tensor([[ 27.,  27.],    [ 27.,  27.]]) , out:  tensor(27.)  </code></pre><p>.requires_grad_（…）按位（in-place）更改现有张量的requires_grad标志。如果没有给出，输入标志默认为True。  </p><pre><code>a = torch.randn(2, 2)a = ((a * 3) / (a - 0))print(a.requires_grad)a.requires_grad_(True)print(a.requires_grad)  b = (a * a).sum()print(b.grad_fn)  </code></pre><h2 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h2><p>现在我们来执行反向传播,因为out包含一个标量,out.backward()相当于执行out.backward(torch.tensor(1)):  </p><pre><code>out.backward()# 输出out对x的梯度d(out)/d(x):print(&quot;x.grad: &quot;, x.grad)  </code></pre><p>输出：<br><img src="https://i.loli.net/2018/06/15/5b2318dedffb5.jpg" alt>  </p><p>你应该得到一个值全为4.5的矩阵，我们把out称为张量O。则  </p><p><img src="https://i.loli.net/2018/06/15/5b231997cbfdf.jpg" alt>  </p><p>我们还可以用自动求导做更多有趣的事：  </p><pre><code>x = torch.randn(3, requires_grad=True)y = x * 2while y.data.norm() &lt; 1000:    y = y * 2print(y)</code></pre><p>输出：<br><img src="https://i.loli.net/2018/06/15/5b231afe51e77.jpg" alt>  </p><p>gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)<br>y.backward(gradients)</p><p>print(x.grad)  </p><p><img src="https://i.loli.net/2018/06/15/5b23228844307.jpg" alt>  </p><p>还可以通过使用torch.no_grad（）包装代码块来停止autograd跟踪在张量上的历史记录，其中require_grad = True：  </p><pre><code>print(x.requires_grad)print((x ** 2).requires_grad)with torch.no_grad():    print((x ** 2 ).requires_grad)  </code></pre><p>输出：  </p><p><img src="https://i.loli.net/2018/06/15/5b232499d1256.jpg" alt>  </p><p>Documentation of autograd and Function is at <a href="http://pytorch.org/docs/autograd" target="_blank" rel="noopener">http://pytorch.org/docs/autograd</a>  </p><h1 id="神经网络（Neural-Networks）"><a href="#神经网络（Neural-Networks）" class="headerlink" title="神经网络（Neural Networks）"></a>神经网络（Neural Networks）</h1><p>可以使用torch.nn包来构建神经网络。<br>我们已知道autograd包,nn包依赖autograd包来定义模型并求导.一个nn.Module包含各个层和一个faward(input)方法,该方法返回output。  </p><p>例如,我们来看一下下面这个分类数字图像的网络。<br><img src="https://pytorch.org/tutorials/_images/mnist.png" alt><br>convnet  </p><p>这是一个简单的前溃神经网络，它接受一个输入，然后一层接着一层的输入，直到最后得到结果。  </p><p>神经网络的典型训练过程如下：  </p><ul><li>定义神经网络模型。它有一些可学习的参数（或者权重）；</li><li>在输入数据集上迭代  </li><li>通过神经网络处理输入，主要体现在网络的前向传播;</li><li>计算损失（输出结果和正确值的差距大小）  </li><li>将梯度反向传播回网络的参数，反向传播求梯度。</li><li>根据梯度值更新网络的参数，主要使用如下简单的更新原则： <strong>weight = weight - learning_rate * gradient</strong>  </li></ul><h2 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h2><p>Let’s define this network:  </p><pre><code>import torch import torch.nn as nnimport torch.nn.functional as FClass Net(nn.Module):    def __init__(self):        # super就是在子类中调用父类方法时用的。        super(Net, self).__init__()   # 对继承自父类的属性进行初始化。而且是用父类的初始化方法来初始化继承的属性。也就是说，子类继承了父类的所有属性和方法，父类属性自然会用父类方法来进行初始化。当然，如果初始化的逻辑与父类的不同，不使用父类的方法，自己重新初始化也是可以的。        # 1 input image channel, 6 output channels, 5*5 square convution        # kernel        self.comv1 = nn.Conv2d(1, 6, 5)        self.comv2 = nn.Conv2d(6, 16, 5)        # an affine operation: y = Wx + b        self.fc1 = nn.Linear(16 * 5 * 5, 120)        self.fc2 = nn.Linear(120, 84)        self.fc3 = nn.Linear(84, 10)       def forward(self, x):        # Max pooling over a (2, 2) window        x = F.max_pool2d(F.relu(self.conv1(x),(2, 2))        # if the size is a square you can only specify a single number        x = F.max_pool2d(F.relu(self.conv2(x), 2)        x = view(-1, self.num_flat_features(x))        x = F.relu(self.fc1(x))        x = F.relu(self.fc2(x))        x = self.fc3(x)        return x        def num_flat_features(self, x):        size = x.size()[1:]        num_features = 1        for s in size:        num_features *= s        return num_featuresnet = Net()print(net)        </code></pre><p>输出：<br><img src="https://i.loli.net/2018/06/15/5b2382a02805a.jpg" alt>  </p><p><strong>learn more about the network：</strong><br>在pytorch中只需要定义forward函数即可，backward函数会在使用autograd时自动创建（其中梯度是计算过的）。可以在forward函数中使用Tensor的任何操作。</p><p>net.aprameters()会返回模型中可学习的参数。</p><pre><code>params = list(net.parameters())print(&apos;可学习参数的个数：&apos;, len(params))#print(&quot;可学习的参数：&quot;, params)print(params[0].size())  # conv1&apos;s的权值for param in params:    print(param.size())</code></pre><p>以上代码段实现将该神经网络的可学习参数都放到params中,并且输出了第一层conv的参数大小.<br>输出：<br><img src="https://i.loli.net/2018/06/20/5b29b0d07aa34.jpg" alt>  </p><p>注意：我们来尝试一个32<em>32的随机输入，这个网络（LeNet）期望的输入大小是32</em>32。如果使用的是MINIST数据集来训练这个网络，请把数据集中图片大小调整到32*32。  </p><pre><code>input = torch.randn（1，1，32，32）print(&quot;input: &quot;, input)out = net(input)print(&quot;out: &quot;, out)  </code></pre><p>输出：<br><img src="https://i.loli.net/2018/06/20/5b29b37ee3361.jpg" alt></p><p>把所有参数的梯度缓存区清零，然后进行随机梯度的的反向传播。  </p><pre><code>net.zero_grad()out.backward(torch.randn(1, 10))  </code></pre><p>Note:  </p><ul><li>torch.nn只支持小批量输入（mini-batches）。整个torch.nn包仅支持作为最小样本量的输入，而不支持单个样本。</li><li>例如，nn.Conv2d只接受一个四维张量（nSamples <em> nChannels </em> Height <em> Width），即样本数</em>通道数<em>高度</em>宽度。</li><li>如果你有单个样本,只需使用input.unsqueeze(0)来添加一个虚假的批量维度.  </li></ul><p>在继续之前,我们回顾一下到目前为止见过的所有类。  </p><p><strong>回顾:</strong>  </p><ul><li><strong>torch.Tensor</strong> - 一个支持autograd等操作（比如banckward()）的多维数组，也支持梯度w、r、t等张量。  </li><li><strong>nn.Module</strong> - 神经网络模块。封装参数的便捷方式，移动到GPU运行，导出，加载等。</li><li><strong>nn.Parameters</strong> - 一种张量，当作为属性赋值给一个模块（Module）时,能被自动注册为一个参数。</li><li><strong>autograd.Function</strong> - 实现一个自动求导操作的前向和反向定义,每个张量操作至少创建一个函数节点，该节点连接到创建Tensor并对其历史进行编码的函数。</li></ul><p>以上内容：</p><ul><li>定义一个神经网络  </li><li>处理输入和调用backward</li></ul><p>剩下的内容:</p><ul><li>计算损失值  </li><li>更新神经网络的权值</li></ul><h2 id="损失函数（Loss-Function）"><a href="#损失函数（Loss-Function）" class="headerlink" title="损失函数（Loss Function）"></a>损失函数（Loss Function）</h2><p>一个损失函数接受一对（output， target）作为输入(output为网络的输出,target为实际值),，计算一个值来评估网络的输出和目标值（实际值）相差多少。</p><p>在nn包中有几种不同的损失函数。一个简单的损失函数是:nn.MSELoss,它计算的是网络的输出和目标值之间的均方误差。  </p><p>例如：   </p><pre><code>output = net(input)target = torch.arrange(1, 11)  # 例如，虚拟目标target = target.view(1, -1)# 使其与输出(output)形状相同criterion = nn.MSELoss()loss = criterion(output, target)print(loss)</code></pre><p>输出：<br><img src="https://i.loli.net/2018/06/21/5b2b1073be2a2.jpg" alt></p><p>现在，如果反向跟踪loss,用它的属性.grad_fn，你会的到下面这样的一个计算图：</p><p><img src="https://i.loli.net/2018/06/22/5b2cb85890b77.jpg" alt></p><p>所以, 当调用loss.backward(),整个图与w、r、t的损失不同,图中所有变量（其requres_grad=True）将拥有.grad变量来累计他们的梯度.</p><p>为了说明,我们反向跟踪几步:</p><pre><code>print(loss.grad_fn)  # MSELossprint(loss.grad_fn.next_functions[0][0])  # Linearprint(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU</code></pre><p>输出：  </p><p><img src="https://i.loli.net/2018/06/24/5b2f2cc535de0.jpg" alt>  </p><h2 id="反向传播（Backprop）"><a href="#反向传播（Backprop）" class="headerlink" title="反向传播（Backprop）"></a>反向传播（Backprop）</h2><p>为了反向传播误差,我们所需做的是调用loss.backward().你需要清除已存在的梯度,否则梯度将被累加到已存在的梯度。  </p><p>现在，我们将调用loss.backward(),并查看conv1层的偏置项在反向传播前后的梯度。</p><pre><code>net.zero_grad()# zeroes the gradient buffers of all parametersprint(&apos;conv1.bias.grad before backward&apos;)print(net.conv1.bias.grad) loss.backward() print(&apos;conv1.bias.grad after backward&apos;)print(net.conv1.bias.grad)</code></pre><p>输出：</p><p><img src="https://i.loli.net/2018/06/22/5b2cb7c7eab0b.jpg" alt>  </p><p>现在我们也知道了如何使用损失函数。</p><p>神经网络包包含各种深度神经网络的构建模块和损失函数。完整的文档列表在<a href="http://pytorch.org/docs/nn" target="_blank" rel="noopener">这里</a>。  </p><p>接下来是更新权重。</p><h2 id="更新权重"><a href="#更新权重" class="headerlink" title="更新权重"></a>更新权重</h2><p>最简单的更新权重的方法是：随机梯度下降（SGD）。  </p><pre><code>weight = weight - learning_rate * gradient</code></pre><p>可以通过以下代码实现梯度更新：</p><pre><code>learning_rate = 0.01for f in net.parameters():    f.data.sub_(f.grad.data * learning_rate)  </code></pre><p>当使用神经网络时，有几种不同的权重更新方法，例如有SGD,Nesterov-SGD,Adam,RMSProp等。为了能够更好地使用这些方法，Pytorch提供了一个小工具包：torch.optim来实现上述所说的更新方法。使用起来也很简单，代码如下：  </p><pre><code>import torch.optim as optim  # create your optimizeroptimizer = optim.SGD(net.parameters(), lr=0.01)# in your training loop:optimizer.zero_grad()    # zero the gradient buffersoutput = net(input)loss = criterion(output, target)loss.backward()optimizer.step()         # Does the update  </code></pre><h1 id="训练分类器（Training-a-classifier）"><a href="#训练分类器（Training-a-classifier）" class="headerlink" title="训练分类器（Training a classifier）"></a>训练分类器（Training a classifier）</h1><h2 id="What-about-Data"><a href="#What-about-Data" class="headerlink" title="What about Data?"></a>What about Data?</h2><p>一般来说，在处理图像、文本、音频或者视频数据时可以使用标准的python包把数据加载成一个numpy数组， 然后再把这个数组转换成一个 torch.*Tensor。  </p><ul><li>对于图像来说，可以使用Pillow、OpenCV等包。  </li><li>对于音频来说，可以使用scipy和librosa包。  </li><li>对于文本来说，无论是原始的Python还是基于Cython的加载，或者NLTK和SpaCy都是有用的</li></ul><p>特别是对于视觉，我们已经创建了一个名为torchvision的软件包，它具有常用数据集的数据加载器，如Imagenet，CIFAR10，MNIST等，以及图像数据转换器，即torchvision.datasets 和 torch.utils.data.DataLoader。  </p><p>这提供了巨大的便利并避免了编写样板代码。  </p><p>本例中我们将使用CIFAR-10数据集。它有以下几类： ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’。在CIFAR-10数据集中的数据大小是3<em>32</em>32，例如尺寸为32x32像素的3通道彩色图像。  </p><p><img src="https://pytorch.org/tutorials/_images/cifar10.png" alt>  </p><h2 id="训练一个图像分类器"><a href="#训练一个图像分类器" class="headerlink" title="训练一个图像分类器"></a>训练一个图像分类器</h2><ul><li>使用torchvision加载并归一化（normalizing）CIFAR10训练和测试数据集。</li><li>定义一个卷积神经网络  </li><li>定义一个损失函数</li><li>在训练集上训练网络  </li><li>在测试集上测试数据  </li></ul><ol><li>加载和归一化CIFAR10  </li></ol><p>torchvison能够很简单的加载CIFAR10。  </p><pre><code>import torch  import torchvision  import torchvision.transforms as transforms  </code></pre><p>torchvision数集的输出是范围为[0, 1]的PILImage图像。我们将其转换为归一化范围[-1, 1]的张量。  </p><pre><code>transform = transform.Compose([transform])  </code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Deep-Learning-with-PyTorch-A-60-Minute-Blitz&quot;&gt;&lt;a href=&quot;#Deep-Learning-with-PyTorch-A-60-Minute-Blitz&quot; class=&quot;headerlink&quot; title=&quot;Deep
      
    
    </summary>
    
      <category term="Pytorch" scheme="https://xiaoxiaoaurora.github.io/categories/Pytorch/"/>
    
      <category term="Tutorials" scheme="https://xiaoxiaoaurora.github.io/categories/Pytorch/Tutorials/"/>
    
    
      <category term="Pytorch" scheme="https://xiaoxiaoaurora.github.io/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Adapting Grammatical Error Correction Based on the Native Language of Writers with Neural Network Joint Models</title>
    <link href="https://xiaoxiaoaurora.github.io/2018/06/04/Adapting-Grammatical-Error-Correction-Based-on-the-Native-Language-of-Writers-with-Neural-Network-Joint-Models/"/>
    <id>https://xiaoxiaoaurora.github.io/2018/06/04/Adapting-Grammatical-Error-Correction-Based-on-the-Native-Language-of-Writers-with-Neural-Network-Joint-Models/</id>
    <published>2018-06-04T01:34:57.000Z</published>
    <updated>2019-04-04T10:02:03.722Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Adapting-Grammatical-Error-Correction-Based-on-the-Native-Language-of-Writers-with-Neural-Network-Joint-Models"><a href="#Adapting-Grammatical-Error-Correction-Based-on-the-Native-Language-of-Writers-with-Neural-Network-Joint-Models" class="headerlink" title="Adapting Grammatical Error Correction Based on the Native Language of Writers with Neural Network Joint Models"></a>Adapting Grammatical Error Correction Based on the Native Language of Writers with Neural Network Joint Models</h1><p><strong>本篇论文： 采用一个使用L1-specific学习者文本的NNJM（神经网络联合模型），并把它作为一个feature整合到一个基于GEC的统计机器翻译系统（解码器）中。</strong><br>本文的两点贡献：  </p><ul><li>这是第一个使用SMT方法并覆盖所有错误类型的工作来对GEC执行基于L1的自适应  </li><li>我们引入了一种新的NNJM适应方法，并证明该方法可以处理比一般域数据小得多的域内数据。  适应（Adaptation）是通过使用训练在一般域数据上的未适应的NNJM来完成的。使用自归一化的对数似然目标函数作为起点，使用较小的L1-specific的域内数据进行后续迭代训练，并使用包含Kullback-Leibler (KL)离散正则项的修正目标函数。  </li></ul><p><img src="https://i.loli.net/2018/06/04/5b1504c415ae2.jpg" alt>  </p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>尽管第一语言(The Native Language,L1)对第二语言(The Second Language,L2)的写作有显著的影响，但是基于作者母语（L1)的适应（Adaptation）是 语法纠错(GEC)任务仍未充分探索的一个重要方面。<strong>本文采用神经网络联合模型(神经网络联合模型,NNJM)，使用L1-specific的学习者文本，将其集成到基于统计机器翻译(SMT)的GEC系统中。具体地说，我们针对一般学习者文本(general learner text，不是L1-specific的)训练NNJM，然后再使用Kullback-Leibler divergence正则化目标函数训练L1-specific的数据，以保持模型的泛化</strong>。我们将这个调整后的NNJM作为一个基于SMT的英语GEC系统的功能，并表明该系统在L1中文、俄语和西班牙语作者的英语文本上获得了显著的F0.5。</p><h2 id="为什么考虑-L1-specific-学习者文本？"><a href="#为什么考虑-L1-specific-学习者文本？" class="headerlink" title="为什么考虑 L1-specific 学习者文本？"></a>为什么考虑 L1-specific 学习者文本？</h2><p>主要是L1背景不同，学习第二语言时有不同的影响，也就是L1和L2之间的跨语言影响。</p><ul><li><p>芬兰的英语学习者：过度概括了介词‘in’的使用。 </p><p><code>例如：“When they had escaped in the police car they sat under the tree.”这个句子中的介词&quot;in&quot; 应该为 “from” 。</code></p></li><li><p>中国的英语学习者：由于汉语中没有动词形态变化，所以在书写英语时经常会出现动词时态和动词形式错误。</p></li><li><p>第一语言对第二语言写作的跨语言影响是一个非常复杂的现象，学习者所犯的错误不能直接归因于两种语言的相似或不同。  </p></li><li>学习者似乎遵循着两个互补的原则（Ortega 2009）:第一语言中起作用的可能在第二语言中起作用，因为人类语言基本上是相似的;但如果听起来太像L1，那么在L2中可能就行不通了。</li></ul><ul><li><strong>因此本文采用数据驱动（data-driven）的方法对这些影响因素进行建模,并使用具有相同L1背景的作者撰写的L2文本对GEC系统进行调整。</strong>  </li></ul><h2 id="Why-SMT"><a href="#Why-SMT" class="headerlink" title="Why SMT ?"></a>Why SMT ?</h2><p><strong>GEC中两个最常用的方法是：分类方法（the classification approach）和 统计机器翻译方法（the statistical machine translation approach）。</strong></p><p><strong>SMT的优势</strong>：</p><ul><li><p>SMT方法把不合语法的文本转换成格式良好的文本的学习文本转换的能力，使得它能够纠正各种各样的错误，包括复杂的错误，而这些错误是很难用分类方法（the classification approach）处理的，这也使得SMT成为GEC的流行范例。</p></li><li><p>SMT方法并不用于专门的错误类型建模，也不需要像解析（parsing）和词性标注(POS tagging)这样的语言分析。</p></li></ul><h2 id="NNJM-–-gt-Neural-Network-Joint-Model"><a href="#NNJM-–-gt-Neural-Network-Joint-Model" class="headerlink" title="NNJM –&gt; Neural Network Joint Model"></a>NNJM –&gt; Neural Network Joint Model</h2><p>关于NNJM在论文<a href="http://www.aclweb.org/anthology/P/P14/P14-1129.pdf" target="_blank" rel="noopener">Fast and Robust Neural Network Joint Models for Statistical Machine<br>Translation</a>（Devlin et al.,2014）</p><p><strong>NNJM:</strong>通过一个源上下文窗口扩展NNLM（which augments the NNJM with a source context window）。该模型是纯词汇化(purely lexicalized)的，可以集成到任何MT的Decoder中。具体来说，该模型利用m-word源窗口扩展一个n-gram目标语言模型。和以往的联合模型不同，该模型能够很容易作为一个feature被整合到任何SMT解码器中。</p><p><img src="https://i.loli.net/2018/06/11/5b1dde6c03fe7.jpg" alt>    </p><p><strong>NNJM近似地估计了以源句子S为条件的目标假设T的概率。遵循目标的标准n-gram LM分解，其中每个目标字ti都受前面的n- 1个目标字的制约。为了使这个模型成为一个联合模型，对源上下文向量 <img src="https://i.loli.net/2018/06/11/5b1de290abead.jpg" alt>进行了条件分析：</strong>  </p><p><img src="https://i.loli.net/2018/06/11/5b1de1e00531f.jpg" alt>  </p><p>每一个目标词ti都直接对应着一个在位置ai的源词，<img src="https://i.loli.net/2018/06/11/5b1de290abead.jpg" alt>是在以ai为中心的m-word的源窗口。<br><img src="https://i.loli.net/2018/06/11/5b1de8f27cf9e.jpg" alt>  </p><p>这种从属（affiliation）概念源自单词对齐，但与单词对齐不同，每个目标单词必须与一个非空(non-NULL)源单词相关联。  </p><p><img src="https://i.loli.net/2018/06/11/5b1de8f27cf9e.jpg" alt>  </p><p><img src="https://i.loli.net/2018/06/11/5b1dec1261a97.jpg" alt>  </p><p>中文-英语平行句子的NNJM上下文模型例子如下图：  </p><p><img src="https://i.loli.net/2018/06/11/5b1deb3bb7995.jpg" alt></p><p>论文中采用的是n=4、m=11的15-gram LM 模型（神经网络语言模型能够优雅地扩展并利用任意大的上下文大小）。论文中的神经网络结构与Bengio et.al等人的前溃神经网络语言模型结构基本相似，如下图。<br><img src="https://i.loli.net/2018/06/11/5b1dfed06cf34.jpg" alt></p><h3 id="NNJM中的神经网络结构"><a href="#NNJM中的神经网络结构" class="headerlink" title="NNJM中的神经网络结构"></a>NNJM中的神经网络结构</h3><p>NNJM中的神经网络架构与Bengio et al.(2003)所描述的原始前馈NNLM体系结构（feed-forward NNJM architecture）几乎相同。<br>隐藏层大小、词汇表大小和源窗口大小选择了这些值:  </p><p><img src="https://i.loli.net/2018/06/11/5b1e2e496fa49.jpg" alt>    </p><p>由于NNJM使用的是一个固定窗口的上下文，所以很容易将其整合到SMT解码器框架中，实验结果也证明了这样提升了SMT-based GEC的性能。  </p><h2 id="A-MT-Framework-For-GEC"><a href="#A-MT-Framework-For-GEC" class="headerlink" title="A MT Framework For GEC"></a>A MT Framework For GEC</h2><p>本文中将GEC视为从一个可能错误的输入句子到一个纠正句子的翻译过程。<br>框架设计细节：</p><ul><li>采用一个基于短语的SMT系统–Moses框架,它主要是通过一个对数线性模型来找到最佳假设 T*：</li></ul><p><img src="https://i.loli.net/2018/06/04/5b150df813df1.jpg" alt></p><ul><li><p>SMT中两个主要部分：翻译模型(TM)和语言模型（LM）。</p><ul><li><strong>TM:</strong>  主要负责生成假设T（通常是短语表），使用并行数据（即，学习者写入的句子（源数据）及其相应的校正句子（目标数据））进行训练。还使用正向和反向短语翻译概率和词汇权重等特征对假设进行评分，从而选出最佳假设T*。  </li><li><strong>LM:</strong>  在格式良好的文本上进行驯良从而保证输出的流畅性。用MERT计算特征权重<img src="https://i.loli.net/2018/06/04/5b150d9c6e9d7.jpg" alt>，用开发集优化<img src="https://i.loli.net/2018/06/04/5b150dcc44d7c.jpg" alt>度量。  </li></ul></li></ul><ul><li>由于NNJM有依赖于固定上下文的前馈结构，所以很容易将其作为一个feature整合到SMT解码器框架中。特征值由logP(T|S)给出，这个logP(T|S)是给出上下文的假设T中每个单词的对数概率总和。  </li></ul><p><img src="https://i.loli.net/2018/06/04/5b150f88e0c9a.jpg" alt>  </p><p>   上下文hi由n-1个之前的目标词和围绕与目标词ti对齐的源词的m个源词组成。</p><ul><li>神经网络输出层的每个维度(Chollampatt et al.， 2016)给出了给定上下文h的输出词汇表中单词t出现的概率。  </li></ul><p><img src="https://i.loli.net/2018/06/04/5b151001051d9.jpg" alt></p><ul><li>神经网络中的参数包括权值、偏差和嵌入矩阵都是用带随机梯度下降反向传播进行训练，损失函数使用的是与Devlin等所用(2014)相似的自归一项的对数似然目标函数。  </li></ul><p><img src="https://i.loli.net/2018/06/04/5b15114094bcb.jpg" alt><br><img src="https://i.loli.net/2018/06/04/5b15117c5edb3.jpg" alt></p><h2 id="KL-Divergence-Regularized-Adaptation"><a href="#KL-Divergence-Regularized-Adaptation" class="headerlink" title="KL Divergence Regularized Adaptation"></a>KL Divergence Regularized Adaptation</h2><p><img src="https://i.loli.net/2018/06/04/5b151221b01ab.jpg" alt></p><p><strong>in-domain data：</strong>  由L1-specific的作者所写的错误文本及其相应的修正文本组成。  </p><p>这种自适应训练是使用具有正则化项K的修正目标函数来完成的，该函数用于最小化pGD(y|h)与网络输出概率分布p(y|h)之间的KL离散度。 K将防止训练期间估计的概率分布偏离通用域NNJM的分布。<br><img src="https://i.loli.net/2018/06/04/5b151644e457b.jpg" alt></p><p><strong>最终的自适应步骤的目标函数是L和K中的项的线性组合。</strong><br><img src="https://i.loli.net/2018/06/04/5b151631874de.jpg" alt> </p><h2 id="数据和评价"><a href="#数据和评价" class="headerlink" title="数据和评价"></a>数据和评价</h2><h3 id="训练数据处理和来源："><a href="#训练数据处理和来源：" class="headerlink" title="训练数据处理和来源："></a>训练数据处理和来源：</h3><p><strong>来源：</strong>  </p><ul><li>新加坡国立大学学生英语语料库（the NUS<br>Corpus of Learner English (NUCLE) (Dahlmeier<br>et al., 2013)）  </li><li>Lang-8学习者语料库（the Lang-8 Learner Corpora v2(Mizumoto et al., 2011)），Lang-8提取的是只学习英语的学习者的文本。    </li></ul><p><strong>处理：</strong>  </p><ul><li>用语言识别工具langid.py（<a href="https://github.com/saffsd/langid.py" target="_blank" rel="noopener">https://github.com/saffsd/langid.py</a>）来获取纯净的英语句子  </li><li>删除Lang-8中的噪声源-目标句子对（ noisy sourcetarget sentence pairs），即其中源句子和目标句子长度的比率在[0.5,2.0]之外的句子对，或者它们的单词重叠比率小于0.2的句子对。</li><li>删除NUCLE和Lang-8中源句子或目标句子超过80个单词的句子对。</li></ul><p>预处理后训练数据的统计见Table1：<br><img src="https://i.loli.net/2018/06/04/5b14988d1cd10.jpg" alt>  </p><p><img src="https://i.loli.net/2018/06/04/5b1498c160aef.jpg" alt>  </p><ul><li>基于Lang-8中提供的L1信息获取的自适应L1-specific 域内信息。</li><li>每一个L1，它的域外数据是由除L1-specific域内数据在外的联合训练数据（CONCAT）中获取的。</li></ul><h3 id="开发测试集"><a href="#开发测试集" class="headerlink" title="开发测试集"></a>开发测试集</h3><p>从公共可用的 CLC-FCE 语料库中获取。FCE语料库包含由1,244位不同候选人在1,2000年和2001年参加剑桥ESOL英语第一证书（FCE）考试所写的1,244个脚本。根据脚本数量分成数量大致相等的两部分作为开发集和测试集。<br><img src="https://i.loli.net/2018/06/04/5b149c046f755.jpg" alt>  </p><p><strong>Evaluation</strong><br><img src="https://i.loli.net/2018/06/04/5b149cac95784.jpg" alt>  </p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="Baseline-SMT-based-GEC-sysytem"><a href="#Baseline-SMT-based-GEC-sysytem" class="headerlink" title="Baseline SMT-based GEC sysytem"></a>Baseline SMT-based GEC sysytem</h3><p>用Moses(Version 3)构建所有基于SMT的GEC系统。</p><p><img src="https://i.loli.net/2018/06/04/5b14a16c9de10.jpg" alt>  </p><h3 id="NNJM-Adaptation"><a href="#NNJM-Adaptation" class="headerlink" title="NNJM Adaptation"></a>NNJM Adaptation</h3><p><img src="https://i.loli.net/2018/06/04/5b14a23eb11e3.jpg" alt> :用全部的训练数据训练10个epoch。源上下文窗口大小设置为5，目标上下文窗口大小设置为4，形成一个(5+5)-gram的联合模型。使用一个mini-batch大小为128、学习率为0.1的随机梯度下降（SGD）进行训练。  </p><h3 id="Comparison-to-Other-Adaptation-Techniques"><a href="#Comparison-to-Other-Adaptation-Techniques" class="headerlink" title="Comparison to Other Adaptation Techniques"></a>Comparison to Other Adaptation Techniques</h3><h3 id="Effect-of-Adaptation-Data"><a href="#Effect-of-Adaptation-Data" class="headerlink" title="Effect of Adaptation Data"></a>Effect of Adaptation Data</h3><h3 id="关于正则化的影响"><a href="#关于正则化的影响" class="headerlink" title="关于正则化的影响"></a>关于正则化的影响</h3><p><img src="https://i.loli.net/2018/06/04/5b14b9ae57f13.jpg" alt></p><h3 id="Evaluation-on-Benchmark-Dataset"><a href="#Evaluation-on-Benchmark-Dataset" class="headerlink" title="Evaluation on Benchmark Dataset"></a>Evaluation on Benchmark Dataset</h3><p><img src="https://i.loli.net/2018/06/04/5b14bb9cf0938.jpg" alt>   </p><h2 id="讨论和错误分析"><a href="#讨论和错误分析" class="headerlink" title="讨论和错误分析"></a>讨论和错误分析</h2><p><img src="https://i.loli.net/2018/06/04/5b14c2fdb74b8.jpg" alt>  </p><h1 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h1><ul><li>HOO(Helping Our Own)和 CoNll共享任务 使得GEC变得普及流行。</li><li>GEC已发表的相关工作旨在构建针对具体错误类型分类器和基于规则的系统，并将其结合构建成混合系统（hybrid systems）。</li></ul><p>L1和L2间的跨语言影响主要用于母语识别任务，还用于类型学预测和ESL数据的预测误差分布。  </p><ul><li>最近，针对GEC提出了端对端（end-to-end）神经机器翻译框架，显示出了具有竞争力的结果。  </li><li>本文中利用SMT方法和神经网络联合模型的优点，将基于L1背景的作者的NNJM模型整合到SMT框架中。通过KL离散正则化自适应来避免在较小的域内数据中的过拟合。  </li><li>SMT中其它调节技术包括混合建模（mixture modeling）和可选的解码路径（alternative decoding paths）。  </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Adapting-Grammatical-Error-Correction-Based-on-the-Native-Language-of-Writers-with-Neural-Network-Joint-Models&quot;&gt;&lt;a href=&quot;#Adapting-G
      
    
    </summary>
    
      <category term="Papers" scheme="https://xiaoxiaoaurora.github.io/categories/Papers/"/>
    
      <category term="GEC" scheme="https://xiaoxiaoaurora.github.io/categories/Papers/GEC/"/>
    
    
      <category term="Papers" scheme="https://xiaoxiaoaurora.github.io/tags/Papers/"/>
    
      <category term="论文阅读笔记" scheme="https://xiaoxiaoaurora.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    
      <category term="GEC" scheme="https://xiaoxiaoaurora.github.io/tags/GEC/"/>
    
      <category term="NNJM" scheme="https://xiaoxiaoaurora.github.io/tags/NNJM/"/>
    
  </entry>
  
  <entry>
    <title>Python之argparse使用</title>
    <link href="https://xiaoxiaoaurora.github.io/2018/05/09/Python%E4%B9%8Bargparse%E4%BD%BF%E7%94%A8/"/>
    <id>https://xiaoxiaoaurora.github.io/2018/05/09/Python之argparse使用/</id>
    <published>2018-05-09T01:34:57.000Z</published>
    <updated>2019-04-04T16:01:28.175Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python-命令行解析工具Argparse的介绍及使用"><a href="#Python-命令行解析工具Argparse的介绍及使用" class="headerlink" title="Python 命令行解析工具Argparse的介绍及使用"></a>Python 命令行解析工具Argparse的介绍及使用</h1><h2 id="Argparse-Tutorial-smile"><a href="#Argparse-Tutorial-smile" class="headerlink" title="Argparse Tutorial:smile:"></a>Argparse Tutorial:smile:</h2><p>argparse是python的命令行解析工具，是Python标准库推荐使用的命令行参数解析模块，负责从sys.argv中解析程序所需的参数，同时argparse还可以自动生成帮助信息和错误提示。</p><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>以下代码是一个Python程序，它采用整数列表并生成总和或最大值:  </p><p><img src="https://i.loli.net/2018/07/13/5b488865257db.jpg" alt>  </p><p><img src="https://i.loli.net/2018/07/13/5b488843cba17.jpg" alt></p><p>假设上面的Python代码保存在名为argparse_tu1.py的文件中, 它可以在命令行运行，并提供有用的帮助消息:  </p><p><img src="https://i.loli.net/2018/07/13/5b48889fd6df3.jpg" alt>  </p><p>当使用适当的参数运行时，它将输出命令行整数的和或最大值:  </p><p><img src="https://i.loli.net/2018/07/13/5b4889c570a96.jpg" alt>  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python argparse_tu1.py <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> --sum</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2018/07/13/5b488a38483e6.jpg" alt>  </p><p>如果传入无效的参数，它将发出一个错误:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python argparse_tu1.py</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2018/07/13/5b488abee045c.jpg" alt></p><p>以下部分将引导完成此示例:</p><h3 id="创建一个parser"><a href="#创建一个parser" class="headerlink" title="创建一个parser"></a>创建一个parser</h3><p>使用argparse的第一步就是创建一个ArgumentParser对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser = argparse.ArgumentParser(description <span class="string">'Process some integers.'</span>)</span><br></pre></td></tr></table></figure><p>创建的ArgumentParser对象保存了将命令行参数解析为Python数据类型的所需要的所有信息。    </p><h3 id="添加参数"><a href="#添加参数" class="headerlink" title="添加参数"></a>添加参数</h3><p>创建ArgumentParser对象之后，需要向其声明指定程序所需的参数信息（几个参数、是否必须、参数类型等等），这一步需要调用add_argument()方法实现。add_argument()方法会告诉ArgumentParser对象如何在命令行上获取字符串（注意：所有的命令行参数都是string类型）并将它们转换为程序所需的对象。调用parse_args（）时会存储和使用此信息。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(<span class="string">'integers'</span>, metavar=<span class="string">'N'</span>, type=int, nargs=<span class="string">'+'</span>, help=<span class="string">'an integer for the accumulator'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--sum'</span>, dest=<span class="string">'accumulator'</span>, action=<span class="string">'store_const'</span>, const=sum, default=max, help=<span class="string">'sum the integers(default: find the max)'</span>)</span><br></pre></td></tr></table></figure><p>That’s very very boring ! :cry:</p><p>乌拉乌拉乌拉:smile: :cry: :happy: :awkward:</p><p>参考：<a href="https://docs.python.org/3/library/argparse.html" target="_blank" rel="noopener">https://docs.python.org/3/library/argparse.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Python-命令行解析工具Argparse的介绍及使用&quot;&gt;&lt;a href=&quot;#Python-命令行解析工具Argparse的介绍及使用&quot; class=&quot;headerlink&quot; title=&quot;Python 命令行解析工具Argparse的介绍及使用&quot;&gt;&lt;/a&gt;Py
      
    
    </summary>
    
      <category term="Python" scheme="https://xiaoxiaoaurora.github.io/categories/Python/"/>
    
    
      <category term="Python" scheme="https://xiaoxiaoaurora.github.io/tags/Python/"/>
    
      <category term="Argparse" scheme="https://xiaoxiaoaurora.github.io/tags/Argparse/"/>
    
  </entry>
  
  <entry>
    <title>Python | 对象和类</title>
    <link href="https://xiaoxiaoaurora.github.io/2018/05/01/Python%E4%B9%8B%E5%AF%B9%E8%B1%A1%E5%92%8C%E7%B1%BB/"/>
    <id>https://xiaoxiaoaurora.github.io/2018/05/01/Python之对象和类/</id>
    <published>2018-05-01T01:34:57.000Z</published>
    <updated>2019-04-04T10:23:15.460Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://i.loli.net/2018/05/31/5b0f529386dd5.jpg" alt>  </p><h1 id="Python之对象和类"><a href="#Python之对象和类" class="headerlink" title="Python之对象和类"></a>Python之对象和类</h1><h2 id="1-什么是对象"><a href="#1-什么是对象" class="headerlink" title="1. 什么是对象"></a>1. 什么是对象</h2><p><img src="https://i.loli.net/2018/06/07/5b18cf7e08af7.jpg" alt><br><img src="https://i.loli.net/2018/06/07/5b18cf9e823d9.jpg" alt></p><h2 id="2-使用class定义类"><a href="#2-使用class定义类" class="headerlink" title="2. 使用class定义类"></a>2. 使用class定义类</h2><p>如果把类比作塑料盒子，类则像是制作和自用的模具。例如，Python的内置类String可以创建像‘cat’和‘duck’这样的字符串对象。Python中还有许多用来创建其他标准数据类型的类，包括字典、列表等。如果想要在Python中创建属于自己的对象，首先必须用关键词class来定义一个类。<br><img src="https://i.loli.net/2018/05/31/5b0f597a67cff.jpg" alt></p>   <figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#创建一个空类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">Person</span>():</span></span><br><span class="line"><span class="class">         pass  </span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">      # 通过类名创建对象，同调用函数一样：</span></span><br><span class="line"><span class="class">      someone = <span class="type">Person</span>()</span></span><br><span class="line"><span class="class">      (以上例子中，<span class="type">Person</span>()创建了一个<span class="type">Person</span>类的对象，并给它赋值someone这个名字。但是由于<span class="type">Person</span>类是空的，所以由它创建的对象someone实际上什么也做不了。)</span></span><br></pre></td></tr></table></figure><p>重新定义类，将Python中特殊的对象初始化方法<strong>init</strong>放入其中：  </p><pre><code><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>():</span></span><br><span class="line">       <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>，name)</span></span>:      </span><br><span class="line">       <span class="comment">#实际的Python类的定义形式。 __init__()是Python中一个特殊的函数名，用于根据类的定义创建实例对象。self参数指向了这个正在被创建的对象本身。当你在类声明里定义__init__()方法时，第一个参数必须为self。尽管self并不是一个Python保留字，但它很常用。</span></span><br><span class="line">       <span class="comment"># 在初始化方法中添加name参数</span></span><br><span class="line">           <span class="keyword">self</span>.name = name</span><br><span class="line">            </span><br><span class="line"><span class="comment">#用Person类创建一个对象，为name特性传递一个字符串参数。</span></span><br><span class="line">hunter = Person(<span class="string">"Elmer Fudd"</span>)</span><br></pre></td></tr></table></figure></code></pre><p><img src="https://i.loli.net/2018/05/31/5b0f5f79e4568.jpg" alt>    </p><p><img src="https://i.loli.net/2018/05/31/5b0f5fd20cec4.jpg" alt></p><pre><code>print(&quot;The mighty hunter:&quot;, hunter.name)  The mighty hunter: Elmer Fudd  </code></pre><p><img src="https://i.loli.net/2018/05/31/5b0f6198229f2.jpg" alt></p><h2 id="3-继承"><a href="#3-继承" class="headerlink" title="3. 继承"></a>3. 继承</h2><p><img src="https://i.loli.net/2018/05/31/5b0f620f41ace.jpg" alt>  </p><pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Car</span><span class="params">()</span>:</span></span><br><span class="line">   <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Yugo</span><span class="params">(Car)</span>:</span></span><br><span class="line"><span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="comment">#接着，为每个类创建一个实例对象:</span></span><br><span class="line">   give_me_a_car = Car()</span><br><span class="line">   give_me_a_yugo = Yugo()</span><br></pre></td></tr></table></figure></code></pre><p><img src="https://i.loli.net/2018/05/31/5b0f6abc595c8.jpg" alt>  </p><pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Car</span><span class="params">()</span>:</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">exclaim</span><span class="params">(self)</span>:</span></span><br><span class="line">print(<span class="string">"I'm a Car!"</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Yugo</span><span class="params">(Car)</span>:</span></span><br><span class="line"><span class="keyword">pass</span></span><br></pre></td></tr></table></figure># 最后，为每个类创建一个对象，并调用刚刚声明的exclaim方法：&gt;&gt;&gt; give_me_a_car = Car()&gt;&gt;&gt; give_me_a_yugo = Yugo()&gt;&gt;&gt; give_me_a_car.exclaim()I&apos;m a Car!&gt;&gt;&gt; give_me_a_yugo.exclaim()I&apos;m a Car!</code></pre><p>我们不需要做任何特殊的操作，Yugo就自动从Car那里继承了exclaim()方法。但事实上，我们并不希望Yugo在exclaim()方法里宣称它是一个Car，这可能会造成身份危机（无法区分Car和Yugo）。让我们来看看怎么解决这个问题。  </p><h2 id="4-覆盖方法"><a href="#4-覆盖方法" class="headerlink" title="4. 覆盖方法"></a>4. 覆盖方法</h2><p>新创建的子类会自动继承父类的所有信息。那子类是如何替代–或者说覆盖（override）–父类的方法。Yugo和Car一定存在着某些区别，不然的话，创建它又有什么意义？</p><p>尝试改写以下Yugo中的exclaim()方法的功能：</p><pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Car</span><span class="params">()</span>:</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">exclaim</span><span class="params">(self)</span>:</span></span><br><span class="line">print(<span class="string">"I'm a Car!"</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Yugo</span><span class="params">(Car)</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exclaim</span><span class="params">(self)</span>:</span></span><br><span class="line">print(<span class="string">"I'm a Yugo! Much like a Car , but more Yugo-ish."</span>)  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   <span class="comment">#接着，为每个类创建一个实例对象:</span></span><br><span class="line">   give_me_a_car = Car()</span><br><span class="line">   give_me_a_yugo = Yugo()</span><br></pre></td></tr></table></figure></code></pre><p>看看它们各自会宣称什么?  </p><pre><code>&gt;&gt;&gt; give_me_a_car.exclaim()I&apos;m a Car!&gt;&gt;&gt; give_me_a_yugo.exclaim()I&apos;m a Yugo! Much like a Car , but more Yugo-ish.  </code></pre><p>在上面的例子中，覆盖了父类的exclaim（）方法，在子类中，可以覆盖任何父类的方法，包括<strong>init</strong>()。下面的例子使用了之前创建过的Person类。我们来创建两个子类，分别代表医生（MDPerson）和律师（JDPerson）：    </p><pre><code>class Person():    def __init__(self，name):         self.name = nameclass MDPerson(Person):    def __init__(self，name):         self.name = &quot;Doctor&quot; + nameclass JDPerson(Person):    def __init__(self，name):         self.name = name + &quot;Esquire&quot;</code></pre><p>在上面的例子中，子类的初始化方法<strong>init</strong>()接收的参数和父类Person一样，但存储到对象内部name特性的值却不尽相同：  </p><pre><code>&gt;&gt;&gt; person = Person(&quot;Fudd&quot;)  &gt;&gt;&gt; doctor = MDPerson(&quot;Fudd&quot;)  &gt;&gt;&gt; lawyer = JDPerson(&quot;Fudd&quot;) &gt;&gt;&gt; print(person.name)Fudd&gt;&gt;&gt; print(doctor.name)Doctor Fudd&gt;&gt;&gt; print(lawyer.name)Fudd Esquire  </code></pre><h2 id="5-添加新方法"><a href="#5-添加新方法" class="headerlink" title="5. 添加新方法"></a>5. 添加新方法</h2><p><img src="https://i.loli.net/2018/06/05/5b1636660dfc1.jpg" alt></p><h2 id="6-使用super从父类得到帮助"><a href="#6-使用super从父类得到帮助" class="headerlink" title="6. 使用super从父类得到帮助"></a>6. 使用super从父类得到帮助</h2><p>调用父类的方法—-&gt; super()  </p><p>下面这个例子将定义一个新的类EmailPerson,用于表示有电子邮箱的Person。首先，来定义熟悉的Person类：</p><pre><code>class Person():    def __init__(self，name):         self.name = name  </code></pre><p>下面是子类的定义，注意，子类的初始化方法<strong>init</strong>()中添加了一个额外的email参数：  </p><pre><code>class EmailPerson(Person):    def __init__(self，name, email):        super().__init__(name)         self.email = email  </code></pre><p><img src="https://i.loli.net/2018/06/05/5b1640d76cb41.jpg" alt></p><h2 id="7-self的自辩"><a href="#7-self的自辩" class="headerlink" title="7. self的自辩"></a>7. self的自辩</h2><p>Python中必须把self设置为实例方法（前面例子中你见到的所有方法都是实例方法）的第一个参数。 Python使用self参数来找到正确的对象所包含的特性和方法。通过下面的例子，来查看调用对象方法背后Python实际做的工作。</p><p>前面例子中所讲的Car类，再次调用exclaim()方法：  </p><pre><code>&gt;&gt;&gt; car = Car()&gt;&gt;&gt; car.exclaim()I&apos;m a Car!  </code></pre><p><img src="https://i.loli.net/2018/06/05/5b162ee23298d.jpg" alt></p><p>没看完，后续补充 </p><h2 id="8"><a href="#8" class="headerlink" title="8."></a>8.</h2><h2 id="9"><a href="#9" class="headerlink" title="9."></a>9.</h2><h2 id="10"><a href="#10" class="headerlink" title="10."></a>10.</h2><h2 id="11"><a href="#11" class="headerlink" title="11."></a>11.</h2><h2 id="12"><a href="#12" class="headerlink" title="12."></a>12.</h2><h2 id="13"><a href="#13" class="headerlink" title="13."></a>13.</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2018/05/31/5b0f529386dd5.jpg&quot; alt&gt;  &lt;/p&gt;
&lt;h1 id=&quot;Python之对象和类&quot;&gt;&lt;a href=&quot;#Python之对象和类&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
      <category term="Python" scheme="https://xiaoxiaoaurora.github.io/categories/Python/"/>
    
    
      <category term="Python" scheme="https://xiaoxiaoaurora.github.io/tags/Python/"/>
    
      <category term="对象" scheme="https://xiaoxiaoaurora.github.io/tags/%E5%AF%B9%E8%B1%A1/"/>
    
      <category term="类" scheme="https://xiaoxiaoaurora.github.io/tags/%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>keras实现CRF层遇到的问题：3D张量的报错</title>
    <link href="https://xiaoxiaoaurora.github.io/2018/04/25/keras%E5%AE%9E%E7%8E%B0CRF%E5%B1%82%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%9A3D%E5%BC%A0%E9%87%8F%E7%9A%84%E6%8A%A5%E9%94%99/"/>
    <id>https://xiaoxiaoaurora.github.io/2018/04/25/keras实现CRF层遇到的问题：3D张量的报错/</id>
    <published>2018-04-25T12:09:45.000Z</published>
    <updated>2019-04-04T09:46:30.631Z</updated>
    
    <content type="html"><![CDATA[<h1 id="keras实现CRF层遇到的问题：3D张量的报错"><a href="#keras实现CRF层遇到的问题：3D张量的报错" class="headerlink" title="keras实现CRF层遇到的问题：3D张量的报错"></a>keras实现CRF层遇到的问题：3D张量的报错</h1><p>问题：在使用CRF层构建模型进行训练时，总是报错：  </p><p><img src="https://i.loli.net/2018/04/25/5ae07550f3a28.jpg" alt>  </p><p>下面是ValueError的完整信息：</p><pre><code>ValueError: Index out of range using input dim 2; input has only 2 dims for &apos;loss/crf_1_loss/strided_slice&apos; (op: &apos;StridedSlice&apos;) with input shapes: [?,?], [3], [3], [3] and with computed input tensors: input[3] = &lt;1 1 1&gt;.    </code></pre><p>是张量的shape问题。具体的原因还需要进一步查找，先标记下，五一假后解决。  </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;keras实现CRF层遇到的问题：3D张量的报错&quot;&gt;&lt;a href=&quot;#keras实现CRF层遇到的问题：3D张量的报错&quot; class=&quot;headerlink&quot; title=&quot;keras实现CRF层遇到的问题：3D张量的报错&quot;&gt;&lt;/a&gt;keras实现CRF层遇到的
      
    
    </summary>
    
      <category term="Keras" scheme="https://xiaoxiaoaurora.github.io/categories/Keras/"/>
    
      <category term="CodingErrors" scheme="https://xiaoxiaoaurora.github.io/categories/Keras/CodingErrors/"/>
    
    
      <category term="Keras" scheme="https://xiaoxiaoaurora.github.io/tags/Keras/"/>
    
      <category term="CodingErrors" scheme="https://xiaoxiaoaurora.github.io/tags/CodingErrors/"/>
    
      <category term="CRF" scheme="https://xiaoxiaoaurora.github.io/tags/CRF/"/>
    
  </entry>
  
  <entry>
    <title>softmax() got an unexpected keyword argument &#39;axis&#39;</title>
    <link href="https://xiaoxiaoaurora.github.io/2018/04/25/Keras%E4%B9%8Bsoftmax-got-an-unexpected-keyword-argument-axis/"/>
    <id>https://xiaoxiaoaurora.github.io/2018/04/25/Keras之softmax-got-an-unexpected-keyword-argument-axis/</id>
    <published>2018-04-25T02:09:45.000Z</published>
    <updated>2019-04-04T09:45:36.667Z</updated>
    
    <content type="html"><![CDATA[<h1 id="softmax-got-an-unexpected-keyword-argument-‘axis’"><a href="#softmax-got-an-unexpected-keyword-argument-‘axis’" class="headerlink" title="softmax() got an unexpected keyword argument ‘axis’"></a>softmax() got an unexpected keyword argument ‘axis’</h1><p>在使用keras构建模型时出现了以下错误：  </p><p><img src="https://i.loli.net/2018/04/25/5ae0295ac11f3.jpg" alt> </p><p>根据错误位置提示的最后一行<br><img src="https://i.loli.net/2018/04/25/5ae02d6730d70.jpg" alt></p><p>可以知道是在tensoflow_backend.py中出现错误，如下图：<br><img src="https://i.loli.net/2018/04/25/5ae02f1f09a2e.jpg" alt>   </p><p>解决方法：<br>把  </p><pre><code>return tf.nn.softmax(x, axis=axis)  </code></pre><p>改为：</p><pre><code>return tf.nn.softmax(x,axis)  </code></pre><p>简单粗暴好用的办法！<br>感谢<a href="https://blog.csdn.net/s_sunnyy/article/details/70469600" target="_blank" rel="noopener"> TypeError: concat() got an unexpected keyword argument ‘axis’</a> 提供参考意见。  </p><p>ps: 遇到狐假虎威的狼外婆和心怀险恶的小红帽，鲁智深是解决不了的，但是曹操自己能做到~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;softmax-got-an-unexpected-keyword-argument-‘axis’&quot;&gt;&lt;a href=&quot;#softmax-got-an-unexpected-keyword-argument-‘axis’&quot; class=&quot;headerlink&quot; t
      
    
    </summary>
    
      <category term="Keras" scheme="https://xiaoxiaoaurora.github.io/categories/Keras/"/>
    
      <category term="CodingErrors" scheme="https://xiaoxiaoaurora.github.io/categories/Keras/CodingErrors/"/>
    
    
      <category term="Keras" scheme="https://xiaoxiaoaurora.github.io/tags/Keras/"/>
    
      <category term="CodingErrors" scheme="https://xiaoxiaoaurora.github.io/tags/CodingErrors/"/>
    
      <category term="Softmax" scheme="https://xiaoxiaoaurora.github.io/tags/Softmax/"/>
    
  </entry>
  
</feed>
